{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f615f3d5-3e16-4904-980a-15dd0652bbcc",
   "metadata": {},
   "source": [
    "### RAG Pipeline - Data Ingestion to Vector DB Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce367947-7c7d-42ee-916c-ddf19db1e737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\User\\Jansen\\Self Study\\2025 - 11 - NOVEMBER\\RAG LangChain Thesis\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca07c187-1bc1-4026-ba4c-372e20d50aef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 PDF files to process\n",
      "\n",
      "Processing: A Dataset and Model for Realistic License Plate Deblurring.pdf\n",
      "Loaded 9 pages\n",
      "\n",
      "Processing: Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf\n",
      "Loaded 19 pages\n",
      "\n",
      "Processing: Arcega_CV.pdf\n",
      "Loaded 1 pages\n",
      "\n",
      "Processing: Jansen_Cruz_CV.pdf\n",
      "Loaded 1 pages\n",
      "\n",
      "Total documents loaded: 30\n"
     ]
    }
   ],
   "source": [
    "# Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory: str) -> list[Document]:\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "\n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "\n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata[\"source_file\"] = pdf_file.name\n",
    "                doc.metadata[\"file_type\"] = \"pdf\"\n",
    "\n",
    "            all_documents.extend(documents)\n",
    "            print(f\"Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "108382fc-5356-4f9a-87b7-f9330b49e3d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='A Dataset and Model for Realistic License Plate Deblurring\\nHaoyan Gong, Yuzheng Feng, Zhenrong Zhang, Xianxu Hou, Jingxin Liu, Siqi\\nHuang and Hongbin Liu∗\\nSchool of AI and Advanced Computing, Xi’an Jiaotong-Liverpool University\\n{haoyan.gong21, yuzheng.feng21, zhenrong.zhang21}@student.xjtlu.edu.cn, {xianxu.hou, jingxin.liu,\\nsiqi.huang, hongbin.liu}@xjtlu.edu.cn\\nAbstract\\nVehicle license plate recognition is a crucial task\\nin intelligent traffic management systems. How-\\never, the challenge of achieving accurate recogni-\\ntion persists due to motion blur from fast-moving\\nvehicles. Despite the widespread use of image\\nsynthesis approaches in existing deblurring and\\nrecognition algorithms, their effectiveness in real-\\nworld scenarios remains unproven. To address this,\\nwe introduce the first large-scale license plate de-\\nblurring dataset named License Plate Blur (LP-\\nBlur), captured by a dual-camera system and pro-\\ncessed through a post-processing pipeline to avoid\\nmisalignment issues. Then, we propose a Li-\\ncense Plate Deblurring Generative Adversarial Net-\\nwork (LPDGAN) to tackle the license plate de-\\nblurring: 1) a Feature Fusion Module to integrate\\nmulti-scale latent codes; 2) a Text Reconstruction\\nModule to restore structure through textual modal-\\nity; 3) a Partition Discriminator Module to en-\\nhance the model’s perception of details in each let-\\nter. Extensive experiments validate the reliabil-\\nity of the LPBlur dataset for both model training\\nand testing, showcasing that our proposed model\\noutperforms other state-of-the-art motion deblur-\\nring methods in realistic license plate deblurring\\nscenarios. The dataset and code are available at\\nhttps://github.com/haoyGONG/LPDGAN.\\n1 Introduction\\nEfficient recognition of vehicle license plates is crucial for\\nintelligent traffic management systems, however real-world\\nscenarios often pose a significant challenge due to motion\\nblur. This blur, making license plates unreadable, is espe-\\ncially problematic in situations involving high-speed vehicles\\nor low-light conditions. Such issues are exacerbated during\\nnighttime or in bad weather, resulting in considerable motion\\nblur in captured images. To tackle these issues, our study in-\\ntroduces a comprehensive dataset and a novel model tailored\\nfor realistic license plate deblurring.\\n∗Corresponding author\\nMSSNet LBAGMIMO-UnetBlurred \\nLicense Plate Ground Truth\\nLPDGAN\\n(Ours)\\nFigure 1: The visual deblurring results of several state-of-the-art\\nmodels and our model for real-world motion blurred license plate\\nimages.\\nImage deblurring is a key task in computer vision, focused\\non restoring blurred images to clear ones for accurate obser-\\nvation and identification. The progress in this field heavily\\ndepends on the development of relevant datasets. Current\\nmethods for creating deblurring datasets fall into three main\\ncategories: (1) synthetic blurring using blur kernels [Sun\\net al. , 2013; K ¨ohler et al. , 2012; Lai et al. , 2016 ], which\\nleads to a lack of generalization capability for models trained\\non these synthesized images when applied to real-world im-\\nages. (2) The generation of blurred images from sharp frames\\nvia averaging or fusion [Nah et al., 2017; Shen et al., 2019;\\nJiang et al. , 2020 ], which doesn’t fully mimic real-world\\noverexposure outliers [Chang et al., 2021]. (3) Lastly, beam-\\nsplitting systems capture sharp and blurred image pairs via\\ncamera shake [Rim et al., 2020], with potential issues in color\\naccuracy and alignment. Each approach contributes to the\\nfield but also has inherent limitations impacting the realism\\nand utility of the datasets.\\nWith the advent of deep learning, numerous convolutional\\nneural network (CNN)-based methods have surfaced [Sun et\\nal., 2015; Gong et al., 2017; Tao et al., 2018; Shen et al.,\\n2019; Zhang et al. , 2023 ], playing an essential role in the\\nmotion deblurring task. Recently, the proposal of Genera-\\ntive Adversarial Networks (GAN) has also profoundly im-\\npacted the image deblurring field[Ramakrishnan et al., 2017;\\nKupyn et al., 2018; Kupyn et al., 2019; Zhao et al., 2022].\\nDespite these advancements, deblurring license plate images\\nremains a significant challenge, primarily due to the lack of\\nlarge-scale, tailored datasets. The complexity of license plate\\nblurring, with its more severe degradation compared to stan-\\ndard motion blur, poses an additional challenge. To better jus-\\ntify the performance of existing image deblurring algorithms\\non real-world blurred license plate images, we evaluate the\\nperformance of several state-of-the-art deblurring algorithms\\narXiv:2404.13677v1  [cs.CV]  21 Apr 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='with blurred license plate images. As shown in Figure 1, we\\ncan conclude that all these methods fail to perform well in this\\ntask. It underscores the necessity for further research specifi-\\ncally targeting real-world vehicle license plate deblurring.\\nTo address these challenges, we present a comprehensive\\nsolution consisting of a large-scale paired license plate dataset\\nand a dedicated license plate deblurring model. Our data col-\\nlection employs a dual-camera setup with different shutter\\nspeeds to capture sharp and blurred images simultaneously,\\neliminating color deviations and enabling post-processing\\nalignment. Our innovative end-to-end model leverages an en-\\ncoder and latent fusion module for handling multi-scale la-\\ntent codes, featuring the Swin transformer block ( [Liu et al.,\\n2021]) for effective long-range modeling. To enhance let-\\nter reconstruction and text legibility, we introduce a partition\\ndiscriminator assessing per-letter sharpness. Extensive ex-\\nperiments using our LPBlur dataset, including metrics such\\nas L1 loss, Peak Signal-to-Noise Ratio (PSNR), Structural\\nSimilarity Index (SSIM), Perceptual Loss (PerL), and Text\\nLevenshtein Distance (TLD) [Levenshtein and others, 1966],\\nvalidate its suitability for training and testing. Our proposed\\nmodel outperforms state-of-the-art motion deblurring meth-\\nods in real-world license plate deblurring scenarios.\\nIn summary, our main contributions are as follows:\\n• We present a real-world sharp-blurred license plate\\ndataset, named LPBlur. This dataset consists of 10,288\\npaired images, meticulously collected under diverse\\nreal-road scenarios using our designed dual-camera sys-\\ntem, and corrected by a post-processing pipeline.\\n• We introduce a novel LPDGAN, a license plate deblur-\\nring model that leverages multi-scale latent codes as ref-\\nerences. It incorporates both a partition discriminator\\nand text reconstruction techniques, which enhance the\\nmodel’s capability to generate high-quality license plate\\nimages through spatial architecture and textual informa-\\ntion, respectively.\\n• Extensive experiments demonstrate that our dataset LP-\\nBlur is highly effective for model training and evalu-\\nation. Compared to other state-of-the-art (SOTA) de-\\nblurring models, our proposed LPDGAN can achieve\\n21.24% license plate recognition accuracy improve-\\nment.\\n2 Related Work\\n2.1 Image Deblurring Datasets\\nImage deblurring relies on paired sharp-blur image datasets.\\nTraditionally, blurred images are generated by convolving\\nsharp images with uniform or non-uniform blur kernels\\n[Levin et al. , 2009; Sun et al. , 2013; K ¨ohler et al. , 2012;\\nLai et al., 2016]. Consequently, some researchers attempt to\\ncapture sequences of sharp frames while vibrating the cam-\\nera, averaging or fusing such sequences of frames into cor-\\nresponding motion-blurred images [Nah et al., 2017; Shen et\\nal., 2019; Jiang et al. , 2020; Noroozi et al. , 2017 ]. HIDE\\ndataset [Shen et al., 2019 ] is created by averaging 11 con-\\nsecutive frames, with the central frame serving as the sharp\\nimage. The same strategy is employed in the collection of the\\nBlur-DVS dataset [Jiang et al., 2020] and MSCNN (WILD)\\ndataset [Noroozi et al., 2017]. However, models lack general-\\nizability to real-world scenarios when they are trained on such\\nsynthetic datasets generated using the aforementioned meth-\\nods. Recently, certain researchers gathered authentic pairs of\\nsharp-blur images employing beam-splitting systems. They\\nposition two cameras at a fixed angle to ensure that both im-\\nages share the same visual field, as described in works by Rim\\net al. and Li et al.. However, this approach can lead to color\\ncast discrepancies in the paired images due to inherent issues\\nwith beam-splitting systems.\\n2.2 Blind Deblurring\\nThe majority of conventional approaches employ priors of\\nnatural images to estimate latent images or blur kernels [Fer-\\ngus et al. , 2006; Shan et al. , 2008; Cho and Lee, 2009;\\nRen et al., 2018; Whyte et al., 2012 ]. However, the afore-\\nmentioned techniques have certain limitations by predicat-\\ning upon the assumption of uniform image blur. To address\\nthis issue, some methods [Ren et al., 2017; Hyun Kim et al.,\\n2013] estimate blur kernels at a pixel level, thereby accom-\\nmodating more complex blurring situations.\\nWith the advent of deep learning technologies, significant\\nstrides have been made in image deblurring, applying deep\\nlearning to predict blur kernels or latent images to procure\\nclear images. In the work of [Sun et al. , 2015 ], a method\\nbased on CNNs is proposed to predict the probability distri-\\nbution of block-level motion blur. Gong et al. introduces a\\nmethod to directly estimate the motion flow of blurred im-\\nages, recovering non-blurred images from the estimated mo-\\ntion flow. MIMO-UNet [Cho et al., 2021] deploys a multi-\\ninput-multi-output single Unet network to simulate multi-\\nlevel Unet for noise reduction across various image scales.\\nMSSNet [Kim et al., 2022] enhances deblurring network per-\\nformance by using a stage configuration reflecting blur scales,\\nan inter-scale information propagation scheme, and a pixel-\\nshuffle-based multi-scale scheme. XYDeblur [Ji et al., 2022]\\nfurther augments network efficiency and deblurring perfor-\\nmance by employing rotated and shared kernels within the\\ndecoder.\\n2.3 GAN-Based Deblurring\\nIn recent years, following the inception of GANs, their ap-\\nplication in the domain of image deblurring has achieved\\nremarkable success. DeblurGAN [Kupyn et al., 2018 ] first\\npresents an end-to-end learning method for motion deblur-\\nring, and also introduces a new method for blur generation.\\nDeblurGAN-v2 [Kupyn et al., 2019] introduces a dual-scale\\ndiscriminator based on a relative conditional GAN framework\\nand incorporates a feature pyramid into the deblurring pro-\\ncess, which permits the flexible substitution of the backbone\\nnetwork. MSG-GAN [Karnewar and Wang, 2020] addresses\\nthe issue of insufficient overlap between the true and false\\nsupport distributions during the transfer from discriminator to\\ngenerator in GANs by allowing multi-scale gradient networks\\nfrom the discriminator to the generator. FCL-GAN [Zhao\\net al. , 2022 ] designs a lightweight domain conversion unit\\n(LDCU) and a parameter-free frequency-domain contrastive'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='Pedestrian Bridge\\nTrigger\\nPower\\nPower\\n(a) (b)\\nDenosing\\nGeometrical \\nAlignment\\nLicense Plate \\nCropping\\nCam S Cam L\\nSynchronizer\\nCam L\\nCam L\\nCam L\\nCam L\\nOutput from Cam L\\nOutput from Cam L\\nOutput from Cam S\\nOutput from Cam S\\nCam L\\nCam L\\nCam S\\nCam S\\nCam S\\nCam S\\nCam S\\nCam S\\nFigure 2: (a) A schematic diagram of the paired image acquisition system collecting data in a pedestrian bridge. (b) The pipeline of paired\\nimages post-processing.\\nunit (PFCU) for lightweight property and performance su-\\nperiority. The aforementioned methods can handle standard\\nblurred images, but they struggle to deliver satisfactory re-\\nsults on license plate blurring with very severe degradation.\\nWe propose an end-to-end generative model that accommo-\\ndates multi-scale inputs and outputs, employing several novel\\nmodules to accomplish the license plate deblurring task bet-\\nter.\\n3 Proposed LPBlur Dataset\\n3.1 Data Collection\\nCauses of Motion Blur.Motion blur refers to the percepti-\\nble streaking effect observed when capturing the movement\\nof objects. In the capturing process, the correlation be-\\ntween the amount of light entering the photosensitive compo-\\nnent and the camera’s basic parameters satisfies the following\\nequation:\\nLa ∝ SL × ISO × Et × (Ap)2 , (1)\\nwhere La denotes the number of photons received by the\\ncamera, SL represents the light intensity of the scene, ISO\\nrepresents the camera ISO value, Et is the exposure time, Ap\\ndenotes the camera aperture size. Cameras adjust these pa-\\nrameters automatically within limits depending on the light-\\ning situation. For example, cameras increase their aperture\\nand exposure time in low-lighting settings to capture enough\\nlight. Fast-moving objects leave trajectories within a single\\nframe during this extended exposure time, resulting in a mo-\\ntion blur effect.\\nPaired Image Acquisition. To collect paired sharp-blur\\nimages, we use two identical scientific cameras that are set\\nwith different exposure times. As shown in Figure 2a, cam-\\neras S and L are fixed parallel on a tripod to maintain hori-\\nzontally to the ground. Specifically, Camera S is set by an\\nextremely short exposure time Ets, employed for the collec-\\ntion of sharp images, while Camera L is set by a relatively\\nlonger exposure time Etl for the acquisition of blurred im-\\nages. Both of these cameras are interfaced with a computer\\nvia a synchronizer, which ensures the synchronization of the\\nstart of exposures, and both cameras capture the same scene.\\nScenes are taken in a variety of locations, including above,\\non the right side, and on the left side of roadways, to guaran-\\ntee the dataset’s diversity. Also, depending on the road and\\nthe illumination conditions, we dynamically modulate cam-\\nera exposure time according to the subsequent equation:\\nv · Et = b · D\\np · f , (2)\\nwhere v denotes vehicular velocity, Et represents exposure\\ntime, b represents pixels blurred, D is the distance between\\nvehicle and camera, f denotes camera focal length, and p is\\npixel edge length on the sensor. Given that the actual speeds\\nof individual vehicles are indeterminable, we standardized\\nimage captures on high-speed road sections with a regulatory\\nspeed limit of 70 km/h to ensure that D remained within a\\nrange of 10-20 meters.\\nMoreover, to ensure equality in exposure between two\\ncameras, we make their exposure times and ISO values to\\nsatisfy the following equation:\\nISOs\\nISOl\\n= Etl\\nEts\\n, (3)\\nwhere ISO s is the ISO value for Camera S and ISO l Cam-\\nera L. However, variations in ISO values can cause changes\\nin image noise levels, in post-processing, we incorporate a\\ndenoising step for sharp images.\\n3.2 Data Post-processing\\nAs shown in Figure 2b, the paired image post-processing in-\\ncludes denoising, geometrical alignment, and license plate\\ncropping.\\nDenoising. Due to the disparate ISO settings of the two\\ncameras, Camera S and Camera L captured images with a\\ndifferent noise level. Consequently, during the conversion of\\nRAW images to RGB format, wavelet denoising [Liu et al.,\\n2020] is employed after white-balancing, color mapping, and\\ngamma correction.\\nGeometrical Alignment. Cameras S and L capture sharp\\nand blurred image pairs with slight horizontal misalignment\\neven though they are closely aligned left-to-right to minimize\\nthe difference. To align these image pairs perfectly, we first\\ntake a static image pair without any moving vehicles as the\\nreference pair for each scene. Then, the Enhanced Corre-\\nlation Coefficient Maximization [Evangelidis and Psarakis,\\n2008] is adopted to estimate the geometric transformation be-\\ntween the sharp and blur of the reference image pair. Finally,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='Fake ImageLatent \\nFusion\\nLatent \\nFusion\\nConcat \\nLatent \\nCodes\\nPartition\\nDiscriminator \\nModule\\nGlobal\\nDiscriminator\\nConv\\nConv\\nα β\\nLower-Res Feature\\nHigher-Res \\nFeature\\nLatent Fusion Module\\nIdentity\\nSwin \\nTransformer \\nEncoder\\nSwin \\nTransformer \\nEncoder\\nSwin \\nTransformer \\nEncoder\\nSwin Transformer \\nDecoder\\nInput Image\\nInput Image\\nInput Image\\nInput Image\\nInput Image\\nInput Image\\nInput Image\\nInput Image\\nInput Image\\nInput Image\\nInput Image\\nInput Image\\nFake Image\\nFake Image\\nText \\nReconstruction \\nModule\\nFigure 3: Overview of the proposed Licence Plate Deblurring Generative Adversarial Network.\\nthe estimated geometric transformation is applied to the im-\\nage pairs in the same scene.\\nLicense Plate Cropping. A pre-trained YOLO v5 [Jocher,\\n2020] and a pre-trained CRNN [Shi et al., 2016] model are\\nfacilitated the detection and recognition of license plates un-\\nder standard conditions, both models are pre-trained on the\\nCCPD [Xu et al., 2018] license plate dataset. Following the\\ngeometrical alignment, the pre-trained YOLO v5 and CRNN\\ndetect and recognize the bounding box of each sharp plate in\\nthe paired images, both the sharp and blurred images are then\\ncropped using the same detected coordinates.\\nIn conclusion, we collect 10,288 image pairs, with an orig-\\ninal image size of 1920 × 1220. Post-processing crops image\\nsize to 224 × 112 with blur size ranging from 20-50 pixels.\\nAmong them, 5672 pairs are captured under normal light con-\\nditions and 4616 pairs under low light conditions, including\\n1,000 pairs under rainy environmental conditions. For more\\ninformation, please refer to the released dataset on the GitHub\\nrepository.\\n4 Method\\nOverview. The goal of our work is to improve the clarity of\\nlicense plate images using a meticulously crafted image-to-\\nimage translation framework, called LPDGAN. As depicted\\nin Figure 3, our approach first constructs a multi-scale fea-\\nture extraction and fusion module designed to encode input\\nblurred images effectively. Subsequently, an image decoder\\nis employed to generate sharp and high-quality images. To\\nfurther enhance the overall image quality, we integrate both a\\nglobal discriminator and a partition discriminator for adver-\\nsarial training. Additionally, we incorporate a text reconstruc-\\ntion module to enrich the semantic information embedded in\\nthe generated license plate images.\\n4.1 Multi-scale Feature Extraction and Latent\\nFusion Module\\nFeature Extraction. In real-world scenarios, license plate\\nimages affected by motion blur often exhibit intricate degra-\\ndations, including noise, low resolution, and ghosting effects.\\nOur feature encoder, denoted as E, is specifically used to\\naddress these degradations, extracting essential image fea-\\ntures for subsequent processing. In particular, the Swin trans-\\nformer block [Liu et al., 2021] is selected for its ability to cap-\\nture global information through self-attention mechanisms.\\nThis is crucial to resolve the elongated ghosting artifacts that\\noften appear in motion-blurred license plate images. To ad-\\ndress variations in license plate image sizes due to differing\\ncapture distances, our approach employs a multi-scale fea-\\nture extraction strategy, which is illustrated in Figure 3. This\\napproach facilitates the encoding of features at each scale,\\nwhich are represented as E(xi) for i = 1, 2, 3.\\nLatent Fusion. Based on the Spatial Feature Transform\\n(SFT) [Wanget al., 2018], we further propose a Latent Fusion\\nModule F (see Figure 3). This module is designed based on\\nan affine transformation to effectively integrate the obtained\\nmulti-scale features. Specifically, for the fusion of E(x1)\\nand E(x2), we first split E(x2) along the channel dimension.\\nEach part is then processed through a series of convolutional\\nlayers to derive the fusion parameters α and β. These pa-\\nrameters are employed to modulate E(x1) through scaling\\nand shifting operations. Moreover, we reintegrate the origi-\\nnal E(x2) using a skip connection, which is then combined\\nwith the modified E(x1) along the channel dimension. This\\nfusion process is also applied betweenE(x2) and E(x3). The\\ncorresponding formulas are provided below:\\nα, β= Conv (E(xi+1)sp1 + E(xi+1)sp2) ,\\nFi,i+1 = Concat (α ⊙ E(xi) +β, E(xi)) , (4)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='Fake \\nPartitions\\nReal \\nPartitions\\nRandom \\nSelection\\nPartition Discriminator Module\\nReal Image\\nFake Image\\nPartition \\nDiscriminator\\nP\\nFigure 4: The architecture of Partition Discriminator Module.\\nwhere E(xi+1)sp1 and E(xi+1)sp2 represent the two parts\\ninto which E(xi+1) is divided along the channel dimension.\\n4.2 Decoder and Discriminator\\nDecoder. As depicted in Figure 3, our decoder D is com-\\nposed of a sequence of Swin transformer blocks and patch\\nexpanding operations. Similar to the encoder, our decoder is\\ndesigned to generate sharp images in a multi-scale fashion,\\nand the output images are denoted as ˜y1, ˜y2 and ˜y3 accord-\\ningly.\\nDiscriminator. We design two discriminators: 1) the\\nGlobal Discriminator (Dg) enhances overall spatial and color\\ninformation in the restored images; 2) the Partition Discrimi-\\nnator (Dp) focuses specifically on refining character informa-\\ntion by examining n randomly selected partitions of letters\\nwithin the license plate image. The structure of Dp is shown\\nin Figure 4. It identifies and locates letter positions in both\\nthe real image y and the fake image ˜y. Following this, n\\npartition images are randomly chosen for evaluation by Par-\\ntition Discriminator. In the early stages of training, when our\\nmode’s capacity to produce sharp images is still developing,\\nthe generated image might not be recognized with high ac-\\ncuracy. To address this, an average partitioning approach is\\napplied to both y and ˜y, initially setting n to 7. As training\\nprogresses, a pre-trained YOLO v5 model is used for precise\\nletter detection and the number of partitions n is set to 3. In\\nour experiment, we employ WGAN-GP to train our model.\\nIn particular, the adversarial loss for the Global Discrimina-\\ntor can be formulated as follows:\\nLDg =E˜y [Dg (˜y)] − Ey [Dg (y)]\\n+ λgp1Eˆy\\nh\\x00\\n∥∇ˆyDg (ˆy)∥2 − 1\\n\\x012i\\n,\\nˆy = ϵ · ˜y + (1− ϵ) · y, ϵ∼ U [0, 1] .\\n(5)\\nSimilarly, for the Partition DiscriminatorDp, the formulation\\nis as follows:\\nLDp =E˜y [Dp (P (˜y))] − Ey [Dp (P (y))]\\n+ λgp2Eˆy\\nh\\x00\\n∥∇ˆyDp (P (ˆy))∥2 − 1\\n\\x012i\\n,\\nˆy = ϵ · P (˜y) + (1− ϵ) · P (y) , ϵ∼ U [0, 1] ,\\n(6)\\nwhere P is the partition operation, λgp1 and λgp2 are the\\nweighting parameters used to control the gradient penalty.\\nNote that we apply both discriminators to the outputs at three\\ndifferent scales.\\nIn addition to the adversarial loss, we also incorporate re-\\nconstruction loss Lrec, defined as follows:\\nLrec = λl1 ∥y − ˜y∥1 + λper ∥ψvgg (y) − ψvgg (˜y)∥2 , (7)\\nwhere the ψvgg represents a pre-trained VGG-19 network[Si-\\nmonyan and Zisserman, 2014 ], from which we use feature\\nmaps from the 8 th, 15th, and 22nd ReLU layers to compare\\nshallow textures and deep features between real and gener-\\nated images.\\n4.3 Text Reconstruction Module\\nWe also incorporate a Text Reconstruction Module T specif-\\nically designed to enhance our model’s ability to accurately\\ninterpret characters on license plate images. T merges the\\nDecoder’s intermediate featureFD with the fusion latent code\\nF2,3 along the channel dimension. This combined feature\\nthen traverses a series of convolutional and linear layers, re-\\nsulting in a vector that represents the recognized text. Con-\\ncurrently, a pre-trained CRNN model extracts ground-truth\\ntext vectors from real images. We calculate the L1 loss be-\\ntween the output vector from our Text Reconstruction Module\\nT and the ground-truth text vector. The loss is defined as:\\nLtext = ∥T (F2,3, FD) − ψcrnn (y)∥1 , (8)\\nwhere FD is the feature maps obtained from the middle layers\\nof Decoder, ψcrnn represents the pre-trained CRNN model.\\n4.4 Fully Objective\\nOur full objective is\\nL(E, D, F, Dg, Dp, T) =Lrec + λgLDg + λpLDp\\n+ λtLtext, (9)\\nwhere λg, λp and λt control the relative importance of the\\ndifferent objectives. We aim to solve:\\nE∗, F∗, D∗ = arg min\\nE,F,D,T\\nmax\\nDg,Dp\\nL(E, D, F, Dg, Dp, T).\\n(10)\\n5 Experiment\\n5.1 Experimental Setups\\nDataset setup. The proposed LPBlur dataset is partitioned\\ninto a training set with 9,288 image pairs, and a test set with\\n1,000 image pairs. The test set encompasses 500 pairs ac-\\nquired under normal light conditions and another 500 pairs\\ncaptured in low light conditions.\\nEvaluation metrics. To evaluate the image quality of de-\\nblurred images, we adopt three evaluation metrics: PSNR,\\nSSIM, and Perceptual Loss (PerL). The Perceptual Loss is\\nspecifically defined by comparing the generated images with\\nthe ground truth images at the output feature maps of sequen-\\ntial layers 8, 15, and 22 of a pre-trained VGG-19 model.\\nTo assess the recognisability of the generated license plate\\nimages, we calculate the Text Levenshtein Distance (TLD)\\n[Levenshtein and others, 1966 ] between the detected text of\\nthe generated images and the real images.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='Ground \\nTruth\\nLPDGAN \\n(ours)\\nLBAG\\nMSSNet\\nMIMO-Unet\\nPix2Pix\\nDeblurGAN-v2\\nBlurred \\nImage\\nNormal Light Low Light\\nFigure 5: Visual comparison of different deblurring methods on LPBlur dataset. The test scene is divided into normal light and low light.\\nSince the results in low light scenes are difficult to distinguish visually, we uniformly increase their brightness. The original brightness of the\\nlow light scene refers to the first line, which is Blur Image.\\nTable 1: Quantitative results of comparing motion deblurring models. “PerL” and “TLD” denote Perceptual Loss and Text Levenshtein\\nDistance, respectively.\\nScenario Normal Light Low Light\\nPerL↓ PSNR↑ SSIM↑ TLD↓ PerL↓ PSNR↑ SSIM↑ TLD↓\\nPix2Pix 5.57 28.89 0.6669 1.35 2.24 28.71 0.7491 2.53\\nDeblurGAN v2 8.02 28.51 0.5257 2.28 4.32 28.11 0.5451 4.34\\nMIMO-UNet 3.79 29.12 0.7448 1.02 1.65 29.03 0.8083 2.68\\nMSSNet 3.39 29.63 0.7891 0.62 2.74 29.62 0.8725 1.05\\nLBAG 3.34 29.24 0.7916 0.58 1.44 29.44 0.8889 1.13\\nLPDGAN (ours) 3.31 29.95 0.7950 0.57 1.01 30.96 0.9214 0.81\\nImplementation details. The shape of multi-scale input\\nimages for LPDGAN are (112, 224, 3), (56, 112, 3), and\\n(28, 56, 3) respectively. Random rain adding and random\\ncutout are utilized for data augmentation. The optimizer we\\nuse is Adam [Kingma and Ba, 2014 ]. The batch size is set\\nto 7. The initial learning rate is 10−4, and the linear weight\\ndecay is used after the100th epoch. All experiments are con-\\nducted on a GeForce RTX 3090 GPU.\\n5.2 Deblur Results\\nTo evaluate the deblur performance of our method, we com-\\npare LPDGAN with five SOTA methods: Pix2Pix [Isola et\\nal., 2017], DeblurGAN v2 [Kupyn et al., 2019], MIMO-Unet\\n[Cho et al., 2021], MSSNet [Kim et al., 2022], and LBAG[Li\\net al., 2023] on LPBlur.\\nFrom the results presented in Table 1, it can be observed\\nthat our LPDGAN outperforms all other models in both nor-\\nmal and low light conditions. In normal light conditions,\\nour LPDGAN achieves a PerL of 3.31, PSNR of 29.95, and\\nSSIM of 0.795, which are superior to the latest deblurring\\nmethod LBAG. The performance gap becomes even more\\npronounced when dealing with low light images, with our\\nmodel exhibiting improvements of 29.8%, 4.5%, and 3.7%\\nin PerL, PSNR, and SSIM, respectively, compared to LBAG\\nand MSSNet.\\nFigure 5 provides a visual comparison between sets of\\nblurred and deblurred images under two light conditions. In\\nthe case of normal light, our LPDGAN effectively restores\\nlicense plates afflicted with severe motion blur, accurately\\ngenerating and reconstructing characters such as ‘D’, ‘O’ and\\n‘B’, as well as numbers like ‘7’, ‘1’, and ‘L’, which often\\npose challenges for other models, as shown in the 1st, 3rd\\nand 7th column of Figure 5. In low light scenarios, where\\nlicense plates are barely visible to the human eye, our model\\nexcels in generating details that significantly surpass the per-\\nformance of other models. This highlights the inadequacy of\\nmodels designed for minor blurs in large scenes when applied\\nto the deblurring of license plates, which are subject to more\\nsevere blurring.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='Blurred \\nLicense Plate\\nTrained on \\nSynthetic Data\\nTrained on \\nLPBlur Data\\nGround \\nTruth\\nFigure 6: Visual results comparison of LPDGAN trained on syn-\\nthetic data and LPBlur data.\\nNormal Light PerL ↓ PSNR↑ SSIM↑ TLD↓\\nSynthetic Data 3.45 28.74 0.7061 1.68\\nLPBlur Data 3.31 29.95 0.7950 0.57\\nLow Light PerL ↓ PSNR↑ SSIM↑ TLD↓\\nSynthetic Data 2.05 28.28 0.7933 2.65\\nLPBlur Data 1.01 30.96 0.9214 0.81\\nTable 2: Quantitative comparison using synthetic and LPBlur data\\nin normal and low light scenarios, respectively.\\n5.3 Text Recognition Results\\nWe further evaluate the plate text recognition accuracy based\\non those deblurred images. The CRNN [Shi et al. , 2016 ]\\nis incorporated for the recognition of generated and sharp\\nlicense plate characters. The 4th and 8th columns in Ta-\\nble 1 compare the TLD between the generated images and\\noriginal sharp images. In the context of normal light condi-\\ntions, LPDGAN exhibits comparable performance to LBAG\\nin terms of TLD but surpasses all other models. Under low\\nlight conditions, LPDGAN is the only model with a TLD\\nlower than 1. This implies that a pre-trained CRNN model,\\nwhen employed to recognize deblurred license plate images\\nproduced by LPDGAN, will obtain an average error in less\\nthan one character per instance. Consequently, LPDGAN has\\nthe best performance overall, demonstrating robust capability\\nin deblurring license plates across two scenarios.\\n5.4 Ablation Study\\nTo evaluate the effectiveness of each proposed module, a se-\\nries of ablation experiments are performed, which is shown\\nin Table 3. The omission of the Latent Fusion Module led to\\na decline in global metrics, underscoring its effectiveness in\\nfusing multi-scale features and improving the model’s perfor-\\nmance in restoring sharp images. Removing the Text Recon-\\nstruction Module resulted in a significant downturn in global\\nmetrics, particularly noticeable under low light conditions.\\nThis highlights the pivotal role of the Text Reconstruction\\nModule in enabling the model to have a deeper understand-\\ning and restoration capability for license plates affected by\\nsevere pixel disruption. Similarly, the exclusion of the Par-\\ntition Discriminator Module deteriorated global metrics and\\nnotably affected the SSIM metric to a greater extent. This\\nconfirms the module’s contribution to enhancing the model\\nfocus on the details of each letter on the license plate.\\nModel No.\\nNormal Light\\nLa. T e. P DPerL↓ PSNR↑ SSIM↑ TLD↓\\n1 ✓ ✓ 3.49 29.68 0.7883 0.69\\n2 ✓ ✓ 3.56 29.41 0.7797 0.72\\n3 ✓ ✓ 3.41 29.73 0.7829 0.61\\nLPDGAN ✓ ✓ ✓ 3.31 29.95 0.7950 0.57\\nModel No.\\nLow Light\\nLa. T e. P DPerL↓ PSNR↑ SSIM↑ TLD↓\\n1 ✓ ✓ 1.26 30.05 0.9165 0.92\\n2 ✓ ✓ 1.79 29.12 0.8861 1.45\\n3 ✓ ✓ 1.38 29.93 0.9012 1.01\\nLPDGAN ✓ ✓ ✓ 1.01 30.96 0.9214 0.81\\nTable 3: Ablations of LPDGAN on LPBlur. La.,Te. and PD de-\\nnote the Latent Fusion Module, Text Reconstruction Module and\\nPartition Discriminator Module, respectively.\\n5.5 Necessity of LPBlur\\nWe further demonstrate the importance of introducing a\\ndataset consisting of real blurred images for the task of li-\\ncense plate deblurring. To assess this, we employ different\\nblur kernels randomly to the sharp images in LPBlur dataset\\nand finally create a synthetic dataset. The result samples, il-\\nlustrated in Figure 6 and summarized in Table 2, clearly in-\\ndicate that LPDGAN trained on the synthetic dataset fails to\\neliminate real-world license plate blur effectively. This is ev-\\nident both visually and in the metric evaluations, showcasing\\npoorer performance compared to when trained on the LPBlur\\ndataset.\\nThe aforementioned outcomes highlight a significant dis-\\nparity between synthetic and real-world license plate blur,\\nemphasizing that synthetic blurred image data cannot serve as\\na substitute for the LPBlur dataset. Thus, the LPBlur dataset\\nproves to be more effective in training models for real-world\\nlicense plate deblurring.\\n6 Conclusion\\nIn this paper, we study the issue of motion license plates de-\\nblurring. We introduce the first large-scale license plate de-\\nblurring dataset for this research and address color bias and\\nmisalignment problems through appropriate data collection\\nmethods and post-processing. Furthermore, given that the de-\\ngree of blur caused by vehicular motion substantially exceeds\\nthat induced by camera shake, we propose a model based on\\nmulti-scale input and output for license plate deblurring. This\\nincludes a latent fusion module, a supervision module for tex-\\ntual modality information, and a partition discriminator mod-\\nule. Experimental results indicate that our model performs\\nfavorably in comparison to current state-of-the-art deblurring\\nalgorithms. In the future, we intend to augment our dataset\\nwith license plates from a broader range of countries and re-\\ngions to enhance its diversity. Regarding the model, we in-\\ntend to incorporate modules that ensure the restoration capa-\\nbility for spatially complex characters, such as Chinese char-\\nacters.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='Acknowledgments\\nThis work was jointly supported by the National Natural Sci-\\nence Foundation of China (62201474 and 62206180), Suzhou\\nScience and Technology Development Planning Programme\\n(Grant No.ZXL2023171) and XJTLU Research Development\\nFunds (RDF-21-02-084, RDF-22-01-129, RDF-22-01-134,\\nand RDF-23-01-053).\\nEthical Statement\\nTo prevent the disclosure of personal privacy, all private in-\\nformation, including human faces and surrounding scenery, is\\nremoved from the images in the LPBlur, and only the license\\nplate number is retained. In addition, sensitive metadata in\\nthe image is removed, including GPS location, timestamp,\\netc.\\nReferences\\n[Chang et al., 2021] Meng Chang, Chenwei Yang, Huajun\\nFeng, Zhihai Xu, and Qi Li. Beyond camera motion\\nblur removing: How to handle outliers in deblurring.\\nIEEE Transactions on Computational Imaging , 7:463–\\n474, 2021.\\n[Cho and Lee, 2009] Sunghyun Cho and Seungyong Lee.\\nFast motion deblurring. In ACM SIGGRAPH Asia 2009\\npapers, pages 1–8. 2009.\\n[Cho et al., 2021] Sung-Jin Cho, Seo-Won Ji, Jun-Pyo\\nHong, Seung-Won Jung, and Sung-Jea Ko. Rethinking\\ncoarse-to-fine approach in single image deblurring. In\\nProceedings of the IEEE/CVF international conference on\\ncomputer vision, pages 4641–4650, 2021.\\n[Evangelidis and Psarakis, 2008] Georgios D Evangelidis\\nand Emmanouil Z Psarakis. Parametric image align-\\nment using enhanced correlation coefficient maximization.\\nIEEE transactions on pattern analysis and machine intel-\\nligence, 30(10):1858–1865, 2008.\\n[Fergus et al., 2006] Rob Fergus, Barun Singh, Aaron Hertz-\\nmann, Sam T Roweis, and William T Freeman. Removing\\ncamera shake from a single photograph. In Acm Siggraph\\n2006 Papers, pages 787–794. 2006.\\n[Gong et al., 2017] Dong Gong, Jie Yang, Lingqiao Liu,\\nYanning Zhang, Ian Reid, Chunhua Shen, Anton Van\\nDen Hengel, and Qinfeng Shi. From motion blur to mo-\\ntion flow: A deep learning solution for removing heteroge-\\nneous motion blur. In Proceedings of the IEEE conference\\non computer vision and pattern recognition , pages 2319–\\n2328, 2017.\\n[Hyun Kim et al., 2013] Tae Hyun Kim, Byeongjoo Ahn,\\nand Kyoung Mu Lee. Dynamic scene deblurring. In Pro-\\nceedings of the IEEE international conference on com-\\nputer vision, pages 3160–3167, 2013.\\n[Isola et al., 2017] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou,\\nand Alexei A Efros. Image-to-image translation with con-\\nditional adversarial networks. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition ,\\npages 1125–1134, 2017.\\n[Ji et al., 2022] Seo-Won Ji, Jeongmin Lee, Seung-Wook\\nKim, Jun-Pyo Hong, Seung-Jin Baek, Seung-Won Jung,\\nand Sung-Jea Ko. Xydeblur: divide and conquer for sin-\\ngle image deblurring. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition,\\npages 17421–17430, 2022.\\n[Jiang et al., 2020] Zhe Jiang, Yu Zhang, Dongqing Zou,\\nJimmy Ren, Jiancheng Lv, and Yebin Liu. Learning event-\\nbased motion deblurring. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition,\\npages 3320–3329, 2020.\\n[Jocher, 2020] Glenn Jocher. YOLOv5 by Ultralytics, May\\n2020.\\n[Karnewar and Wang, 2020] Animesh Karnewar and Oliver\\nWang. Msg-gan: Multi-scale gradients for generative ad-\\nversarial networks. In Proceedings of the IEEE/CVF con-\\nference on computer vision and pattern recognition, pages\\n7799–7808, 2020.\\n[Kim et al., 2022] Kiyeon Kim, Seungyong Lee, and\\nSunghyun Cho. Mssnet: Multi-scale-stage network for\\nsingle image deblurring. In European Conference on\\nComputer Vision, pages 524–539. Springer, 2022.\\n[Kingma and Ba, 2014] Diederik P Kingma and Jimmy Ba.\\nAdam: A method for stochastic optimization. arXiv\\npreprint arXiv:1412.6980, 2014.\\n[K¨ohler et al., 2012] Rolf K ¨ohler, Michael Hirsch, Betty\\nMohler, Bernhard Sch ¨olkopf, and Stefan Harmeling.\\nRecording and playback of camera shake: Benchmark-\\ning blind deconvolution with a real-world database. In\\nComputer Vision–ECCV 2012: 12th European Conference\\non Computer Vision, Florence, Italy, October 7-13, 2012,\\nProceedings, Part VII 12, pages 27–40. Springer, 2012.\\n[Kupyn et al., 2018] Orest Kupyn, V olodymyr Budzan,\\nMykola Mykhailych, Dmytro Mishkin, and Ji ˇr´ı Matas.\\nDeblurgan: Blind motion deblurring using conditional\\nadversarial networks. In Proceedings of the IEEE confer-\\nence on computer vision and pattern recognition , pages\\n8183–8192, 2018.\\n[Kupyn et al., 2019] Orest Kupyn, Tetiana Martyniuk, Junru\\nWu, and Zhangyang Wang. Deblurgan-v2: Deblurring\\n(orders-of-magnitude) faster and better. In Proceedings of\\nthe IEEE/CVF international conference on computer vi-\\nsion, pages 8878–8887, 2019.\\n[Lai et al., 2016] Wei-Sheng Lai, Jia-Bin Huang, Zhe Hu,\\nNarendra Ahuja, and Ming-Hsuan Yang. A comparative\\nstudy for single image blind deblurring. In Proceedings\\nof the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 1701–1709, 2016.\\n[Levenshtein and others, 1966] Vladimir I Levenshtein et al.\\nBinary codes capable of correcting deletions, insertions,\\nand reversals. In Soviet physics doklady, volume 10, pages\\n707–710. Soviet Union, 1966.\\n[Levin et al., 2009] Anat Levin, Yair Weiss, Fredo Durand,\\nand William T Freeman. Understanding and evaluating\\nblind deconvolution algorithms. In 2009 IEEE conference'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='on computer vision and pattern recognition , pages 1964–\\n1971. IEEE, 2009.\\n[Li et al., 2023] Haoying Li, Ziran Zhang, Tingting Jiang,\\nPeng Luo, Huajun Feng, and Zhihai Xu. Real-world deep\\nlocal motion deblurring. In Proceedings of the AAAI Con-\\nference on Artificial Intelligence, volume 37, pages 1314–\\n1322, 2023.\\n[Liu et al., 2020] Wei Liu, Qiong Yan, and Yuzhi Zhao.\\nDensely self-guided wavelet network for image denoising.\\nIn Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition Workshops , pages 432–\\n433, 2020.\\n[Liu et al., 2021] Ze Liu, Yutong Lin, Yue Cao, Han Hu,\\nYixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\\nSwin transformer: Hierarchical vision transformer using\\nshifted windows. In Proceedings of the IEEE/CVF in-\\nternational conference on computer vision, pages 10012–\\n10022, 2021.\\n[Nah et al., 2017] Seungjun Nah, Tae Hyun Kim, and Ky-\\noung Mu Lee. Deep multi-scale convolutional neural net-\\nwork for dynamic scene deblurring. In Proceedings of the\\nIEEE conference on computer vision and pattern recogni-\\ntion, pages 3883–3891, 2017.\\n[Noroozi et al., 2017] Mehdi Noroozi, Paramanand Chan-\\ndramouli, and Paolo Favaro. Motion deblurring in the\\nwild. In Pattern Recognition: 39th German Conference,\\nGCPR 2017, Basel, Switzerland, September 12–15, 2017,\\nProceedings 39, pages 65–77. Springer, 2017.\\n[Ramakrishnan et al., 2017] Sainandan Ramakrishnan,\\nShubham Pachori, Aalok Gangopadhyay, and Shan-\\nmuganathan Raman. Deep generative filter for motion\\ndeblurring. In Proceedings of the IEEE international con-\\nference on computer vision workshops, pages 2993–3000,\\n2017.\\n[Ren et al., 2017] Dongwei Ren, Wangmeng Zuo, David\\nZhang, Jun Xu, and Lei Zhang. Partial deconvolution with\\ninaccurate blur kernel. IEEE transactions on image pro-\\ncessing, 27(1):511–524, 2017.\\n[Ren et al., 2018] Wenqi Ren, Jiawei Zhang, Lin Ma, Jin-\\nshan Pan, Xiaochun Cao, Wangmeng Zuo, Wei Liu, and\\nMing-Hsuan Yang. Deep non-blind deconvolution via gen-\\neralized low-rank approximation. Advances in neural in-\\nformation processing systems, 31, 2018.\\n[Rim et al., 2020] Jaesung Rim, Haeyun Lee, Jucheol Won,\\nand Sunghyun Cho. Real-world blur dataset for learning\\nand benchmarking deblurring algorithms. In Computer\\nVision–ECCV 2020: 16th European Conference, Glas-\\ngow, UK, August 23–28, 2020, Proceedings, Part XXV 16,\\npages 184–201. Springer, 2020.\\n[Shan et al., 2008] Qi Shan, Jiaya Jia, and Aseem Agarwala.\\nHigh-quality motion deblurring from a single image. Acm\\ntransactions on graphics (tog), 27(3):1–10, 2008.\\n[Shen et al., 2019] Ziyi Shen, Wenguan Wang, Xiankai Lu,\\nJianbing Shen, Haibin Ling, Tingfa Xu, and Ling Shao.\\nHuman-aware motion deblurring. In Proceedings of the\\nIEEE/CVF International Conference on Computer Vision,\\npages 5572–5581, 2019.\\n[Shi et al., 2016] Baoguang Shi, Xiang Bai, and Cong Yao.\\nAn end-to-end trainable neural network for image-based\\nsequence recognition and its application to scene text\\nrecognition. IEEE transactions on pattern analysis and\\nmachine intelligence, 39(11):2298–2304, 2016.\\n[Simonyan and Zisserman, 2014] Karen Simonyan and An-\\ndrew Zisserman. Very deep convolutional networks\\nfor large-scale image recognition. arXiv preprint\\narXiv:1409.1556, 2014.\\n[Sun et al., 2013] Libin Sun, Sunghyun Cho, Jue Wang, and\\nJames Hays. Edge-based blur kernel estimation using\\npatch priors. In IEEE international conference on com-\\nputational photography (ICCP), pages 1–8. IEEE, 2013.\\n[Sun et al., 2015] Jian Sun, Wenfei Cao, Zongben Xu, and\\nJean Ponce. Learning a convolutional neural network for\\nnon-uniform motion blur removal. In Proceedings of the\\nIEEE conference on computer vision and pattern recogni-\\ntion, pages 769–777, 2015.\\n[Tao et al., 2018] Xin Tao, Hongyun Gao, Xiaoyong Shen,\\nJue Wang, and Jiaya Jia. Scale-recurrent network for deep\\nimage deblurring. In Proceedings of the IEEE conference\\non computer vision and pattern recognition , pages 8174–\\n8182, 2018.\\n[Wang et al., 2018] Xintao Wang, Ke Yu, Chao Dong, and\\nChen Change Loy. Recovering realistic texture in image\\nsuper-resolution by deep spatial feature transform. In Pro-\\nceedings of the IEEE conference on computer vision and\\npattern recognition, pages 606–615, 2018.\\n[Whyte et al., 2012] Oliver Whyte, Josef Sivic, Andrew Zis-\\nserman, and Jean Ponce. Non-uniform deblurring for\\nshaken images. International journal of computer vision ,\\n98:168–186, 2012.\\n[Xu et al., 2018] Zhenbo Xu, Wei Yang, Ajin Meng, Nanxue\\nLu, Huan Huang, Changchun Ying, and Liusheng Huang.\\nTowards end-to-end license plate detection and recogni-\\ntion: A large dataset and baseline. In Proceedings of the\\nEuropean conference on computer vision (ECCV) , pages\\n255–271, 2018.\\n[Zhang et al., 2023] Xiang Zhang, Lei Yu, Wen Yang,\\nJianzhuang Liu, and Gui-Song Xia. Generalizing event-\\nbased motion deblurring in real-world scenarios. In Pro-\\nceedings of the IEEE/CVF International Conference on\\nComputer Vision, pages 10734–10744, 2023.\\n[Zhao et al., 2022] Suiyi Zhao, Zhao Zhang, Richang Hong,\\nMingliang Xu, Yi Yang, and Meng Wang. Fcl-gan: A\\nlightweight and real-time baseline for unsupervised blind\\nimage deblurring. In Proceedings of the 30th ACM In-\\nternational Conference on Multimedia, pages 6220–6229,\\n2022.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 0, 'page_label': '26667', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='Received 13 February 2023, accepted 2 March 2023, date of publication 10 March 2023, date of current version 21 March 2023.\\nDigital Object Identifier 10.1 109/ACCESS.2023.3255641\\nAdaptive Lightweight License Plate Image\\nRecovery Using Deep Learning Based on\\nGenerative Adversarial Network\\nWUTTINAN SEREETHAVEKUL\\n AND MONGKOL EKPANYAPONG\\nDepartment of Industrial Systems Engineering, School of Engineering and Technology, Asian Institute of Technology, Khlong Luang, Pathum Thani 12120,\\nThailand\\nCorresponding author: Wuttinan Sereethavekul (st118978@ait.asia)\\nThis work was supported in part by the Thailand Science Research and Innovation (TSRI) under Grant RDG6250036.\\nABSTRACT Many Convolutional Neural Networks (CNNs) methods have already surpassed traditional\\napproaches to image restoration tasks. Those CNNs models were usually designed to enhance single tasks\\nsuch as an image resolution (super-resolution) or image denoising, but we came up with unconventional\\ngoals, that is, multiple recovery tasks from a single network design. Although the Transformer design has\\nrecently gained attention in image recovery tasks, they are too slow. In order to work with license plate\\nimages from a traffic camera stream, the system has to be responsive. So, we proposed a fast and lightweight\\ndeep learning-based data recovery system using a Generative Adversarial Network (GAN) principle named\\nLicense Plate Recovery GAN (LPRGAN). The design has a proposed encoder-decoder style inspired by an\\nautoencoder aided by dual classification networks. This style suits problem-characteristic learning because\\nstrong contextual information is retrieved from the down-scaled representations. This proposed system has\\nthree main features such as identifying a problem, data recovery, and fail-safe mechanism. The core of\\nsystem is a data recovery unit (LPRGAN), is used to recover license plate images from multiple degraded\\ninput images. Most existing image restoration systems do not have self-awareness, leading to an inefficiency\\nproblem. Unlike existing works, this system has anomaly detection and will only process on a degraded input,\\nreducing workload overhead, improving efficiency and a fail-safe feature that prevents an unexpected bad\\noutput. Hence, it is light enough to deploy on a low-power machine such as edge computing devices, opening\\nup new possibilities in on-device computing. Our proposed research can recover several degraded problems\\nup to 720p resolution at 15 frames per second on a single graphic card, 256 × 128 resolution at 17 frames\\nper second on a CPU-only workstation machine, or 7 frames per second on an ultra-low-power tablet PC.\\nINDEX TERMS Data recovery, deep learning, generative adversarial networks, image and video recovery,\\nmachine learning, neural networks, video streaming.\\nI. INTRODUCTION\\nTraffic cameras are now becoming essential tools in part\\nof transportation systems. They are used to monitor traffic\\nactivity and accidents or to detect illegal vehicles on the road.\\nThese cameras help in traffic police workforce reduction.\\nNot only that, a traffic camera can be deployed in very\\nremote areas where traffic police are hard to reach. The\\ntraffic monitoring system can provide a full country-wide\\nThe associate editor coordinating the review of this manuscript and\\napproving it for publication was Yi Zhang\\n.\\nroad area coverage. This traffic monitoring is a worldwide\\nstandard practice to enhance road security and safety. The\\nmost important aspect of traffic monitoring is vehicle license\\nplate reading, such as in road accidents or illegal road vehicles\\nso that police officers can identify them. However, there are\\nmany shortcomings in reading a license plate. Examples are\\noccasionally corrupted data within a streaming frame, low\\nlight area, slow shutter camera speed that is not fast enough\\nto track a plate, or an intention to save disk space by reducing\\nrecording bitrate, resulting in low-quality media files. A sim-\\nple form of data corruption can be seen as a blocky-looking\\nVOLUME 11, 2023\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ 26667'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 1, 'page_label': '26668', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nor blocky artifact due to low bitrate streaming. Fig. 1 shows\\na bitrate requirement in a video streaming at different resolu-\\ntions.\\nTo overcome those mentioned problems, this research has\\nstudied a deep learning principle which is part of artifi-\\ncial intelligence (AI), to address such problems. It has a\\nunique ability to learn and observe input patterns used for\\nmany complex tasks. This technique is suitable for improv-\\ning low-quality images by learning from high-quality ones.\\nThere are many CNNs available that have their benefits. For\\nexample, an autoencoder [1] is useful when an input has a\\nnoise in the image or audio. It is used to remove noise in\\na signal. An autoencoder also has a convolutional version\\ncalled a convolutional autoencoder. It has multiple convolu-\\ntion computation layers as part of a network. The U-Net [2] is\\na network that shares similarities to an autoencoder but even\\nmore complex layers. It consists of two parts, a contracting\\npath, and an expansive path. There are many GAN [3] varia-\\ntions such as Deep Convolutional GAN (DCGAN) [4], Con-\\nditional GAN (CGAN) [5], and CycleGAN [6]. The DCGAN\\nis usually used to generate a new image from random input\\ndata (normal distribution). A downside of DCGAN is that its\\noutput cannot be controlled, so it is impossible to specify\\nan output appearance or class. This reason is why CGAN\\noffers more control over DCGAN. The CycleGAN is used to\\nswap between two input domains. A GAN relies heavily on\\nCNNs because many CNNs models have posed a potential for\\nimage recovery and enhancement. They learn from a pattern\\nfrom big datasets. Most CNNs setups usually have either\\nan encoder-decoder style or a single-size style. In the first\\ncase, it utilizes a down-sampling method to map an input to\\na lower-resolution representation and then applies a reverse\\noperation (up-sampling) to map to an original resolution. This\\noperation is a good way to learn input context by down-\\nsampling resolution, but a downside is that the fine spatial\\ndetails are lost in the process. Thus, an output usually has\\nlower details when compared to an original, making this\\nstyle a lossy process. In the latter case, a single-scaling style\\nutilizes feature processing. It does not contain any down-\\nsampling operation, producing images with more fine details.\\nNevertheless, this single-scale style is commonly less effec-\\ntive in learning a pattern of contextual information due to a\\nlimited representative resolution. So these two examples both\\nhave their benefits and drawbacks.\\nIt is a position-sensitive procedure where pixels from two\\nsources need to be matched in a recovery learning process.\\nThe first source is a reference image, and the second is a\\ndistorted image. A slight shift in pixel position between them\\nis undesired because a distorted pattern must be the only\\ncomponent in that image. Otherwise, a true pattern will be\\nmixed up with a dislocation pixel, making it difficult to learn\\na degradation pattern for a specific problem. Therefore, both\\nreference and degraded input images have to be perfectly\\naligned. Then the learning can even further benefit from a\\nlarge context dataset, i.e., image scaling or problem variation.\\nOur goal is to build a GAN model that is light and fast,\\nFIGURE 1. Bitrate required for video streaming at different resolution.\\ncontaining fewer possible layers and complexions. It aims to\\nhelp a license plate reading in several degraded situations.\\nThe main difference from other works is that many cur-\\nrent works including recently released, Transformer design,\\nusually have a big nonsingular network consisting of several\\nconvolutional and other neural network layers, making them\\nslow and heavy on machine resources. They are unable to\\nbe deployed on edge computing machines since it consumes\\ntoo much processing power and memory. The other dif-\\nferences are some GAN-based models usually cover either\\nimage super-resolution or denoising study areas which are\\nnot the main problem of license plate reading. They are also\\nnot flexible, meaning one network design for one particular\\ntask. Thus, we proposed a GAN-based work that is able to\\nsolve several real world situations by using only one network\\nsetup. It features a GAN, contains a modified generator and\\ndiscriminator, resulting in just under a million parameters\\n(for 256 × 128 image recovery). The image recognition and\\nimage recovery systems work together to detect and recover\\na degraded input, greatly improve system efficiency, instead\\nof a simple barebone system.\\nThis study helps to contribute by developing a new sim-\\nple, light, and fast yet practical license plate image quality\\nrecovery, covering low bitrate, low light, and motion blur\\nsituations with a single network setup. It has a faster leap than\\nother approaches, so it is suitable for real-time stream pro-\\ncessing. Thus, it can work on a typical workstation machine to\\nenhance monitoring potential since the system is simple and\\nlightweight. This study also includes comprehensive experi-\\nments on both simulated and real-world license plate image\\nbenchmarks - mathematical and visual inspection, including\\nlocal and international plates.\\nII. RELATED WORKS\\nImage processing has been developed over the past years,\\nand one that benefits are traffic monitoring. Traffic moni-\\ntoring involves the content transmission from a camera unit\\nto a monitoring unit. An interruption in transmission, such\\nas unavailable bandwidth, can produce poor-quality video\\ntransmission. Many have tried to improve content trans-\\nmission, such as Petrov et al. [7] proposed a technique to\\nreduce a blocking artifact by detecting a blocking artifact in\\na macroblock (8 × 8 pixels) and a displaced blocking effect.\\nThen apply a blocking artifact reduction using their proposed\\n26668 VOLUME 11, 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 2, 'page_label': '26669', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nfilters. They targeted mobile platforms with low bitrate video\\nfocused on operation speed. Dar et al. [8] presented a work\\nthat analyses only one slice of low bitrate video compression.\\nThis paper benefits from applying a spatio-temporal down-\\nscaling, i.e., reduction of frame rate and frame size, before the\\ncompression and a corresponding up-scaling afterward. They\\nleft H.264 codec untouched. Their work covers 16 ×16 mac-\\nroblock from very low (2 bits/slice) through low (around\\n30 bits/slice) and up to high (210 bits/slice) bitrate. So they\\npresented that the downsampling video before compression\\ntook place is better. Li et al. [9] proposed a new five layers\\nCNN-based block up-sampling scheme for intra-frame cod-\\ning. A block can be down-sampled before being compressed\\nby normal intra-coding and then up-sampled to its original\\nresolution. This way differs from the previous hand-craft\\ndown and upsampling because this paper is based on train-\\ning a CNN. A new CNN structure for upsampling features\\nthe deconvolution of feature maps, multi-scale fusion, and\\nresidue learning, making the network compact and efficient.\\nThey also designed different networks for the up-sampling\\nof luma and chroma components, respectively, where the\\nchroma up-sampling CNN utilizes the luma information to\\nboost its performance. This scheme is built into HEVC refer-\\nence software. Resulting in an average 5.5% BD-rate reduc-\\ntion on common test sequences and an average 9.0% BD-\\nrate reduction on ultra-high definition (UHD) test sequences.\\nInstead of using traditional downsampling, they presented\\na CNN-based sampling scheme. Lin et al. [10] proposed\\nan adaptive downsampling-based coding model to improve\\nthe low bitrate compression efficiency of high-efficiency\\nvideo coding (HEVC). They use motion estimation to find\\nthe most similar blocks between upscaled Non-Key Frames\\n(NKFs) and associated high-resolution Key Frames (KFs).\\nThen, an adaptive patching-based method is used to warp\\nthe low-quality NKF blocks with the high-quality KF blocks.\\nTheir experimental results demonstrate significant improve-\\nments compared to existing methods but only work on HEVC.\\nYang et al. [11] worked on enhancing low bitrate HEVC video\\nquality. They enhanced the visual quality of HEVC videos\\non the decoder side. So they proposed a Quality Enhance-\\nment Convolutional Neural Network (QE-CNN) method that\\ndoes not require any encoder modification to achieve qual-\\nity enhancement for HEVC. In particular, their QE-CNN\\nmethod learns QE-CNN-I and QE-CNN-P models to reduce\\nthe distortion of HEVC I and P/B frames, respectively. This\\nmethod differs from the existing CNN-based quality enhance-\\nment approaches, which only handle intra-coding distor-\\ntion and are thus unsuitable for P/B frames. They claimed\\ntheir method validates that the QE-CNN method effectively\\nenhances quality for both I and P/B frames of HEVC videos.\\nThese mentioned works feature both non-machine learning\\nand machine learning forms. Despite the benefit of media\\ntransmissions that enhance streaming quality, resulting in a\\nbetter video output quality, they are restricted to a specific\\nvideo codec.\\nOn the other hand, some previous works tried to improve\\nthe image processing aspect. For instance, Zhu et al. [12]\\nmade use of the CycleGAN to do an image-to-image transla-\\ntion (X − →Y ) which they called the ‘‘pix2pix’’ project [13].\\nHe and his team created this project to swap the texture\\nof two things like, zebra and horse, and swap between two\\npictures style, such as a photograph and an art style. Although\\nswapping two pictures could benefit some areas, such as\\nswapping a noisy image with a clean one in the de-nosing\\ntask, it does not necessarily mean imagery improvement. Guo\\net al. [14] presented a way to improve image denoising with\\nadditive white Gaussian noise (AWGN) by training a con-\\nvolutional blind denoising network (CBDNet) with a more\\nrealistic noise model and real-world noisy-clean image pairs.\\nAlso, they provided an interactive strategy to rectify denois-\\ning results conveniently. A noise estimation subnetwork with\\nasymmetric learning to suppress the underestimation of noise\\nlevel is embedded into CBDNet. Wang et al. [15] built a blind\\nface restoration system. This Generative Facial Prior (GFP)\\nis incorporated into the face restoration process via spatial\\nfeature transform layers, achieving a good balance of real-\\nness and fidelity. The GFP-GAN could jointly restore facial\\ndetails and enhance colors with just a single forward pass.\\nKupyn et al. [16] presented DeblurGAN-v2, a newer ver-\\nsion of DeblurGAN that considerably boosts state-of-the-art\\ndeblurring performance while being much more flexible and\\nefficient. It was claimed to be faster and better than v1. It is\\nmade of GAN with a backend such as Inception ResNet v2.\\nZamir et al. [17] presented the MIRNet, an image restoration\\nmodel. A proposed architecture maintains high-resolution\\nrepresentations throughout the entire network and receives\\ninformation from the low-resolution representations. Existing\\nCNN-based methods usually operate just on full-resolution\\nor low-resolution representations. Although this network\\npacks much functionality, it also contains several modules.\\nThey used three Recursive Residual Groups (RRGs), each\\nof which contains two Multi-scale Residual Block (MRBs),\\nand each MRB also contains three streams. Zamir et al. [18]\\nalso proposed an efficient Transformer model for capturing\\nlong-range pixel interactions, while remaining applicable to\\nlarge images. It can restore images on several tasks. Liang\\net al. [19] proposed a SwinIR model for image restora-\\ntion based on the Swin Transformer. It consists of shallow\\nfeature extraction, deep feature extraction, and high-quality\\nimage reconstruction. Recently, a biomedical paper utilizing\\nGAN from Zhang et al. [20] presented a method of increas-\\ning contrast in CT scanning images for clinical diagnosis.\\nTheir MALAR system is based on CycleGAN. It has dual\\nGANs that work on ultra-low-dose-ICM aorta CT (UDCT)\\nand low-dose-ICM aorta CT (LDCT) images. However, this\\napproach outputs DICOM format and does not work on\\nstandard RGB images. Wu et al. [21] presented an article\\nfor tomographic image reconstruction in a sparse-view CT\\nscan. They proposed a Dual-domain Residual-based Opti-\\nmization NEtwork (DRONE). It consists of three modules\\nVOLUME 11, 2023 26669'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 3, 'page_label': '26670', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nfor embedding, refinement, and awareness. The results from\\nthe embedding and refinement modules in the data and\\nimage domains are regularized for optimized image qual-\\nity in the awareness module, which ensures the consistency\\nbetween measurements and images with the kernel aware-\\nness of compressed sensing. Wu et al. [22] also presented\\na Deep Embedding-Attention-Refinement (DEAR) network\\nto achieve good images from high sparse-view levels in CT\\nreconstruction tomography imaging. This study was based\\non the DRONE and released later. DEAR also consists of\\nthree modules including deep embedding, deep attention, and\\ndeep refinement. The results demonstrate the efficiency of\\nthe DEAR in edge preservation and feature recovery in deep\\ntomographic reconstruction.\\nAlthough this proposed approach is also in an image pro-\\ncessing area, unlike those existing works, because it does\\nnot need a codec modification and is not limited to one\\nspecific media, i.e., image/video, or one specific codec, i.e.,\\nJPG/PNG/A VC/HEVC. It also does not tie to one specific\\ntask, the proposed system is generalized for several tasks by\\nswapping a training dataset. Finally, it utilizes a lightweight\\nGAN design targeting low-power devices for a fast license\\nplate image recovery operation. Those designs are too com-\\nplex and require a high-performance system to run.\\nIII. METHODOLOGY\\nA. THE DESCRIPTION OF THE PROBLEMS\\nAs mentioned earlier, there are many common problems in\\ntraffic camera streams. Most seen problems were categorized\\ninto each group. A grouping is vital because the detection\\nsystem can provide a correspond description of an input to\\nthe recovery system precisely. Each group has its own label\\nand was used to train a detection system. Below is a list of\\nproblems that this study focuses on.\\n• Low Bitrate Dataset - Represents network congestion\\nand low bandwidth network problems\\n• Low Light Dataset - Represents low light and nighttime\\nsituations\\n• Motion Blur - Horizontal Dataset - Represents slow\\ncamera shutter speed and speedy object problems\\n• Motion Blur - Vertical Dataset - Represents slow cam-\\nera shutter speed and camera shaking due to vibration\\nproblems\\n• Normal Dataset (Normal/Good Condition) - Represents\\na high-quality, daylight situation in an ideal case\\nIn the case of low bitrate problems where it is feasible to\\narrange them into sub-groups, these ranges refer to the JPG\\ncompression ratio range, which is mapped to a rating score\\nsystem. This rating system will be useful in cases where a\\nregular mathematical picture assessment is not possible to\\ncalculate.\\n• 1-Star: 0-20 JPG Quality Setting (Poorest Looking)\\n• 2-Star: 20-40 JPG Quality Setting\\n• 3-Star: 40-60 JPG Quality Setting\\n• 4-Star: 60-80 JPG Quality Setting\\n• 5-Star: 80-100 JPG Quality Setting (Best Looking)\\nThe above descriptions are used in a deep learning classifica-\\ntion, which is supervised training [23]. It is trained to detect\\nand categorize an occurring problem and acknowledges a\\ndifference between each compression ratio range to assess an\\noutput product.\\nB. DATASET PREPARATION AND PROCESSING\\nThai license plate images are the dataset used in this research\\nand were provided by the AI Center, Asian Institute of Tech-\\nnology. There are 16,194 images in total, and they were sep-\\narated into 14,500 train images and 1,694 evaluation images.\\nHowever, due to ownership and privacy infringement, these\\nimages cannot be disclosed here. Due to the raw datasets\\nbeing relatively small in resolution, all images after resizing\\nwere capped at 256 × 128 pixels, so a network is primarly\\ndesigned to fit this image size. They were then processed into\\neach category using a random filter to replicate real-world\\nproblems such as low JPG quality level for low bitrate prob-\\nlems or low brightness for a low light problem. These were\\nprepared as the first step before training a model, and their\\ncategory structure is shown below. In Fig. 2a-Fig. 2f, display\\nhistogram plots on each equal probability random distribution\\nof filter level.\\n1) LOW BITRATE DATASET\\nUsing OpenCV2 to rewrite JPG images by setting a quality\\nvalue between 0 to 20 to match a 1-Star rating system range.\\n2) LOW LIGHT DATASET\\nUsing PIL ImageEnhance to drop an original image bright-\\nness from 100% to between 10% to 50% of the original value.\\n3) HORIZONTAL MOTION BLUR DATASET\\nA custom 2D kernel to create a motion blur filter [24] is\\nused with an image to create a motion blur image. Kernel\\nfilter size varies between 10 × 10-40 × 40. A blur kernel\\nfilter size is a pixel-shifting distance. For example, using\\n15 blur kernel size gives a 15-pixel shifting distance from the\\norigin. This method creates a motion blur along the X-axis\\n(0 degrees). A motion blur kernel filter has a formula as\\nin (1).\\nh = 1\\nm (1)\\n• h is the horizontal kernel value\\n• m is the size of the kernel\\nHm×m =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0 0 · · ·0\\n0 0 · · ·0\\n..\\n. .\\n.\\n. · · · .\\n.\\n.\\nhm/2 hm/2 · · ·hm/2\\n.\\n.\\n. .\\n.\\n. ... .\\n.\\n.\\n0 0 · · ·0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\n\\uf8fa\\uf8fb\\n26670 VOLUME 11, 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 4, 'page_label': '26671', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nFIGURE 2. Histogram plot on each problem type dataset - (a) Low Bitrate train set, (b) Low Bitrate test set, (c) Low light train set, (d) Low\\nlight test set, (e) Motion blur train set and (f) Motion blur test set.\\n4) VERTICAL MOTION BLUR DATASET\\nThis vertical motion blur is the same idea as a horizontal\\nmotion blur but is different in the filter kernel. Again, kernel\\nfilter size varies between 10 × 10 − 40 × 40. This method\\ncreates a motion blur along the Y-axis (90 degrees). A motion\\nblur kernel filter has a formula as in (2).\\nv = 1\\nm (2)\\n• v is a vertical kernel value\\n• m is the size of the kernel\\nVm×m =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0 0 · · ·vm/2 · · ·0\\n0 0 · · ·vm/2 · · ·0\\n...\\n... · · ·\\n..\\n. ... .\\n.\\n.\\n0 0 · · ·vm/2 · · ·0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n5) NORMAL DATASET\\nA reference dataset, i.e., normal-looking, high-quality, and\\ngood-condition images. This dataset is used in recovery and\\nmeasurement processes.\\nC. PROPOSED SYSTEM, MODEL AND LAYERS\\nA proposed system is an end-to-end system combining\\ntwo image classifications and one image recovery into one\\napplication. It helps traffic monitoring system to detect the\\nanomaly and efficiently recover a bad one when needed.\\nA detection system is built from CNN image classifica-\\ntion and used to handle an incoming stream frame, detect a\\ndegraded frame and select a matching pre-trained model for\\na recovery system. This detection system can be found as a\\n‘‘Detector’’ in Fig. 3. It is based on VGG-16 [25] network but\\nwith a reduced layers count, resulting in three convolutional\\nlevels. The kernel size used in this network is 2. Its output\\n(Image Description) is used in ‘‘Model Selector’’. It currently\\nhas five description output classes plus five star-rating classes\\nin low-bitrate situations. This unit matches an input descrip-\\ntion with a predefined description found in the description\\nof the problems to select a proper recovery model for that\\ninput, i.e., low bitrate input needs trained low bitrate model.\\nA selected model will then be passed to a recovery system.\\nA recovery system (LPRGAN) is a pixel-based license\\nplate image recovery system. This proposed network features\\na reduction layer count configuration, including replacing\\nthe max pooling layer with a convolutional stride to speed\\nup model performance [26]. After receiving an input image\\nand its corresponding trained model from a model selector,\\nthe LPRGAN uses a trained model to recreate a high-quality\\nversion of the degraded input. Thus, this step is called the\\nrecovery process. The LPRGAN has two parts, a genera-\\ntor, and a discriminator. A generator is based on a convo-\\nlutional autoencoder but optimized with a less complicated\\nconfiguration. The main benefit of an autoencoder is that\\nit can down-sampling data while preserving a significant\\nrepresentation of original data. All max-pooling layers from\\nthe original version are replaced with striding. An upscale\\nprocess is also replaced with a convolutional transpose layer\\ninstead of the original upsampling layer. A generator layer\\nconfiguration can be found in Fig. 4. A discriminator is\\nconfigured with a light VGG-16 version that also features a\\nmax-pooling replacement. Both generator and discriminator\\nhave kernel size 3 in the main layer, except the last one (output\\nlayer) in the generator has kernel size equal to 1. They also\\nfeature a sigmoid activation function instead of a hyperbolic\\ntangent (tanh) for better output value coverage. These setups\\nmake a network more compact and responsive. A generated\\nproduct from the LPRGAN will later be fed to a qualifier for\\nevaluation purposes.\\nAfter a recovery step, a qualifier will validate a result with\\na rating score according to its problem type. This validation\\ncomes in handy when a degraded input is severely distorted\\nand cannot be recovered. Since most existing works do not\\nreport on this case where the input is too distorted beyond\\nGAN recovery capability, It is usually because the output will\\nresult in even worse quality. This occasion can sometimes\\nhappen when GAN could not reproduce the desired output\\nimage due to too much damage in an input image. Because\\nof this inadequacy, our proposed system has one final touch,\\na fail-safe mechanism using a ‘‘Qualifier’’. It will determine\\nwhich final result should be presented, either from a degraded\\ninput or an unrecovered result. This action is to prevent the\\nend user gets an unpleasant disaster output. A qualifier has\\nthe same CNN image classification setup but is trained with\\na different purpose, featuring three convolutional levels. The\\nkernel size used in this network is 2. It is used to evaluate and\\nvalidate a result at the end of the process.\\nD. DATA RECONSTRUCTION USING LPRGAN\\nThe basic idea in any image compression [27] is that the\\nmore the compression ratio is, the more space-saving, but it\\nresults in a blocky, bad-looking image. In the JPEG compres-\\nsion stage [28], once an image is translated from the spatial\\n2D domain into the frequency domain via DCT (Discrete\\nVOLUME 11, 2023 26671'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 5, 'page_label': '26672', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nFIGURE 3. Overview of proposed system block diagram.\\nCosine Transformation), a low to mid-frequency is usually\\ndiscarded to reduce file size, leaving out a high-frequency\\narea untouched. This high-frequency signal comes from a\\nsharp edge in the image. The reason to leave a sharp edge\\narea in the image is that human eyes are sensitive to them.\\nRemoving the rest would not affect on final image in terms\\nof visuals. This mentioned principle is also adopted for video\\ncompression with inter and intra-coding to save a bitrate.\\nOn the other hand, GAN has the unique ability to gener-\\nate fake data based on training in the deep learning world.\\nGAN is used to learn a data loss pattern from a compression\\nmechanism in this case. In the training process, pairs of\\ngood/ordinary-looking images (references) (x ) alongside real\\nlabels (y) and pairs of fake images (x ∗) with fake labels\\n(y∗) are used to train a discriminator (D) to learn on how\\nto differentiate a good and a bad. Then, prepared degraded\\ninput images (X ) with inverted real labels (y) are used in a\\ngenerator to generate a fake image but due to the fact that a\\ngenerator cannot be trained. So to train a GAN (G), the input\\n(X) is passed through a series of Convolutional 2D layers\\n(Conv2D), and their dimensions are halved in every Conv2D\\nlayer until they start to be upscaled back in Convolutional\\n26672 VOLUME 11, 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 6, 'page_label': '26673', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nFIGURE 4. LPRGAN generator layers visualization.\\n2D Transpose layers (Conv2DTranspose). Conv2D is used to\\nextract an input feature, called feature extraction, to represent\\nits distinctive. In contrast, Conv2DTranspose, an invert of\\nConv2D, is used to rebuild pixel information based on the\\nextracted feature. After a generator generates a new image,\\na discriminator will compare a generated image (x ∗) with an\\noriginal/reference image (x ) to decide whether a generated\\nimage looks realistic enough. This way, a discriminator helps\\nimprove a generator’s performance to generate an even better\\nresult as if it was trained. Ultimately, a generator target is to\\ngenerate a realistic image that a discriminator can no longer\\ntell them apart. This recovery system predicts and recovers\\ndata from compression or degraded loss to reduce a blocky\\nartifact, smooth out a color gradient, and brighten or reduce a\\nblurry image. Thus, this concept can help to repair degraded\\nproblems found in real life.\\nE. TRAINING PROCESS\\nA training result can be mediocre if there are insufficient\\nresources such as computation power, dataset, or time avail-\\nable. A sufficient amount of training datasets is crucial since\\nthey directly impact a training performance. The more variety\\nof dataset available, usually the more system performance is\\nlikely to be. Also, a reasonable time is needed for the model\\nto fit perfectly (balanced-fit). This research also features a\\ndecay learning rate [29] to accelerate a begining of training\\nprocess, as shown in (3).\\nLR = LR0 ∗ 1\\n(1 + LRDecay ∗ N) (3)\\n• LR is the current learning rate in that iteration\\n• LR0 is an initial constant learning rate value\\n• LRDecay is a decay value used to control how fast the\\nlearning rate decreases\\n• N is the current iteration number\\nThe training parameters except the β1 value are common to\\nall experiments. The detail of values used in these studies is\\nbelow.\\n• Main Kernel Size = 3\\n• Initial Learning Rate = 0.0001\\n• Optimizer = Adam (Low Bitrate β1 = 0.5, Low Light\\nβ1 = 0.3, Horizontal Motion Blur β1 = 0.7, Vertical\\nMotion Blur β1 = 0.5)\\n• Iterations = 200\\nAlgorithm 1LPRGAN Training\\n1: procedure Train GAN\\n2: for iteration = 1, 2, 3, . . .do\\n3: for batch = 1, 2, 3, . . .do\\n4: Load random image samples x in a mini-batch\\nmanner\\n5: Train the discriminator on loaded samples\\nwith its real labels (x , y)\\n6: Compute the discriminator loss D(x) and\\nbackpropagation a total error θ(D) to minimize a loss\\n7: Using the generator to generate fake images\\nfrom input G(x′) = x∗\\n8: Train the discriminator on fake images and\\nfake labels sample (x ∗, y∗)\\n9: Compute the discriminator loss D(x∗) and\\nbackpropagation a total error θ(D) to minimize a loss\\n10: Train the GAN by using low-quality input\\nimages with real labels sample (x ′, y)\\n11: Compute and update GAN Loss θ(G) to max-\\nimize a loss\\n12: end for\\n13: LR ← LRiteration\\n14: Load different random image samples x0 in a\\nmini-batch\\n15: Evaluate the discriminator on real images set\\n(x0, y)\\n16: Generate fake images from input G(x′\\n0) = x∗\\n0\\n17: Evaluate the discriminator on fake images set\\nwith fake labels (x ∗\\n0 , y∗)\\n18: psnr ← PSNR(x, x∗) and psnr0 ← PSNR(x0, x∗\\n0\\n19: scc ← SCC(x, x∗) and scc0 ← SCC(x0, x∗\\n0 )\\n20: ssim ← SSIM(x, x∗) and ssim0 ← SSIM(x0, x∗\\n0 )\\n21: vif ← VIF(x, x∗) and vif0 ← VIF(x0, x∗\\n0 )\\n22: if psnr0 > saved_psnr0 ∨ scc0 > saved_scc0 ∨\\nssim0 > saved_ssim0 ∨ vif0 > saved_vif0 then\\nsaved_alue ← new_value\\n23: end if\\n24: end for\\n25: fid ← FID(x0, x∗\\n0 )\\n26: end procedure\\nF. METRIC MEASUREMENTS DEFINITION\\nIn order to measure the quality of any given image, there\\nare two major quality assessments. The first is a human\\nperception or human visual system (HVS), and the second is\\nmathematical measurements. Using a human is hard to get a\\nconsistent result since each person has a different perception,\\nand the human eye is difficult to tell a slight change in\\nbetween images. So a deep learning unit (image qualifier) is\\nused to assess in HVS instead. On the other hand, a mathemat-\\nical assessment in this study includes these measurements,\\nFID, PSNR, SCC, SSIM, and VIF. FID is explicitly designed\\nto assess a non-authentic, generated image. This FID score is\\noffered from Pytorch-fid [30] package. The rest of the metrics\\nVOLUME 11, 2023 26673'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 7, 'page_label': '26674', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nare offered by Sewar [31] package. Below list are all metrics\\nconducted in this research.\\n• FID (Fréchet Inception Distance) [32]\\n• PSNR (Peak Signal-to-Noise Ratio) [33]\\n• SCC (Spatial Correlation Coefficient) [34]\\n• SSIM (Structural Similarity Index) [35]\\n• VIF (Visual Information Fidelity) [36]\\n• File size\\n• Time Usage\\n• Render Speed\\n1) FID\\nThe Fréchet inception distance (FID) is a metric used to\\nassess the quality of images specifically created by the GAN.\\nIt compares the distribution of generated images with the\\ndistribution of real images used to train the generator. In other\\nwords, this score tells how well the GAN is from comparing\\ngenerated dataset with a training dataset. FID compares the\\nmean and standard deviation of one of the deeper layers in\\nthe Inception v3 network. These near-end layers are near\\noutput nodes that correspond to real-world objects. Thus,\\nit can mimic the human perception of similarity in images.\\nThe FID value will be 0 if paired datasets are identical, and the\\nvalue will go higher when there is more difference (deviation)\\nbetween two input datasets. The lower it is, the better. FID has\\na formula as in (4).\\nFID = ||µ1 − µ2||2 + tr(61 + 62 − 2(6\\n1\\n2\\n1 · 61 · 6\\n1\\n2\\n2 )\\n1\\n2 )\\n(4)\\n• mu_1 and mu_2 refer to the feature-wise mean of the\\nreal and generated images, i.e., 2,048 element vectors\\nwhere each element is the mean feature observed across\\nthe images.\\n• 61 and 62 are the covariance matrix for the real and\\ngenerated feature vectors\\n• ||µ1−µ2||2 refers to the sum squared difference between\\nthe two mean vectors. tr is the trace linear algebra\\noperation, i.e., the sum of the elements along the main\\ndiagonal of the square matrix.\\n2) PSNR\\nPSNR takes two inputs to calculate a signal power using the\\nMSE of the reference image from the original image. Its range\\nis usually in-between 25-48dB for an 8-bit image, where\\nhigher is better. PSNR has a formula as in (5).\\nPSNR = 10 ∗ log10( 2562\\nMSE ) (5)\\n• MSE is the mean squared error that measures the average\\nof the squares of the errors, the average squared differ-\\nence between estimated values and actual value\\n3) SCC\\nThe Spatial Correlation Coefficient calculates the spatial cor-\\nrelation coefficient score from paired images. SCC is defined\\nas a spatial concordance coefficient for second-order station-\\nary processes, and it detects a misalignment between two\\nimages. It has a value between 0 to 1. The generalised SCC\\nformula [37] is shown in (6).\\nρc(h) = 2σX σY\\nσ2\\nX + σ2\\nY\\nρXY R(h, φXY ) (6)\\n• |ρc(h)| ≤ |ρXY (h)| ≤1\\n• |ρc(h)| =0 if |ρXY (h)| =0\\n• σX is a standard deviation of X, and σY is a standard\\ndeviation of Y . Given that X and Y are two random\\nvariables.\\n• R(h, φXY ) is a correlation function with parameter vector\\nφ in which a covariance function is defined\\n4) SSIM\\nThis image quality assessment techniques rely on quantifying\\nerrors between reference and sample image and needs two\\nimages to do a calculation. SSIM can identify structural infor-\\nmation from a scene and the differences between the infor-\\nmation extracted from reference and sample scenes. It takes\\ntwo inputs, original and distorted. The outcome value varies\\nbetween 0 to 1, where higher is better. SSIM has a generalized\\nformula as in (7).\\nSSIM(i1, i2) = [l(i1, i2)]α · [c(i1, i2)]β · [s(i1, i2)]γ (7)\\n• i1 is the first image\\n• i2 is the second image\\n• l(i1, i2) is luminance comparison function\\n• c(i1, i2) is contrast comparison function\\n• s(i1, i2) is structural comparison function\\n• α > 0, β > 0, γ > 0 denote the relative importance of\\neach of the metrics\\n5) VIF\\nVisual Information Fidelity calculates pixel-based visual\\ninformation fidelity. The VIF is computed for a collection\\nof wavelet coefficients that could represent either an entire\\nsubband of an image or a spatially localized set of subband\\ncoefficients. In the VIF system, the higher score is, the better,\\nwhere 1 is the best case. The scoring system is similar to\\nSSIM, using a value between 0 and 1. A general term of the\\nVIF formula is in (8).\\nVIF = 6j(subbands)I( ⃗CNj ; ⃗FNj |sNj )\\n6j(subbands)I( ⃗CNj ; ⃗ENj |sNj )\\n(8)\\n• I( ⃗CN ; ⃗FN |sN ) and I( ⃗CN ; ⃗EN |sN ) represent the informa-\\ntion that could ideally be extracted by the brain from\\na particular subband of the reference and test images,\\nrespectively\\n• ⃗CNj represents N elements of the RF that describe the\\ncoefficients from subband j, and so on\\n26674 VOLUME 11, 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 8, 'page_label': '26675', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\n6) FILE SIZE\\nUsually, any image file size will be larger when its resolution\\nand compression quality value is bigger. However, suppose\\nall images have the exact resolution and compression ratio.\\nIn that case, their file size can reflect how much that image\\nholds information (image detail). The bigger size, the more\\nimage contains fine detail. The file size in this study has a\\nunit as kilobytes (kB).\\n7) TIME USAGE\\nTime usage in this study is a training time usage per training\\nepoch. It indicates how fast a model setup is, and the faster\\napproach is always preferable. A unit for the time used in this\\nstudy is second.\\n8) RENDER SPEED\\nA render speed has a unit of frames per second (FPS). It indi-\\ncates how fast a model setup is. The faster approach is always\\npreferable. This metric is used in a testing process. This\\nFPS can also tell if a testing model is viable for real-time\\noperation since a CCTV camera typically operates from\\n15 to 30 FPS.\\n9) MEMORY USAGE\\nMemory usage can ultimately be a deciding factor if any\\naaproach could be deployed on edge computing devices since\\nmost of them have a very limited amount of memory. Usually,\\nthese embedded systems have around 32MB to 512MB in\\nmemory capacity. So a model should use as the least RAM as\\npossible.\\nG. HARDWARE\\nBelow is a custom build model training PC running Ubuntu\\n18.04. This PC was used throughout the entire research.\\n• AMD Ryzen 7 2700× 8 Cores 16 Threads CPU\\n• Asrock B450 Gaming K4 Motherboard\\n• Galax NVIDIA GeForce RTX 2080 Ti 11GB GDDR6\\n352-bit 260 Watts GPU\\n• Corsair 32GB DDR4 Memory\\n• WD Green SATA SSD 120GB\\n• Cooler Master 80PLUS Gold Full Modular 750W Power\\nSupply\\nThis is a workstation PC used in testing.\\n• Intel Xeon Processor E3-1200 v6 72 Watts CPU\\n• 8GB DDR4 Memory\\n• 2 × 3.5′′ Enterprise SATA 7.2k 1TB\\nThe Microsoft Surface Go 3 tablet PC is an ultra-low-power\\nPC used in testing.\\n• Intel Core i3 10100Y 5 Watts Ultra Low Power CPU\\n• 1866MHz 8GB DDR3 Memory\\n• 128GB SSD PCIe Storage\\nThis is a specification of a camera used in this study.\\n• Lilin ZR8022EX10 1080p 2MP CMOS Sensor IP\\nCamera\\nFIGURE 5. Test scene illustration.\\nFIGURE 6. Original/reference image.\\nH. TEST SCENE\\nThe below figure illustrates how a test scene was set up.\\nA camera model used in the testing is Lilin ZR8022EX10.\\nThe distance between a camera and the front bumper of a\\ncar where a license plate is located is 3±0.5 meters. A car\\nis stationary in front of the camera to be safe and avoid\\nan accident. There is no physical restriction on the distance\\nbetween a camera and a plate since all collected images are\\ncropped to fit a license plate area in post-processing.\\nIV. RESULT\\nThis section contains all related training and testing exper-\\niments details. The below image is an original/reference\\nimage1 found in Fig. 6. It will be used to compare against\\neach problem type generated images.\\nA. LAYER DEPTH CONFIGURATION\\nTable. 1 shows ablation studies between 11, 13, and 15 of the\\ngenerator’s layers configurations. A training time is a time\\nusage per epoch, unit in seconds. As shown below, the more\\nlayer counts the more computational time is needed making\\noverall performance drop. Also, at the current stage, using\\n11 layers count produces the best outcome for both metrics.\\nB. KERNEL SIZE OPTIMIZATION\\nA Table. 2 represents the combination between each layer and\\nthe last layer kernel size settings. This experiment studies the\\nresult difference between each set and finds the best kernel\\nsize setting for a current generator setup. Furthermore, a main\\nlayer kernel size setting is used in both a generator and a\\ndiscriminator.\\nC. SIGMOID VS TANH AS ACTIVATION FUNCTION\\nThe proposed models’ activation functions have been mod-\\nified to achieve the best result. It is slightly different\\nfrom selecting the Sigmoid to Tanh function. However,\\n1This license plate is the author’s ownership, and it does not violate other\\nprivacy or rights.\\nVOLUME 11, 2023 26675'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 9, 'page_label': '26676', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nTABLE 1. SSIM score on each layer depth configuration table.\\nTABLE 2. SSIM score on each kernel size table.\\nFIGURE 7. Result from Using Sigmoid VS Tanh Function - (a) Sigmoid\\n(SSIM 0.787), (b) Tanh (SSIM 0.784).\\nFIGURE 8. Training performance of learning Rate= 0.001.\\nFIGURE 9. Training performance of learning Rate= 0.0001.\\nan experiment result shows that the Sigmoid function per-\\nforms better in the SSIM score.\\nD. LEARNING RATE ADJUSTMENT\\nThis subsection demonstrates how different when selecting a\\nlearning rate. In Fig. 8, picking too big a learning rate value\\nwould make a training overshooting (overfitting), mean-\\ning that a loss has gone saturated and accuracy hit almost\\n100% (optimal accuracy in GAN training is 50%), includ-\\ning those measurement metrics started to decline, in very\\nearly point of the training. On the other hand, when pick-\\ning a proper learning rate value [Fig. 9], a training loss\\nis saturated and an accuracy value hangs in between the\\nmiddle, especially near the end of the training, accuracy\\nis stable at around 50% (balanced-fit) as well as mea-\\nsurement metrics that reach a very high peak in the later\\nresults.\\nFIGURE 10. Training performance ofβ1 = 0.1.\\nFIGURE 11. Training performance ofβ1 = 0.5.\\nFIGURE 12. Training performance ofβ1 = 0.9.\\nE. ADAM OPTIMIZER ADJUSTMENT\\nAn optimizer used in this work is an Adaptive Moment\\nEstimation (ADAM) optimizer [38]. A β1 is one of the hyper-\\nparameters and adjustable value. It is the initial decay rate\\nused when estimating the first moment of the gradient while\\ntraining, which is multiplied at the end of each training step\\nor batch. Decreasing β1 will slow down a training process and\\nincreasing a value will result in the opposite way. This value\\nneeds to be adjusted according to batch size. Normally, a large\\nbatch size will result in faster learning and a small batch will\\nresult in slower learning. Once using a very large or very low\\nbatch size could lead to non-optimal learning, adjusting this\\nβ1 value can help in these situations. In this showing case,\\nusing β1 = 0.5 (default is 0.9) is a middle ground that matches\\nthe other training values used in this learning process and\\nresults in the best balance point between training speed and\\nperformance. However, each training may also require tuning\\nand has its own optimal β1 value.\\nF. LOW BITRATE PROBLEM\\nThis subsection shows the LPRGAN testing results in low\\nbitrate conditions. Overall performance graphs are displayed\\nin Fig. 13-Fig. 14.\\nIn this case, a poor-quality version of the reference image,\\nis the input image. The input image can be found in\\n26676 VOLUME 11, 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 10, 'page_label': '26677', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nFIGURE 13. Overall training performance on low Bitrate problem.\\nFIGURE 14. Overall evaluating performance on low Bitrate problem.\\nFIGURE 15. Result on simulated Low Bitrate problem - (a) Input Image,\\n(b) CBDNet output image, (c) GFPGAN-SR output image, (d) Convolutional\\nAutoencoder output image, (e) GAN+U-Net output image, (f) SwinIR\\nOutput Image and (g) LPRGAN output image.\\nFig. 15a.2 Output results from each network are shown in\\nFig. 15b-Fig. 15g respectively. As a result, the CBDNet, the\\nGFPGAN-SR and the SwinIR do not work in this case, but the\\nLPRGAN can recover most of the lost data in input images,\\nespecially in low-detail areas like a grey plate background.\\nAt the same time, a convolutional autoencoder has a smooth\\noutput but fails to remove a blocky artifact, and the GAN+U-\\nNet has a not-so-sharp image.\\nHere is an example of using the LPRGAN on a low bitrate\\nvideo, an actual use case, instead of a JPG compression\\nimage. A video was recorded with a very low bitrate, 8kbps,\\nH264 format. It was also resized to a 256 × 128 frame size,\\nmatching a proposed network configuration. Unfortunately,\\nthere is no original/reference image to compare with the\\noutput in an actual situation, so only the input [Fig. 16a]\\nand outputs [Fig. 16b-Fig. 16g] results are available. When\\nzoomed in, all results [Fig. 17a-Fig. 17f] are visible, showing\\nthat the LPRGAN gives out the best result over the rest.\\nIt produces more detail and contrasts. Although the SwinIR\\ncould remove compression artifact, it also removes some\\n2Some PDF viewers (i.e., Preview in macOS) have an antialiasing feature\\nthat smooths out a rough rendered object in a document. In order to see raw\\nresult images, this feature must be turned off.\\nFIGURE 16. Result on Actual Low Bitrate Video Problem - (a) Input Image,\\n(b) CBDNet Output Image, (c) GFPGAN-SR Output Image,\\n(d) Convolutional Autoencoder Output Image, (e) GAN+U-Net Output\\nImage, (f) SwinIR Output Image and (g) LPRGAN Output Image.\\nFIGURE 17. Result on Actual Low Bitrate Video Problem with 3×Zoom on\\nLast 3 Digits - (a) CBDNet Output Image, (b) GFPGAN-SR Output Image,\\n(c) Convolutional Autoencoder Output Image, (d) GAN+U-Net Output\\nImage, (e) SwinIR Output Image and (f) LPRGAN Output Image.\\nFIGURE 18. Overall training performance on low light problem.\\nfine detail from the image, making it looks less sharp. The\\nGFPGAN-SR has an aspect ratio distortion due to the nature\\nof the super-resolution technique. The rest of the outputs are\\ndull and blurry.\\nG. LOW LIGHT PROBLEM\\nThis subsection demonstrates two types of testing images in\\na low-light situation. One simulates a low light by reduc-\\ning image brightness, and the other captures an actual low\\nlight nighttime. Overall performance graphs are displayed in\\nFig. 18-Fig. 19.\\nIn the simulation [Fig. 20a], both the LPRGAN and the\\nGAN+U-Net show improved brightened images but not the\\nrest. In addition, the GAN+U-Net output has some yel-\\nlow tint and is not as sharp whereas the MIRNet and the\\nLPRGAN produce the correct color temperature results.\\nFig. 20b-Fig. 20g show all outputs. In the real-world scene\\ntest [Fig. 21a], only the GAN+U-Net, the MIRNet, and the\\nLRPGAN outputs have improved brightness from the input\\nbut the GAN+U-Net also has a weird tint in the output, only\\nVOLUME 11, 2023 26677'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 11, 'page_label': '26678', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nFIGURE 19. Overall evaluating performance on low light problem.\\nFIGURE 20. Result on simulated low light problem - (a) Input image,\\n(b) CBDNet output image, (c) GFPGAN-SR output image, (d) Convolutional\\nAutoencoder output image, (e) GAN+U-Net output image, (f) MIRnet\\noutput image and (g) LPRGAN output image.\\nFIGURE 21. Result on actual low light problem - (a) Input Image,\\n(b) CBDNet output image, (c) GFPGAN-SR output image, (d) Convolutional\\nautoencoder output image, (e) GAN+U-Net output image, (f) MIRnet\\noutput image and (g) LPRGAN output image.\\nFIGURE 22. Overall training performance on horizontal motion blur\\nproblem.\\nFIGURE 23. Overall evaluating performance on horizontal motion blur\\nproblem.\\nthe MIRNet and the LPRGAN produce the most realistic and\\ncorrected color tone images [Fig. 21b-Fig. 21g]. Ultimately,\\nthe LPRGAN and the MIRNet are the first and second candi-\\ndates in low light recovery, but the CBDNet, the GFPGAN-\\nSR, and a convolutional autoencoder failed completely.\\nH. 1-AXIS MOTION BLUR PROBLEM\\nThese are examples of solving one-directional motion blur\\nproblems. Both horizontal (0 degrees) and vertical (90\\ndegrees) motion blur were studied in this research. Overall\\nperformance graphs are displayed in Fig. 22-Fig. 25.\\nFIGURE 24. Overall training performance on vertical motion blur problem.\\nFIGURE 25. Overall evaluating performance on vertical motion blur\\nproblem.\\nFIGURE 26. Result on simulated horizontal motion blur problem -\\n(a) Input image, (b) CBDNet output image, (c) GFPGAN-SR output image,\\n(d) Convolutional Autoencoder output image, (e) GAN+U-Net output\\nimage, (f) DeblurGANv2+MobileNet output image, (g) Restormer output\\nimage and (h) LPRGAN output image.\\nA blurred input image in the horizontal blur problem is\\nshown in Fig. 26a for a simulated test case. Simulation output\\nresults for horizontal blur demonstrated from each network\\nare shown in Fig. 26b-Fig. 26h. However, in order to get\\nactual motion blur images, a high-speed panning camera\\nin left-right directions creates a horizontal blur [Fig. 27a].\\nThis action is a much safer measurement than driving a car\\nspeeding toward a camera. The actual case output results for\\nhorizontal blur demonstrated from each network are shown\\nin Fig. 27b-Fig. 27h. Also, in a vertical motion blur problem,\\na simulated vertical blur image is in Fig. 28a, and these\\nFig. 28b-Fig. 28h are the output. Once again, in order to\\nget an actual vertical motion blur to quickly pan a camera\\nin up-down directions to create a vertical blur [Fig. 29a].\\nThe outputs are shown in Fig. 29b-Fig. 29h. In the end,\\nthe DeblurGANv2 and the Restormer can fix a motion blur\\nproblem only in a simulation case but not a real-world one,\\nand only the LPRGAN can recover blurred images in both\\ncases.\\n26678 VOLUME 11, 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 12, 'page_label': '26679', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nFIGURE 27. Result on actual horizontal motion blur problem - (a) Input\\nimage, (b) CBDNet output image, (c) GFPGAN-SR output image,\\n(d) Convolutional Autoencoder output image, (e) GAN+U-Net output\\nimage, (f) DeblurGANv2+MobileNet output image, (g) Restormer output\\nimage and (h) LPRGAN output image.\\nFIGURE 28. Result on simulated vertical motion blur problem - (a) Input\\nimage, (b) CBDNet output image, (c) GFPGAN-SR output image,\\n(d) Convolutional Autoencoder output image, (e) GAN+U-Net output\\nimage, (f) DeblurGANv2+MobileNet output image, (g) Restormer output\\nimage and (h) LPRGAN output image.\\nFIGURE 29. Result on actual vertical motion blur problem - (a) Input\\nimage, (b) CBDNet output image, (c) GFPGAN-SR output image,\\n(d) Convolutional Autoencoder output image, (e) GAN+U-Net output\\nimage, (f) DeblurGANv2+MobileNet output image, (g) Restormer output\\nimage and (h) LPRGAN output image.\\nFIGURE 30. Original US and UK License Plates - (a) US and (b) UK.\\nI. INTERNATIONAL COUNTRIES LICENSE PLATE TEST\\nAlthough the current LPRGAN model has been trained on the\\nThai license plate dataset without knowing other countries’\\nplate appearances, this model can still recover a poor-quality\\nplate at a reasonable level thanks to its generalization.\\nOf course, its performance would not be near a Thai license\\nplate recovery on which it was trained, but it can be solved\\nby retraining with a target country dataset. These figures\\n[Fig. 30a-Fig. 30b] show a US [39] and UK [40] license plate\\nsamples.\\nFIGURE 31. Result on US plate from LPRGAN (a) Low Bitrate input image,\\n(b) Low Bitrate output image, (c) Low light input image, (d) Low light\\noutput image, (e) Horizontal motion blur input image, (f) Horizontal\\nmotion blur output image, (g) Vertical motion blur input image and\\n(h) Vertical motion blur output image.\\nFIGURE 32. Result on UK plate from LPRGAN (a) Low Bitrate input image,\\n(b) Low Bitrate output image, (c) Low light input image, (d) Low light\\noutput image, (e) Horizontal motion blur input image, (f) Horizontal\\nmotion blur output image, (g) Vertical motion blur input image and\\n(h) Vertical motion blur output image.\\nJ. METRIC MEASUREMENTS RESULT\\nThere are two types of measurement in this research. First,\\nusing an image qualifier that gives out a Star Rating, and\\nsecond, mathematical measurements 3 including five metrics\\n(FID, PSNR, SCC, SSIM, and VIF) and three synthetic\\nbenchmarks (file size, training time usage per epoch, and\\nrecovery render speed). A proposed model tested up against\\nother approaches4 is presented in this subsection.\\nA quality rating test using an image qualifier shows that the\\nLPRGAN has no problem fixing poor-quality input images.\\nIt generated a 5-Star output image in the low bitrate case\\nand normal-looking images in the rest of the test cases.\\nAt the same time, U-Net architecture with GAN also did\\na reasonable job in most cases. However, a convolutional\\nautoencoder did not perform any good. The reason behind\\nthis measurement is that all mathematical measurements do\\nnot work when there is no reference image (its counterpart)\\nto calculate a value in an actual situation. In reality, only a\\ndegraded image is presented.\\n3Rating score is output from classifier both detector and qualifier. These\\ntwo models have an accuracy of over 99%. All mathematical measurement\\nvalues were computed with a pair of original/reference and distorted images.\\nAll delta values in each case were calculated against each own low-quality\\nimage. However, it is impossible to compute these metrics values in actual\\ntest cases due to a lack of reference material. In the motion blur problem\\ncase, HB is a horizontal blur, and VB is a vertical blur.\\n4All methods experimented in this study were under the same environment\\nand parameters, such as the same computer machine, an equal number of\\niterations, learning rate, batch size, and the same input image under each\\ntest. Also, all images used in this study were encoded at 100% Quality to\\navoid a compression loss, revealing an actual image data size.\\nVOLUME 11, 2023 26679'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 13, 'page_label': '26680', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nTABLE 3. A metric measurement score table on low Bitrate problem.\\nTABLE 4. A metric measurement score table on low light problem.\\nNext, a visual quality inspection using mathematical cal-\\nculation tests shows that the GAN+U-Net is superior to a\\nconvolutional autoencoder. However, the LPRGAN has the\\nbest outcome in most cases among convolutional autoencoder\\nand GAN+U-Net approaches. As in a low bitrate scenario\\n[Table. 3], the LPRGAN generated the biggest output file\\nsize. It also has the highest (as high as SwinIR) SSIM score\\non low bitrate recovery tests. File size value is essential\\nhere since they tell how much an image holds a piece of\\ninformation. The bigger the file size, the more fine detail\\nis generated. SSIM score indicates that reconstuction image\\nhas a similar structure to an original. These outcomes prove\\nthat the LPRGAN helps predict lost data back, producing\\na richer detailed image. In a low light situation [Table. 4],\\nthe LPRGAN beats out the other models on SSIM score,\\nbut when comparing the LPRGAN to the MIRNet in this\\nscenario (MIRNet cannot reconstruct other cases, only a low\\nlight case), a competitor has three out of five metrics (FID,\\nSCC, and VIF) score higher than the LPRGAN. Even though\\nthe LPRGAN has only two metrics (PSNR and SSIM) wins\\nover the MIRNet. Because of low light situation, these PSNR\\nand SSIM metrics are crucial. Since PSNR is a Power Signal\\nto Noise Ratio, the more PSNR is, the more power signal and\\nthe lesser noise, producing a cleaner image, and SSIM is also\\ncalculated based on image luminosity factors meaning that\\nSSIM is high when the output has a structural and luminosity\\nclose to an original one. A higher SSIM is a brighter image\\nbecause a reference is bright. Thus, the LPRGAN output has\\na lower noise yet a brighter image as the result. In motion\\nblur cases [Table. 5], the LPRGAN has SCC and file size (in\\nhorizontal blur case) metrics higher then the rest. The SCC is\\nthe concerning metric here since it is designed to detect any\\npixel location shifting, which is suitable for this case. A better\\nSCC value means the lesser pixel is shifted from a reference\\nimage. In other words, the lesser blurry image.\\nOn the other hand, US and UK were chosen for this inter-\\nnational plate test. The result from Table. 6 shows that every\\nproblem case passes a visual inspection test. Even though the\\nLPRGAN could not always produce better metric values than\\nthe original input, all generated output always contains more\\ndata than the original ones by looking at the file size metric.\\nThe outputs from the LPRGAN surprisingly have consistent\\nfile sizes on every problem test in a margin of ±1.4 kB.\\nMove over to a speed test, a speed comparison between\\neach approached method [Table. 7] shows that although a\\nconvolutional autoencoder is the fastest in an FPS count due\\nto a non-GAN design it does not produce an output well at all.\\nThe CBDNet is also a non-GAN design but it performs slowly\\nat the same level as GAN-based designs. In contrarily, com-\\nparing GAN-based models, results show that the LPRGAN\\nis the fastest in every render speed test. When compared\\nto the GAN+U-Net approache in 256 × 128 resolution, the\\n26680 VOLUME 11, 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 14, 'page_label': '26681', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nTABLE 5. A metric measurement score table on motion blur problem.\\nTABLE 6. A metric measurement score table on other countries plate.\\nLPRGAN is 2.93× speedup in training speed and more\\nthan twice as fast (2.42×) in a render test. When com-\\npared to the Restomer and the SwinIR in render test, the\\nLPRGAN is 1.77× and 9.32× speedup. At a higher resolu-\\ntion (600 × 400), comparing the LPRGAN to the MIRNet,\\nresults are as expected because the MIRNet has a much\\nbigger network (one complex parallel stream vs. one sim-\\nplified serial stream) and is very limited in resolution (only\\nproducing 600 × 400 output), so the LPRGAN is way faster\\nthan the MIRNet by 25.26× in the render test while the\\nDeblurGANv2+MobileNet is a relatively compact network,\\nthere is still a gap between itself and the LPRGAN in render\\nspeed (1.71× difference), while the GAN+U-Net and the\\nRestomer are dropped behind the LPRGAN. The same story\\nat HD resolution, the LPRGAN is slightly faster than the\\nDeblurGANv2+MobileNet (1.36×) in the same test sce-\\nnario due to fewer parameter counts. The GAN+U-Net is\\neven lacking behind these two GAN-based models while the\\nSwinIR could not finish a test due to painfully low speed. This\\nmakes the LPRGAN the fastest.\\nVOLUME 11, 2023 26681'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 15, 'page_label': '26682', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nTABLE 7. Approached methods speed comparison table.\\nFurthermore, there are three videos used in the LPRGAN\\nspeed on a different type of processor test [Table. 8]. The first\\nvideo is 21.5 seconds long, 932 frames, full-length (100%\\ndegradation) degraded video file named Video#1. The second\\nand the third test case are 21.5 seconds long, 932 frames,\\nhalf-length (50%, 466 frames degradation) degraded video\\nfile and 21.5 seconds long, 932 frames, 25% length (233\\nframes degradation) degraded video files, named Video#2\\nand Video#3. This test represents real-world usage in a mix-\\ning environment because not every video frame would be\\ndegraded all the time, so running recovery on a good frame\\nis a waste. This test shows a difference between the plain\\nLPRGAN and the adaptive LPRGAN (detection+recovery)\\nperformance in action. However, the adaptive LPRGAN did\\nnot actually double the frame rate in Video#2 even though\\na test video contains only half degraded frames due to\\nan additional detection workload, but it is still close to\\nTABLE 8. LPRGAN speed test table.\\nTABLE 9. Memory usage comparison table.\\ndoubling a frame rate than an unequipped detection system by\\n1.71× and 3.3× in Video#2 and #3 on ultra-low power CPU,\\n1.76× and 3.4× in Video#2 and #3 on workstation CPU and\\n1.7× and 2.92× in Video#2 and #3 on a single GPU.\\nThe last one is the memory usage test [Table. 9] (unit in\\nMB), this test is a measurement of memory usage on a single\\nframe recovery from each method. Separated into two groups,\\na non-GAN and a GAN based. Although, non-GAN methods\\ntend to use less RAM as result in this article shows that they\\ndo not work well in many situations. On the other hand, the\\nLPRGAN still uses the least amount of RAM in GAN based\\n26682 VOLUME 11, 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 16, 'page_label': '26683', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nFIGURE 33. Result on additional actual thai license plates from LPRGAN\\n(a) Low Bitrate Input Image#1, (b) Low Bitrate Output Image#1, (c) Low\\nBitrate Input Image#2, (d) Low Bitrate Output Image#2, (e) Low Light\\nInput Image#1, (f) Low Light Output Image#1, (g) Low Light Input\\nImage#2, (h) Low Light Output Image#2, (i) Horizontal Blur Input\\nImage#1, (j) Horizontal Blur Output Image#1, (k) Horizontal Blur Input\\nImage#2, (l) Horizontal Blur Output Image#2, (m) Vertical Blur Input\\nImage#1, (n) Vertical Blur Output Image#1, (o) Vertical Blur Input\\nImage#2 and (p) Vertical Blur Output Image#2.\\nFIGURE 34. Average real world data recovery performance result.\\ngroup. Thus, it is possible to be used in an edge computing\\ndevice where it has a limited amount of RAM.\\nK. REAL WORLD LICENSE PLATES TEST\\nThese are real-world samples [Fig. 33a-Fig. 33p] recovery\\nusing the proposed system. However, these plates in this\\nsubsection do not belong to the author, so to protect their\\nowner’s privacy, they cannot be exposed to the full license\\nplate area [41]. The data recovery result in Fig. 34 shows that\\nin every degraded type, average file sizes have gained more\\ndata after a recovery in low bitrate, low light, horizontal blur,\\nand vertical blur situations at rate 1.51×, 1.25×, 1.61×, and\\n1.63×, respectively.\\n1) DIFFICULT REAL WORLD SITUATION\\nIn extremely poor input images where input information is too\\nmuch distorted, a generated result could be confusing as in\\nFig. 33k- 33l. This problem comes from the generator apply-\\ning deblurring aggressively from its learning which can cause\\nFIGURE 35. (a) Input image, (b) DeblurGANv2+MobileNet output image,\\n(c) Restormer output image and (d) LPRGAN output image.\\nFIGURE 36. Recognizer prediction confidence result.\\nFIGURE 37. Images set used in recognition test.\\nFIGURE 38. Average real world prediction confidence result.\\nmisleading information. It can be solved by selecting a differ-\\nent weight from a lower number of iterations where the gener-\\nator is not overpowered by the discriminator. When compar-\\ning the LPRGAN to both the DeblurGANv2+MobileNet and\\nthe Restormer, only the LPRGAN output produces a sharper\\nresult while the rest do not deblur well since those remain\\nunsharp in this real license plate test.\\nL. REAL WORLD LICENSE PLATE RECOGNITION TEST\\nThis test is another aspect of the LPRGAN helping license\\nplate recognition. A recognizer is a recognition system simi-\\nlar to a classifier, a typical VGG-16 network found in Fig. 3\\nVOLUME 11, 2023 26683'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 17, 'page_label': '26684', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nwhich has three classes, EU, TH 5 [42] and US plates (EU-\\nBelgian and US datasets were from [43] and [44]), in total\\n1,237 training images. These training images differ from\\nthe LPRGAN dataset, making predictions the most neutral.\\nHowever, we only focus on the TH license plate class to prove\\nthat with the help of LPRGAN can make a recognizer has\\nmore confidence in recognizing a license plate. In Fig. 36 is\\na result from Fig. 37 images set, there is not much different\\nresult in low bitrate and vertical blur problems but in low light\\nand horizontal blur cases result in a great benefit from using\\nthe LPRGAN system, whereas normal is a good quality image\\nso its confidence is the highest. The next test is an average\\nconfidence value result by sampling a set of each degraded\\ntype from real-world images (some of them were shown in\\nFig. 33a-Fig. 33p). The result in Fig. 38 shows that recovery\\nimages can increase an average prediction performance in\\nlow bitrate by 1.1 ×, low light by 1.41×, horizontal blur by\\n1.52×, and horizontal blur by 1.16×.\\nV. CONCLUSION\\nThe research presented in this article studied a way to imple-\\nment a fast license plate image quality recovery for traffic\\nmonitoring in various poor situations. The proposed frame-\\nwork uses the optimized lightweight encoder-decoder style\\nCNN architecture built inside a GAN model to do a recov-\\nery job alongside image classifications that detect inputs\\nand verify outputs, helping the LPRGAN in a much more\\nefficient and effective way. This study proved that it could\\nimprove low bitrate, low light, and motion blur problems\\nfrom a single design network in many test cases. Not only\\nthat, this system is able to outpace or be at the same quality\\nlevel as other complex networks while performing the task\\nquickest. As a result, the proposed system can run on less\\ncomputational power machines like most typical workstation\\nPCs without a discreet graphic card at a reasonable pace\\nand is possible to deploy in embedded systems such as edge\\ncomputing devices. This study opens a new door for many\\npower-constrained image recovery applications. Such bene-\\nfits make this framework easy to be deployed on traffic officer\\ncomputers or even embedded within camera recording boxes,\\naiding them in identifying vehicle licenses in inadequate\\nconditions. Thus removing the need for a high-performance\\nserver machine and greatly reducing network bandwidth\\nusage between devices.\\nAt this stage, the LPRGAN can render a real-time frame\\nrecovery up to 1280 × 720@15fps, which is sufficient for\\nmost typical CCTV/IP cameras, for example, Merit Lilin\\nZG1232EX3 (3MP, 15FPS) or Merit Lilin LR832 (2MP,\\n15FPS). As time passes, license plates can be collected more,\\ngiving a model retraining even more performance gain. How-\\never, this study demonstrated a few applications that this\\nsystem could handle. It depends on how the user provides a\\ndataset for training because the system uses a good dataset\\n5This set of Thai license plates contains partial dummy plates and is\\navailable upon request.\\nas a template and learns how a distorted dataset differs, so it\\nwould theoretically work in other situations too. Last but not\\nleast, here are some limitations that need to be concerned.\\n• Output dimension size is a fixed size (256 × 128 × 3).\\nIt cannot be used to create other larger or smaller sizes\\nunless reconfiguration and retraining are required.\\n• Output quality is limited by the training dataset. It cannot\\ncreate a better quality than the original dataset.\\n• Some small detail areas in the generated image could\\nbe somewhat mediocre, such as the province name in\\nsome problem cases. This drawback can be avoided by\\nhaving a higher training dataset resolution i.e., collect-\\ning higher-resolution images or using a super-resolution\\ntechnique.\\n• While the proposed model could be used in other coun-\\ntries rather than its origin, but its performance would\\nnot be on par. Hence, retraining in a target country is\\nrequired.\\nREFERENCES\\n[1] Autoencoder. Building Autoencoders in Keras. Accessed: Jul. 15, 2022.\\n[Online]. Available: https://blog.keras.io/building-autoencoders-in-\\nkeras.html\\n[2] U-Net. U-Net Explained. Accessed: Jul. 15, 2022. [Online]. Available:\\nhttps://paperswithcode.com/method/u-net\\n[3] J. Langr and V. Bok, GANs in Action—Deep Learning With Generative\\nAdversarial Networks, vol. 3, 8th ed. Shelter Island, NY, USA: Manning,\\n2019.\\n[4] DCGAN. Deep Convolutional Generative Adversarial Network.\\nAccessed: Jul. 15, 2022. [Online]. Available: https://www.tensorflow.\\norg/tutorials/generative/dcgan\\n[5] CGAN. Conditional GAN. Accessed: Jul. 15, 2022. [Online]. Available:\\nhttps://keras.io/examples/generative/conditional_gan\\n[6] CycleGAN. Accessed: Jul. 15, 2022. [Online]. Available: https://\\npaperswithcode.com/method/cyclegan\\n[7] A. Petrov, T. Kartalov, and Z. Ivanovski, ‘‘Blocking effect reduc-\\ntion in low bitrate video on a mobile platform,’’ presented at the\\nIEEE Int. Conf. Image Process. (ICIP), Nov. 2009. [Online]. Available:\\nhttps://ieeexplore.ieee.org/abstract/document/5414031\\n[8] Y. Dar and A. M. Bruckstein, ‘‘Improving low bit-rate video coding using\\nspatio-temporal down-scaling,’’ 2014, arXiv:1404.4026.\\n[9] Y. Li, D. Liu, H. Li, L. Li, F. Wu, H. Zhang, and H. Yang, ‘‘Convolutional\\nneural network-based block up-sampling for intra frame coding,’’ 2017,\\narXiv:1702.06728.\\n[10] H. Lin, X. He, and L. Qing, ‘‘Improved low-bitrate HEVC video coding\\nusing deep learning based super-resolution and adaptive block patching,’’\\nIEEE Trans. Multimedia, vol. 21, no. 12, pp. 3010–3023, Dec. 2019.\\n[11] R. Yang, M. Xu, T. Liu, Z. Wang, and Z. Guan, ‘‘Enhancing quality for\\nHEVC compressed videos,’’ 2017, arXiv:1709.06734.\\n[12] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, ‘‘Unpaired image-to-\\nimage translation using cycle-consistent adversarial networks,’’ 2017,\\narXiv:1703.10593.\\n[13] Pix2Pix. Image-to-Image Translation using Pix2Pix. Accessed:\\nJul. 15, 2022. [Online]. Available: https://www.geeksforgeeks.org/image-\\nto-image-translation-using-pix2pix\\n[14] S. Guo, Z. Yan, K. Zhang, W. Zuo, and L. Zhang, ‘‘Toward convolutional\\nblind denoising of real photographs,’’ presented at the IEEE/CVF Conf.\\nComput. Vis. Pattern Recognit. (CVPR), Jun. 2019. [Online]. Available:\\nhttps://ieeexplore.ieee.org/document/8954448\\n[15] X. Wang, Y. Li, H. Zhang, and Y. Shan, ‘‘Towards real-world blind\\nface restoration with generative facial prior,’’ in Proc. CVPR, Jun. 2021,\\npp. 9168–9178.\\n[16] O. Kupyn, T. Martyniuk, J. Wu, and Z. Wang, ‘‘DeblurGAN-v2:\\nDeblurring (orders-of-magnitude) faster and better,’’ presented at the\\nIEEE/CVF Int. Conf. Comput. Vis. (ICCV), Aug. 2019. [Online]. Avail-\\nable: https://ieeexplore.ieee.org/document/9008540\\n26684 VOLUME 11, 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 18, 'page_label': '26685', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\n[17] S. W. Zamir, A. Arora, and S. Khan, ‘‘Zamir2020MIRNet : Learning\\nenriched features for real image restoration and enhancement,’’ in Proc.\\nEur. Conf. Comput. Vis. (ECCV), Aug. 2020, pp. 1934–1948.\\n[18] S. W. Zamir, A. Arora, and S. Khan, ‘‘Restormer: Efficient transformer\\nfor high-resolution image restoration,’’ presented at the IEEE/CVF Conf.\\nComput. Vis. Pattern Recognit. (CVPR), Sep. 2022. [Online]. Available:\\nhttps://ieeexplore.ieee.org/document/9878962\\n[19] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte, ‘‘SwinIR:\\nImage restoration using swin transformer,’’ 2021, arXiv:2108.10257.\\n[20] W. Zhang, Z. Zhou, Z. Gao, G. Yang, L. Xu, W. Wu, and H. Zhang,\\n‘‘Multiple adversarial learning based angiography reconstruction for ultra-\\nlow-dose contrast medium CT,’’IEEE J. Biomed. Health Informat., vol. 27,\\nno. 1, pp. 409–420, Jan. 2023.\\n[21] W. Wu, D. Hu, C. Niu, H. Yu, V. Vardhanabhuti, and G. Wang, ‘‘DRONE:\\nDual-domain residual-based optimization NEtwork for sparse-view CT\\nreconstruction,’’IEEE Trans. Med. Imag., vol. 40, no. 11, pp. 3002–3014,\\nNov. 2021.\\n[22] W. Wu, X. Guo, Y. Chen, S. Wang, and J. Chen, ‘‘Deep embedding-\\nattention-refinement for sparse-view CT reconstruction,’’ IEEE Trans.\\nInstrum. Meas., vol. 72, pp. 1–11, 2023.\\n[23] Supervised vs Unsupervised. Clustering vs Classification: Difference\\nBetween Clustering & Classification. Accessed: Jul. 15, 2022. [Online].\\nAvailable: https://www.upgrad.com/blog/clustering-vs-classification\\n[24] Motion Blur in Numpy Array. How to Add Motion Blur to Numpy\\nArray. Accessed: Jul. 15, 2022. [Online]. Available: https://stackoverflow.\\ncom/questions/40305933/how-to-add-motion-blur-to-numpy-array\\n[25] VGG-16. VGG-16 CNN Model. Accessed: Jul. 15, 2022. [Online]. Avail-\\nable: https://www.geeksforgeeks.org/vgg-16-cnn-model\\n[26] J. Tobias Springenberg, A. Dosovitskiy, T. Brox, and\\nM. Riedmiller, ‘‘Striving for simplicity: The all convolutional net,’’\\n2014, arXiv:1412.6806.\\n[27] Image Compression. Wikipedia. Accessed: Jul. 15, 2022. [Online]. Avail-\\nable: https://en.wikipedia.org/wiki/Image_compression\\n[28] JPEG. Wikipedia. Accessed: Jul. 15, 2022. [Online]. Available:\\nhttps://en.wikipedia.org/wiki/JPEG\\n[29] Decay Learning Rate. Learning Rate Decay and Methods in Deep Learn-\\ning. Accessed: Jul. 15, 2022. [Online]. Available: https://medium.com/\\nanalytics-vidhya/learning-rate-decay-and-methods-in-deep-learning-\\n2cee564f910b\\n[30] Pytorch-FID. FID Score for PyTorch. Accessed: Jul. 15, 2022. [Online].\\nAvailable: https://pypi.org/project/pytorch-fid\\n[31] Sewar. Sewar Python Package. Accessed: Jul. 15, 2022. [Online]. Avail-\\nable: https://pypi.org/project/sewar\\n[32] FID. How to Implement the Frechet Inception Distance (FID) for\\nEvaluating GANs. Accessed: Jul. 15, 2022. [Online]. Available:\\nhttps://machinelearningmastery.com/how-to-implement-the-frechet-\\ninception-distance-fid-from-scratch\\n[33] PSNR. Peak to Signal Noise Ratio. Accessed: Jul. 15, 2022. [Online].\\nAvailable: https://sonalsart.com/what-is-psnr\\n[34] R. Vallejos, J. Pérez, A. M. Ellison, and A. D. Richardson, ‘‘A spatial\\nconcordance correlation coefficient with an application to image analysis,’’\\n2019, arXiv:1905.05016.\\n[35] SSIM. Structural Similarity Index. Accessed: Jul. 15, 2022. [Online].\\nAvailable: https://medium.com/srm-mic/all-about-structural-similarity-\\nindex-ssim-theory-code-in-pytorch-6551b455541e\\n[36] VIF. Visual Information Fidelity . Accessed: Jul. 15, 2022. [Online].\\nAvailable: https://www.sciencedirect.com/topics/computer-science/\\nvisual-information-fidelity\\n[37] SCC. Pearson Correlation Coefficient Formula. Accessed: Aug. 15, 2022.\\n[Online]. Available: https://www.cuemath.com/correlation-coefficient-\\nformula\\n[38] ADAM. Adam Optimizer in Tensorflow. Accessed: Jan. 13, 2023. [Online].\\nAvailable: https://www.geeksforgeeks.org/adam-optimizer-in-tensorflow\\n[39] Jeff’s License Plates. Jeffsplates. Accessed: Aug. 17, 2022. [Online].\\nAvailable: https://www.jeffsplates.ca/wp-content/uploads/2018/07/\\nE5E421DF-5108-456C-AE5B-A479BA65A1B2.jpeg\\n[40] U.K. European License Plate. European License Plates. Accessed:\\nAug. 17, 2022. [Online]. Available: https://www.customeuropeanplates.\\ncom/images/uk-license-plate.jpg\\n[41] Use of Disclosure of Personal Data, Section 24 and 27. Thai\\nGovernment Gazette. Accessed: Aug. 20, 2022. [Online].\\nAvailable: https://data.opendevelopmentmekong.net/dataset/78c90118-\\n6671-4c19-afe1-7bfbace4d46a/resource/ec616be5-9fbf-\\n4071-b4b5-cb1f3e46e826/download/entranslation_\\nof_the_personal_data_protection_act_0.pdf\\n[42] Offence Relating to Documents. Chapter 3, Section 264. Thailand Penal\\nCode Thai Criminal Law. Accessed: Nov. 25, 2022. [Online]. Available:\\nhttps://www.samuiforsale.com/law-texts/thailand-penal-code.html#3\\n[43] Belgian License Plates. Kaggle. Accessed: Nov. 24, 2022. [Online]. Avail-\\nable: https://www.kaggle.com/datasets/aladdinss/license-plate-annotated-\\nimage-dataset\\n[44] U.S. License Plates. Kaggle. Accessed: Nov. 24, 2022. [Online]. Available:\\nhttps://www.kaggle.com/datasets/tolgadincer/us-license-plates\\nWUTTINAN SEREETHAVEKUL received the\\nB.Eng. degree in electrical engineering from\\nKasetsart University, Bangkok, Thailand, in 2009,\\nand the M.Eng. degree from Asian Institute of\\nTechnology, Pathum Thani, Thailand, in 2013,\\nwhere he is currently pursuing the D.Eng. degree\\nwith the Department of Industrial Systems Engi-\\nneering. His main research interests include\\nimage/video processing, data recovery, deep learn-\\ning, microelectronics, and computer architecture.\\nMONGKOL EKPANYAPONG received the\\nB.Eng. degree from Chulalongkorn University,\\nThailand, in 1997, the M.Eng. degree from Asian\\nInstitute of Technology, Thailand, in 2000, and the\\nM.Sc. and Ph.D. degrees from Georgia Institute\\nof Technology, in 2003 and 2006, respectively.\\nFrom 1997 to 1998, he was a System Engineer\\nwith United Communication Network, Thailand.\\nFrom 2006 to 2009, he was a Senior Computer\\nArchitect with the Core 2 Architecture Design\\nTeam, Intel Corporation, USA. He joined the School of Engineering and\\nTechnology, Asian Institute of Technology, in 2009, where he is currently\\nan Associate Professor. His research interests include microarchitecture,\\nembedded systems, mechatronics, deep learning, and computer vision.\\nVOLUME 11, 2023 26685'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-11-04T18:57:35+00:00', 'title': 'Arcega_CV', 'moddate': '2025-11-04T18:57:35+00:00', 'keywords': 'DAG3vzieRD8,BAEa-_UscSk,0', 'author': 'Lance Angelo Arcega', 'source': '..\\\\data\\\\pdf_files\\\\Arcega_CV.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'Arcega_CV.pdf', 'file_type': 'pdf'}, page_content='L A N C E  A N G E L O  P  A R C E G A\\nK E Y  S K I L L S P r o g r a m m i n g  L a n g u a g e s  ( J a v a ,\\nJ S ,  P y t h o n ,  M y S Q L )\\nD a t a  A n a l y s i s\\nO r g a n i z a t i o n a l  a n d  t i m e\\nm a n a g e m e n t  s k i l l s .  \\nP r o b l e m - s o l v i n g .\\nT e a m  M a n a g e m e n t / S e r v i c e\\nA t t e n t i o n  t o  d e t a i l s\\nM i c r o s o f t  3 6 5\\nE D U C A T I O N A u g  2 0 2 2  -  P r e s e n tB a c h e l o r  o f  S c i e n c e  i n  C o m p u t e r  S c i e n c e\\nA n g e l e s  U n i v e r s i t y  F o u n d a t i o n\\nS p e c i a l i z a t i o n  i n  D a t a  S c i e n c e\\nC o l l e g e  S c h o l a r\\nP R O J E C T S J a n  2 0 2 5  -  D e c  2 0 2 5R i c e  P r o d u c t i o n  P r e d i c t i o n  u s i n g  R F + G P R  H y b r i d\\nA  w e b  a p p l i c a t i o n  d e s i g n e d  t o  a s s i s t  p o l i c y m a k e r s  i n  p r e d i c t i n g  p o t e n t i a l  r i c e\\np r o d u c t i o n  b a s e d  o n  s p e c i f i c  c o n d i t i o n s  a n d  p a r a m e t e r s .\\nO n e  o f  t h e  d e v e l o p e r s  r e s p o n s i b l e  f o r  d a t a  c o l l e c t i o n ,  d i s a g g r e g a t i o n ,  a n d\\np r e p r o c e s s i n g  t o  t r a i n  t h e  h y b r i d  m o d e l  u s e d  f o r  g e n e r a t i n g  p r e d i c t i o n s .\\nJ a n  2 0 2 3  -  M a y  2 0 2 3I r i s  C h a t  A p p\\nA  c l o u d - b a s e d  m e s s e n g e r  a p p l i c a t i o n  d e v e l o p e d  u s i n g  C #  f o r  t h e  f r o n t  e n d\\na n d  M i c r o s o f t  A z u r e  S Q L  f o r  t h e  b a c k  e n d .\\nO n e  o f  t h e  d e v e l o p e r s  r e s p o n s i b l e  f o r  b u i l d i n g  t h e  c h a t  s y s t e m  a n d  d e p l o y i n g\\nt h e  i n t e g r a t i o n  b e t w e e n  t h e  a p p l i c a t i o n  a n d  t h e  d a t a b a s e .\\nF e b  2 0 2 4  -  D e c  2 0 2 4A U F  B a r a n g a y  E M R\\nA n  E l e c t r o n i c  M e d i c a l  R e c o r d  s y s t e m  d e s i g n e d  t o  a u t o m a t e ,  s t r e a m l i n e ,  a n d\\no r g a n i z e  m e d i c a l  d a t a .\\nC o - d e v e l o p e r  o f  t h e  s y s t e m ,  i n c l u d i n g  t h e  i m p l e m e n t a t i o n  o f  a n  o n l i n e  f o r m\\nt h a t  c a n  b e  c o n v e r t e d  i n t o  a  p r i n t e d  c o p y  i n  c o m p l i a n c e  w i t h  D e p a r t m e n t  o f\\nH e a l t h  ( D O H )  s t a n d a r d s .\\nS U M M A R Y\\nI  a m  a  f o u r t h - y e a r  C o m p u t e r  S c i e n c e  U n d e r g r a d u a t e  s p e c i a l i z i n g  i n  D a t a  S c i e n c e  a t\\nA n g e l e s  U n i v e r s i t y  F o u n d a t i o n .  O v e r  t h e  c o u r s e  o f  m y  c o l l e g e  u n d e r t a k i n g ,  I\\nd e v e l o p e d  a  d e e p  u n d e r s t a n d i n g  i n  d a t a  e n g i n e e r i n g ,  d a t a  a n a l y s i s ,  A I  i n t e g r a t i o n\\nt h r o u g h  m a c h i n e  l e a r n i n g  a n d  d e e p  l e a r n i n g .\\na r c e g a . l a n c e a n g e l o @ o u t l o o k . c o m  |  9 1 8 - 6 4 0 - 9 0 9 1  |  C i t y  o f  S a n  F e r n a n d o ,  P a m p a n g a ,  P h i l i p p i n e s  \\nh t t p s : / / w w w . l i n k e d i n . c o m / i n / l a n c e - a n g e l o - a r c e g a /\\nG e s t u r a  O S S e p  2 0 2 5  -  O c t  2 0 2 5\\nD e v e l o p e d  a n  a p p l i c a t i o n  t h a t  u s e s  c o m p u t e r  v i s i o n  t o  c o n t r o l  t h e  c u r s o r\\nt h r o u g h  h a n d  g e s t u r e s ,  u t i l i z i n g  a  c a m e r a  t o  d e t e c t  h a n d  m o v e m e n t s  a n d\\np e r f o r m  m o u s e  f u n c t i o n s .\\nC E R T I F I C A T I O N A Z 9 0 0  A z u r e  F u n d a m e n t a l s\\nC r e d e n t i a l  I D :  A 7 F 2 4 A 8 9 1 0 4 A D E 0 5'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-19T21:22:00+08:00', 'author': 'JANSEN  CRUZ', 'moddate': '2025-11-19T21:22:00+08:00', 'source': '..\\\\data\\\\pdf_files\\\\Jansen_Cruz_CV.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'Jansen_Cruz_CV.pdf', 'file_type': 'pdf'}, page_content='JANSEN C. CRUZ \\njansen.c.cruz@gmail.com | LinkedIn | GitHub | +63-921-723-8199 \\nEDUCATION \\nAngeles University Foundation – BS Computer Science       2022 - 2026 \\nUniversity Scholar (President’s Lister), BYA Scholar, Ranked 1st in Program (2022 – Present) \\nRelevant Coursework: Artificial Intelligence, Machine Learning, Data Science, Software Engineering \\nSTI College Angeles – IT in Mobile App and Web Development     2020 - 2022 \\nRanked 3rd in the graduating batch \\n \\nPROJECTS \\nLicense Plate AI Deblurring – AI/ML Engineer, Backend Developer | Thesis                July 2025 – Oct 2025 \\n• Designed and trained deep learning models achieving 96.2% accuracy in distortion classification \\n(CNN) and 90% accuracy license plate recognition (CRNN), enhancing the reliability and \\neffectiveness of traffic enforcement in low-quality surveillance conditions. \\n• Developed 4x distortion-specific GANs using Residual Attention U-Net architectures that restored \\nimage quality by up to 84% (SSIM), effectively reducing motion blur, low-light artifacts, and \\ncompression noise critical for real-world Philippines license recognition. \\n• Optimized end-to-end pipeline performance to achieve real-time processing speeds of  \\n1.57s (GPU) / 2.42s (CPU), enabling practical deployment on resource-constrained devices.  \\nLoan Management System – Full-stack Developer | Freelance                      Aug 2025 – Sept 2025 \\n• Engineered a scalable financial web application using C# ASP .NET Core, MSSQL, and Entity \\nFramework Core, applying Clean Architecture and CQRS patterns for high maintainability. \\n• Delivered end-to-end features including loan, borrower, and branch management with automated \\npayment tracking and real-time KPI dashboards with interactive charts for actionable insights. \\n• Replaced manual Excel tracking with a secure, automated solution featuring role-based and real-time \\ncustomer tracking, reducing operational overhead by ~70%. \\n• Received positive client validation: “Files are secure and customer tracking is effortless.” \\nBrgy. Ninoy Aquino EMR – Full-stack Developer, Project Lead | Community Project                Feb 2024 – Dec 2024 \\n• Spearheaded a 5-member team to develop, deploy, and maintain a healthcare management system \\nusing C#, ASP .NET Core, Entity Framework Core, and MySQL, implementing MVC architecture to \\nreplace disorganized paper records and eliminate data loss risks.  \\n• Developed data analytics dashboards transforming raw health data into actionable insights using \\nreal-time visualizations, enabling evidence-based decision making for midwives. \\n• Integrated FINDRISC-based diabetes risk assessment module; validated through CSUQ surveys \\nwith a near-perfect usability score (mean = 1.06, where 1.00 is best) and “Best Imaginable” ratings \\nacross all dimensions – information quality, interface quality, and overall usability.  \\n \\nCERTIFICATIONS \\nMicrosoft AZ-900: Azure Fundamentals            May 2025 \\nIT Specialist – Cloud Computing                                 Oct 2025 \\nOracle Cloud Infrastructure: AI Foundations Associate                             Oct 2025 \\nOracle Cloud Infrastructure: Generative AI Professional                            Oct 2025 \\n \\nTECHNICAL SKILLS  \\nLanguages: Python, C#, JavaScript, Java, Dart, SQL, HTML, CSS \\nFrameworks: PyTorch, TensorFlow / Keras, LangChain, ASP .NET Core, Entity Framework Core, Django, Tailwind \\nDeveloper Tools: Git, GitHub, Anaconda, Jupyter, Visual Studio Code, Visual Studio, Hugging Face, Power BI \\n \\nACTIVITIES & AWARDS \\nHackathons: JPCS Hackathon Champion (2025), 2nd Place (2025), 3rd Place (2024), iCode 2nd Place (2021, 2022)')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edd13e6-d9aa-4354-ba7b-f7ff62aaa263",
   "metadata": {},
   "source": [
    "### Text splitting into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea686a81-3728-484a-85e8-972b69161dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents: list[Document], chunk_size=2000, chunk_overlap=500) -> list[Document]:\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"] \n",
    "    )\n",
    "\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ce740cc-651a-4d6e-bad3-36e929f6b089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 30 documents into 89 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: A Dataset and Model for Realistic License Plate Deblurring\n",
      "Haoyan Gong, Yuzheng Feng, Zhenrong Zhang, Xianxu Hou, Jingxin Liu, Siqi\n",
      "Huang and Hongbin Liu∗\n",
      "School of AI and Advanced Computing, Xi’an Ji...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}\n"
     ]
    }
   ],
   "source": [
    "chunks = split_documents(all_pdf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17851aaf-dec7-427f-ad4a-df3820faccf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='A Dataset and Model for Realistic License Plate Deblurring\\nHaoyan Gong, Yuzheng Feng, Zhenrong Zhang, Xianxu Hou, Jingxin Liu, Siqi\\nHuang and Hongbin Liu∗\\nSchool of AI and Advanced Computing, Xi’an Jiaotong-Liverpool University\\n{haoyan.gong21, yuzheng.feng21, zhenrong.zhang21}@student.xjtlu.edu.cn, {xianxu.hou, jingxin.liu,\\nsiqi.huang, hongbin.liu}@xjtlu.edu.cn\\nAbstract\\nVehicle license plate recognition is a crucial task\\nin intelligent traffic management systems. How-\\never, the challenge of achieving accurate recogni-\\ntion persists due to motion blur from fast-moving\\nvehicles. Despite the widespread use of image\\nsynthesis approaches in existing deblurring and\\nrecognition algorithms, their effectiveness in real-\\nworld scenarios remains unproven. To address this,\\nwe introduce the first large-scale license plate de-\\nblurring dataset named License Plate Blur (LP-\\nBlur), captured by a dual-camera system and pro-\\ncessed through a post-processing pipeline to avoid\\nmisalignment issues. Then, we propose a Li-\\ncense Plate Deblurring Generative Adversarial Net-\\nwork (LPDGAN) to tackle the license plate de-\\nblurring: 1) a Feature Fusion Module to integrate\\nmulti-scale latent codes; 2) a Text Reconstruction\\nModule to restore structure through textual modal-\\nity; 3) a Partition Discriminator Module to en-\\nhance the model’s perception of details in each let-\\nter. Extensive experiments validate the reliabil-\\nity of the LPBlur dataset for both model training\\nand testing, showcasing that our proposed model\\noutperforms other state-of-the-art motion deblur-\\nring methods in realistic license plate deblurring\\nscenarios. The dataset and code are available at\\nhttps://github.com/haoyGONG/LPDGAN.\\n1 Introduction\\nEfficient recognition of vehicle license plates is crucial for\\nintelligent traffic management systems, however real-world\\nscenarios often pose a significant challenge due to motion\\nblur. This blur, making license plates unreadable, is espe-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='and testing, showcasing that our proposed model\\noutperforms other state-of-the-art motion deblur-\\nring methods in realistic license plate deblurring\\nscenarios. The dataset and code are available at\\nhttps://github.com/haoyGONG/LPDGAN.\\n1 Introduction\\nEfficient recognition of vehicle license plates is crucial for\\nintelligent traffic management systems, however real-world\\nscenarios often pose a significant challenge due to motion\\nblur. This blur, making license plates unreadable, is espe-\\ncially problematic in situations involving high-speed vehicles\\nor low-light conditions. Such issues are exacerbated during\\nnighttime or in bad weather, resulting in considerable motion\\nblur in captured images. To tackle these issues, our study in-\\ntroduces a comprehensive dataset and a novel model tailored\\nfor realistic license plate deblurring.\\n∗Corresponding author\\nMSSNet LBAGMIMO-UnetBlurred \\nLicense Plate Ground Truth\\nLPDGAN\\n(Ours)\\nFigure 1: The visual deblurring results of several state-of-the-art\\nmodels and our model for real-world motion blurred license plate\\nimages.\\nImage deblurring is a key task in computer vision, focused\\non restoring blurred images to clear ones for accurate obser-\\nvation and identification. The progress in this field heavily\\ndepends on the development of relevant datasets. Current\\nmethods for creating deblurring datasets fall into three main\\ncategories: (1) synthetic blurring using blur kernels [Sun\\net al. , 2013; K ¨ohler et al. , 2012; Lai et al. , 2016 ], which\\nleads to a lack of generalization capability for models trained\\non these synthesized images when applied to real-world im-\\nages. (2) The generation of blurred images from sharp frames\\nvia averaging or fusion [Nah et al., 2017; Shen et al., 2019;\\nJiang et al. , 2020 ], which doesn’t fully mimic real-world\\noverexposure outliers [Chang et al., 2021]. (3) Lastly, beam-\\nsplitting systems capture sharp and blurred image pairs via\\ncamera shake [Rim et al., 2020], with potential issues in color'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='leads to a lack of generalization capability for models trained\\non these synthesized images when applied to real-world im-\\nages. (2) The generation of blurred images from sharp frames\\nvia averaging or fusion [Nah et al., 2017; Shen et al., 2019;\\nJiang et al. , 2020 ], which doesn’t fully mimic real-world\\noverexposure outliers [Chang et al., 2021]. (3) Lastly, beam-\\nsplitting systems capture sharp and blurred image pairs via\\ncamera shake [Rim et al., 2020], with potential issues in color\\naccuracy and alignment. Each approach contributes to the\\nfield but also has inherent limitations impacting the realism\\nand utility of the datasets.\\nWith the advent of deep learning, numerous convolutional\\nneural network (CNN)-based methods have surfaced [Sun et\\nal., 2015; Gong et al., 2017; Tao et al., 2018; Shen et al.,\\n2019; Zhang et al. , 2023 ], playing an essential role in the\\nmotion deblurring task. Recently, the proposal of Genera-\\ntive Adversarial Networks (GAN) has also profoundly im-\\npacted the image deblurring field[Ramakrishnan et al., 2017;\\nKupyn et al., 2018; Kupyn et al., 2019; Zhao et al., 2022].\\nDespite these advancements, deblurring license plate images\\nremains a significant challenge, primarily due to the lack of\\nlarge-scale, tailored datasets. The complexity of license plate\\nblurring, with its more severe degradation compared to stan-\\ndard motion blur, poses an additional challenge. To better jus-\\ntify the performance of existing image deblurring algorithms\\non real-world blurred license plate images, we evaluate the\\nperformance of several state-of-the-art deblurring algorithms\\narXiv:2404.13677v1  [cs.CV]  21 Apr 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='with blurred license plate images. As shown in Figure 1, we\\ncan conclude that all these methods fail to perform well in this\\ntask. It underscores the necessity for further research specifi-\\ncally targeting real-world vehicle license plate deblurring.\\nTo address these challenges, we present a comprehensive\\nsolution consisting of a large-scale paired license plate dataset\\nand a dedicated license plate deblurring model. Our data col-\\nlection employs a dual-camera setup with different shutter\\nspeeds to capture sharp and blurred images simultaneously,\\neliminating color deviations and enabling post-processing\\nalignment. Our innovative end-to-end model leverages an en-\\ncoder and latent fusion module for handling multi-scale la-\\ntent codes, featuring the Swin transformer block ( [Liu et al.,\\n2021]) for effective long-range modeling. To enhance let-\\nter reconstruction and text legibility, we introduce a partition\\ndiscriminator assessing per-letter sharpness. Extensive ex-\\nperiments using our LPBlur dataset, including metrics such\\nas L1 loss, Peak Signal-to-Noise Ratio (PSNR), Structural\\nSimilarity Index (SSIM), Perceptual Loss (PerL), and Text\\nLevenshtein Distance (TLD) [Levenshtein and others, 1966],\\nvalidate its suitability for training and testing. Our proposed\\nmodel outperforms state-of-the-art motion deblurring meth-\\nods in real-world license plate deblurring scenarios.\\nIn summary, our main contributions are as follows:\\n• We present a real-world sharp-blurred license plate\\ndataset, named LPBlur. This dataset consists of 10,288\\npaired images, meticulously collected under diverse\\nreal-road scenarios using our designed dual-camera sys-\\ntem, and corrected by a post-processing pipeline.\\n• We introduce a novel LPDGAN, a license plate deblur-\\nring model that leverages multi-scale latent codes as ref-\\nerences. It incorporates both a partition discriminator\\nand text reconstruction techniques, which enhance the\\nmodel’s capability to generate high-quality license plate'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='dataset, named LPBlur. This dataset consists of 10,288\\npaired images, meticulously collected under diverse\\nreal-road scenarios using our designed dual-camera sys-\\ntem, and corrected by a post-processing pipeline.\\n• We introduce a novel LPDGAN, a license plate deblur-\\nring model that leverages multi-scale latent codes as ref-\\nerences. It incorporates both a partition discriminator\\nand text reconstruction techniques, which enhance the\\nmodel’s capability to generate high-quality license plate\\nimages through spatial architecture and textual informa-\\ntion, respectively.\\n• Extensive experiments demonstrate that our dataset LP-\\nBlur is highly effective for model training and evalu-\\nation. Compared to other state-of-the-art (SOTA) de-\\nblurring models, our proposed LPDGAN can achieve\\n21.24% license plate recognition accuracy improve-\\nment.\\n2 Related Work\\n2.1 Image Deblurring Datasets\\nImage deblurring relies on paired sharp-blur image datasets.\\nTraditionally, blurred images are generated by convolving\\nsharp images with uniform or non-uniform blur kernels\\n[Levin et al. , 2009; Sun et al. , 2013; K ¨ohler et al. , 2012;\\nLai et al., 2016]. Consequently, some researchers attempt to\\ncapture sequences of sharp frames while vibrating the cam-\\nera, averaging or fusing such sequences of frames into cor-\\nresponding motion-blurred images [Nah et al., 2017; Shen et\\nal., 2019; Jiang et al. , 2020; Noroozi et al. , 2017 ]. HIDE\\ndataset [Shen et al., 2019 ] is created by averaging 11 con-\\nsecutive frames, with the central frame serving as the sharp\\nimage. The same strategy is employed in the collection of the\\nBlur-DVS dataset [Jiang et al., 2020] and MSCNN (WILD)\\ndataset [Noroozi et al., 2017]. However, models lack general-\\nizability to real-world scenarios when they are trained on such\\nsynthetic datasets generated using the aforementioned meth-\\nods. Recently, certain researchers gathered authentic pairs of\\nsharp-blur images employing beam-splitting systems. They'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='secutive frames, with the central frame serving as the sharp\\nimage. The same strategy is employed in the collection of the\\nBlur-DVS dataset [Jiang et al., 2020] and MSCNN (WILD)\\ndataset [Noroozi et al., 2017]. However, models lack general-\\nizability to real-world scenarios when they are trained on such\\nsynthetic datasets generated using the aforementioned meth-\\nods. Recently, certain researchers gathered authentic pairs of\\nsharp-blur images employing beam-splitting systems. They\\nposition two cameras at a fixed angle to ensure that both im-\\nages share the same visual field, as described in works by Rim\\net al. and Li et al.. However, this approach can lead to color\\ncast discrepancies in the paired images due to inherent issues\\nwith beam-splitting systems.\\n2.2 Blind Deblurring\\nThe majority of conventional approaches employ priors of\\nnatural images to estimate latent images or blur kernels [Fer-\\ngus et al. , 2006; Shan et al. , 2008; Cho and Lee, 2009;\\nRen et al., 2018; Whyte et al., 2012 ]. However, the afore-\\nmentioned techniques have certain limitations by predicat-\\ning upon the assumption of uniform image blur. To address\\nthis issue, some methods [Ren et al., 2017; Hyun Kim et al.,\\n2013] estimate blur kernels at a pixel level, thereby accom-\\nmodating more complex blurring situations.\\nWith the advent of deep learning technologies, significant\\nstrides have been made in image deblurring, applying deep\\nlearning to predict blur kernels or latent images to procure\\nclear images. In the work of [Sun et al. , 2015 ], a method\\nbased on CNNs is proposed to predict the probability distri-\\nbution of block-level motion blur. Gong et al. introduces a\\nmethod to directly estimate the motion flow of blurred im-\\nages, recovering non-blurred images from the estimated mo-\\ntion flow. MIMO-UNet [Cho et al., 2021] deploys a multi-\\ninput-multi-output single Unet network to simulate multi-\\nlevel Unet for noise reduction across various image scales.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='clear images. In the work of [Sun et al. , 2015 ], a method\\nbased on CNNs is proposed to predict the probability distri-\\nbution of block-level motion blur. Gong et al. introduces a\\nmethod to directly estimate the motion flow of blurred im-\\nages, recovering non-blurred images from the estimated mo-\\ntion flow. MIMO-UNet [Cho et al., 2021] deploys a multi-\\ninput-multi-output single Unet network to simulate multi-\\nlevel Unet for noise reduction across various image scales.\\nMSSNet [Kim et al., 2022] enhances deblurring network per-\\nformance by using a stage configuration reflecting blur scales,\\nan inter-scale information propagation scheme, and a pixel-\\nshuffle-based multi-scale scheme. XYDeblur [Ji et al., 2022]\\nfurther augments network efficiency and deblurring perfor-\\nmance by employing rotated and shared kernels within the\\ndecoder.\\n2.3 GAN-Based Deblurring\\nIn recent years, following the inception of GANs, their ap-\\nplication in the domain of image deblurring has achieved\\nremarkable success. DeblurGAN [Kupyn et al., 2018 ] first\\npresents an end-to-end learning method for motion deblur-\\nring, and also introduces a new method for blur generation.\\nDeblurGAN-v2 [Kupyn et al., 2019] introduces a dual-scale\\ndiscriminator based on a relative conditional GAN framework\\nand incorporates a feature pyramid into the deblurring pro-\\ncess, which permits the flexible substitution of the backbone\\nnetwork. MSG-GAN [Karnewar and Wang, 2020] addresses\\nthe issue of insufficient overlap between the true and false\\nsupport distributions during the transfer from discriminator to\\ngenerator in GANs by allowing multi-scale gradient networks\\nfrom the discriminator to the generator. FCL-GAN [Zhao\\net al. , 2022 ] designs a lightweight domain conversion unit\\n(LDCU) and a parameter-free frequency-domain contrastive'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='Pedestrian Bridge\\nTrigger\\nPower\\nPower\\n(a) (b)\\nDenosing\\nGeometrical \\nAlignment\\nLicense Plate \\nCropping\\nCam S Cam L\\nSynchronizer\\nCam L\\nCam L\\nCam L\\nCam L\\nOutput from Cam L\\nOutput from Cam L\\nOutput from Cam S\\nOutput from Cam S\\nCam L\\nCam L\\nCam S\\nCam S\\nCam S\\nCam S\\nCam S\\nCam S\\nFigure 2: (a) A schematic diagram of the paired image acquisition system collecting data in a pedestrian bridge. (b) The pipeline of paired\\nimages post-processing.\\nunit (PFCU) for lightweight property and performance su-\\nperiority. The aforementioned methods can handle standard\\nblurred images, but they struggle to deliver satisfactory re-\\nsults on license plate blurring with very severe degradation.\\nWe propose an end-to-end generative model that accommo-\\ndates multi-scale inputs and outputs, employing several novel\\nmodules to accomplish the license plate deblurring task bet-\\nter.\\n3 Proposed LPBlur Dataset\\n3.1 Data Collection\\nCauses of Motion Blur.Motion blur refers to the percepti-\\nble streaking effect observed when capturing the movement\\nof objects. In the capturing process, the correlation be-\\ntween the amount of light entering the photosensitive compo-\\nnent and the camera’s basic parameters satisfies the following\\nequation:\\nLa ∝ SL × ISO × Et × (Ap)2 , (1)\\nwhere La denotes the number of photons received by the\\ncamera, SL represents the light intensity of the scene, ISO\\nrepresents the camera ISO value, Et is the exposure time, Ap\\ndenotes the camera aperture size. Cameras adjust these pa-\\nrameters automatically within limits depending on the light-\\ning situation. For example, cameras increase their aperture\\nand exposure time in low-lighting settings to capture enough\\nlight. Fast-moving objects leave trajectories within a single\\nframe during this extended exposure time, resulting in a mo-\\ntion blur effect.\\nPaired Image Acquisition. To collect paired sharp-blur\\nimages, we use two identical scientific cameras that are set\\nwith different exposure times. As shown in Figure 2a, cam-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='rameters automatically within limits depending on the light-\\ning situation. For example, cameras increase their aperture\\nand exposure time in low-lighting settings to capture enough\\nlight. Fast-moving objects leave trajectories within a single\\nframe during this extended exposure time, resulting in a mo-\\ntion blur effect.\\nPaired Image Acquisition. To collect paired sharp-blur\\nimages, we use two identical scientific cameras that are set\\nwith different exposure times. As shown in Figure 2a, cam-\\neras S and L are fixed parallel on a tripod to maintain hori-\\nzontally to the ground. Specifically, Camera S is set by an\\nextremely short exposure time Ets, employed for the collec-\\ntion of sharp images, while Camera L is set by a relatively\\nlonger exposure time Etl for the acquisition of blurred im-\\nages. Both of these cameras are interfaced with a computer\\nvia a synchronizer, which ensures the synchronization of the\\nstart of exposures, and both cameras capture the same scene.\\nScenes are taken in a variety of locations, including above,\\non the right side, and on the left side of roadways, to guaran-\\ntee the dataset’s diversity. Also, depending on the road and\\nthe illumination conditions, we dynamically modulate cam-\\nera exposure time according to the subsequent equation:\\nv · Et = b · D\\np · f , (2)\\nwhere v denotes vehicular velocity, Et represents exposure\\ntime, b represents pixels blurred, D is the distance between\\nvehicle and camera, f denotes camera focal length, and p is\\npixel edge length on the sensor. Given that the actual speeds\\nof individual vehicles are indeterminable, we standardized\\nimage captures on high-speed road sections with a regulatory\\nspeed limit of 70 km/h to ensure that D remained within a\\nrange of 10-20 meters.\\nMoreover, to ensure equality in exposure between two\\ncameras, we make their exposure times and ISO values to\\nsatisfy the following equation:\\nISOs\\nISOl\\n= Etl\\nEts\\n, (3)\\nwhere ISO s is the ISO value for Camera S and ISO l Cam-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='pixel edge length on the sensor. Given that the actual speeds\\nof individual vehicles are indeterminable, we standardized\\nimage captures on high-speed road sections with a regulatory\\nspeed limit of 70 km/h to ensure that D remained within a\\nrange of 10-20 meters.\\nMoreover, to ensure equality in exposure between two\\ncameras, we make their exposure times and ISO values to\\nsatisfy the following equation:\\nISOs\\nISOl\\n= Etl\\nEts\\n, (3)\\nwhere ISO s is the ISO value for Camera S and ISO l Cam-\\nera L. However, variations in ISO values can cause changes\\nin image noise levels, in post-processing, we incorporate a\\ndenoising step for sharp images.\\n3.2 Data Post-processing\\nAs shown in Figure 2b, the paired image post-processing in-\\ncludes denoising, geometrical alignment, and license plate\\ncropping.\\nDenoising. Due to the disparate ISO settings of the two\\ncameras, Camera S and Camera L captured images with a\\ndifferent noise level. Consequently, during the conversion of\\nRAW images to RGB format, wavelet denoising [Liu et al.,\\n2020] is employed after white-balancing, color mapping, and\\ngamma correction.\\nGeometrical Alignment. Cameras S and L capture sharp\\nand blurred image pairs with slight horizontal misalignment\\neven though they are closely aligned left-to-right to minimize\\nthe difference. To align these image pairs perfectly, we first\\ntake a static image pair without any moving vehicles as the\\nreference pair for each scene. Then, the Enhanced Corre-\\nlation Coefficient Maximization [Evangelidis and Psarakis,\\n2008] is adopted to estimate the geometric transformation be-\\ntween the sharp and blur of the reference image pair. Finally,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='Fake ImageLatent \\nFusion\\nLatent \\nFusion\\nConcat \\nLatent \\nCodes\\nPartition\\nDiscriminator \\nModule\\nGlobal\\nDiscriminator\\nConv\\nConv\\nα β\\nLower-Res Feature\\nHigher-Res \\nFeature\\nLatent Fusion Module\\nIdentity\\nSwin \\nTransformer \\nEncoder\\nSwin \\nTransformer \\nEncoder\\nSwin \\nTransformer \\nEncoder\\nSwin Transformer \\nDecoder\\nInput Image\\nInput Image\\nInput Image\\nInput Image\\nInput Image\\nInput Image\\nInput Image\\nInput Image\\nInput Image\\nInput Image\\nInput Image\\nInput Image\\nFake Image\\nFake Image\\nText \\nReconstruction \\nModule\\nFigure 3: Overview of the proposed Licence Plate Deblurring Generative Adversarial Network.\\nthe estimated geometric transformation is applied to the im-\\nage pairs in the same scene.\\nLicense Plate Cropping. A pre-trained YOLO v5 [Jocher,\\n2020] and a pre-trained CRNN [Shi et al., 2016] model are\\nfacilitated the detection and recognition of license plates un-\\nder standard conditions, both models are pre-trained on the\\nCCPD [Xu et al., 2018] license plate dataset. Following the\\ngeometrical alignment, the pre-trained YOLO v5 and CRNN\\ndetect and recognize the bounding box of each sharp plate in\\nthe paired images, both the sharp and blurred images are then\\ncropped using the same detected coordinates.\\nIn conclusion, we collect 10,288 image pairs, with an orig-\\ninal image size of 1920 × 1220. Post-processing crops image\\nsize to 224 × 112 with blur size ranging from 20-50 pixels.\\nAmong them, 5672 pairs are captured under normal light con-\\nditions and 4616 pairs under low light conditions, including\\n1,000 pairs under rainy environmental conditions. For more\\ninformation, please refer to the released dataset on the GitHub\\nrepository.\\n4 Method\\nOverview. The goal of our work is to improve the clarity of\\nlicense plate images using a meticulously crafted image-to-\\nimage translation framework, called LPDGAN. As depicted\\nin Figure 3, our approach first constructs a multi-scale fea-\\nture extraction and fusion module designed to encode input'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='1,000 pairs under rainy environmental conditions. For more\\ninformation, please refer to the released dataset on the GitHub\\nrepository.\\n4 Method\\nOverview. The goal of our work is to improve the clarity of\\nlicense plate images using a meticulously crafted image-to-\\nimage translation framework, called LPDGAN. As depicted\\nin Figure 3, our approach first constructs a multi-scale fea-\\nture extraction and fusion module designed to encode input\\nblurred images effectively. Subsequently, an image decoder\\nis employed to generate sharp and high-quality images. To\\nfurther enhance the overall image quality, we integrate both a\\nglobal discriminator and a partition discriminator for adver-\\nsarial training. Additionally, we incorporate a text reconstruc-\\ntion module to enrich the semantic information embedded in\\nthe generated license plate images.\\n4.1 Multi-scale Feature Extraction and Latent\\nFusion Module\\nFeature Extraction. In real-world scenarios, license plate\\nimages affected by motion blur often exhibit intricate degra-\\ndations, including noise, low resolution, and ghosting effects.\\nOur feature encoder, denoted as E, is specifically used to\\naddress these degradations, extracting essential image fea-\\ntures for subsequent processing. In particular, the Swin trans-\\nformer block [Liu et al., 2021] is selected for its ability to cap-\\nture global information through self-attention mechanisms.\\nThis is crucial to resolve the elongated ghosting artifacts that\\noften appear in motion-blurred license plate images. To ad-\\ndress variations in license plate image sizes due to differing\\ncapture distances, our approach employs a multi-scale fea-\\nture extraction strategy, which is illustrated in Figure 3. This\\napproach facilitates the encoding of features at each scale,\\nwhich are represented as E(xi) for i = 1, 2, 3.\\nLatent Fusion. Based on the Spatial Feature Transform\\n(SFT) [Wanget al., 2018], we further propose a Latent Fusion\\nModule F (see Figure 3). This module is designed based on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='dress variations in license plate image sizes due to differing\\ncapture distances, our approach employs a multi-scale fea-\\nture extraction strategy, which is illustrated in Figure 3. This\\napproach facilitates the encoding of features at each scale,\\nwhich are represented as E(xi) for i = 1, 2, 3.\\nLatent Fusion. Based on the Spatial Feature Transform\\n(SFT) [Wanget al., 2018], we further propose a Latent Fusion\\nModule F (see Figure 3). This module is designed based on\\nan affine transformation to effectively integrate the obtained\\nmulti-scale features. Specifically, for the fusion of E(x1)\\nand E(x2), we first split E(x2) along the channel dimension.\\nEach part is then processed through a series of convolutional\\nlayers to derive the fusion parameters α and β. These pa-\\nrameters are employed to modulate E(x1) through scaling\\nand shifting operations. Moreover, we reintegrate the origi-\\nnal E(x2) using a skip connection, which is then combined\\nwith the modified E(x1) along the channel dimension. This\\nfusion process is also applied betweenE(x2) and E(x3). The\\ncorresponding formulas are provided below:\\nα, β= Conv (E(xi+1)sp1 + E(xi+1)sp2) ,\\nFi,i+1 = Concat (α ⊙ E(xi) +β, E(xi)) , (4)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='Fake \\nPartitions\\nReal \\nPartitions\\nRandom \\nSelection\\nPartition Discriminator Module\\nReal Image\\nFake Image\\nPartition \\nDiscriminator\\nP\\nFigure 4: The architecture of Partition Discriminator Module.\\nwhere E(xi+1)sp1 and E(xi+1)sp2 represent the two parts\\ninto which E(xi+1) is divided along the channel dimension.\\n4.2 Decoder and Discriminator\\nDecoder. As depicted in Figure 3, our decoder D is com-\\nposed of a sequence of Swin transformer blocks and patch\\nexpanding operations. Similar to the encoder, our decoder is\\ndesigned to generate sharp images in a multi-scale fashion,\\nand the output images are denoted as ˜y1, ˜y2 and ˜y3 accord-\\ningly.\\nDiscriminator. We design two discriminators: 1) the\\nGlobal Discriminator (Dg) enhances overall spatial and color\\ninformation in the restored images; 2) the Partition Discrimi-\\nnator (Dp) focuses specifically on refining character informa-\\ntion by examining n randomly selected partitions of letters\\nwithin the license plate image. The structure of Dp is shown\\nin Figure 4. It identifies and locates letter positions in both\\nthe real image y and the fake image ˜y. Following this, n\\npartition images are randomly chosen for evaluation by Par-\\ntition Discriminator. In the early stages of training, when our\\nmode’s capacity to produce sharp images is still developing,\\nthe generated image might not be recognized with high ac-\\ncuracy. To address this, an average partitioning approach is\\napplied to both y and ˜y, initially setting n to 7. As training\\nprogresses, a pre-trained YOLO v5 model is used for precise\\nletter detection and the number of partitions n is set to 3. In\\nour experiment, we employ WGAN-GP to train our model.\\nIn particular, the adversarial loss for the Global Discrimina-\\ntor can be formulated as follows:\\nLDg =E˜y [Dg (˜y)] − Ey [Dg (y)]\\n+ λgp1Eˆy\\nh\\x00\\n∥∇ˆyDg (ˆy)∥2 − 1\\n\\x012i\\n,\\nˆy = ϵ · ˜y + (1− ϵ) · y, ϵ∼ U [0, 1] .\\n(5)\\nSimilarly, for the Partition DiscriminatorDp, the formulation\\nis as follows:\\nLDp =E˜y [Dp (P (˜y))] − Ey [Dp (P (y))]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='letter detection and the number of partitions n is set to 3. In\\nour experiment, we employ WGAN-GP to train our model.\\nIn particular, the adversarial loss for the Global Discrimina-\\ntor can be formulated as follows:\\nLDg =E˜y [Dg (˜y)] − Ey [Dg (y)]\\n+ λgp1Eˆy\\nh\\x00\\n∥∇ˆyDg (ˆy)∥2 − 1\\n\\x012i\\n,\\nˆy = ϵ · ˜y + (1− ϵ) · y, ϵ∼ U [0, 1] .\\n(5)\\nSimilarly, for the Partition DiscriminatorDp, the formulation\\nis as follows:\\nLDp =E˜y [Dp (P (˜y))] − Ey [Dp (P (y))]\\n+ λgp2Eˆy\\nh\\x00\\n∥∇ˆyDp (P (ˆy))∥2 − 1\\n\\x012i\\n,\\nˆy = ϵ · P (˜y) + (1− ϵ) · P (y) , ϵ∼ U [0, 1] ,\\n(6)\\nwhere P is the partition operation, λgp1 and λgp2 are the\\nweighting parameters used to control the gradient penalty.\\nNote that we apply both discriminators to the outputs at three\\ndifferent scales.\\nIn addition to the adversarial loss, we also incorporate re-\\nconstruction loss Lrec, defined as follows:\\nLrec = λl1 ∥y − ˜y∥1 + λper ∥ψvgg (y) − ψvgg (˜y)∥2 , (7)\\nwhere the ψvgg represents a pre-trained VGG-19 network[Si-\\nmonyan and Zisserman, 2014 ], from which we use feature\\nmaps from the 8 th, 15th, and 22nd ReLU layers to compare\\nshallow textures and deep features between real and gener-\\nated images.\\n4.3 Text Reconstruction Module\\nWe also incorporate a Text Reconstruction Module T specif-\\nically designed to enhance our model’s ability to accurately\\ninterpret characters on license plate images. T merges the\\nDecoder’s intermediate featureFD with the fusion latent code\\nF2,3 along the channel dimension. This combined feature\\nthen traverses a series of convolutional and linear layers, re-\\nsulting in a vector that represents the recognized text. Con-\\ncurrently, a pre-trained CRNN model extracts ground-truth\\ntext vectors from real images. We calculate the L1 loss be-\\ntween the output vector from our Text Reconstruction Module\\nT and the ground-truth text vector. The loss is defined as:\\nLtext = ∥T (F2,3, FD) − ψcrnn (y)∥1 , (8)\\nwhere FD is the feature maps obtained from the middle layers\\nof Decoder, ψcrnn represents the pre-trained CRNN model.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='sulting in a vector that represents the recognized text. Con-\\ncurrently, a pre-trained CRNN model extracts ground-truth\\ntext vectors from real images. We calculate the L1 loss be-\\ntween the output vector from our Text Reconstruction Module\\nT and the ground-truth text vector. The loss is defined as:\\nLtext = ∥T (F2,3, FD) − ψcrnn (y)∥1 , (8)\\nwhere FD is the feature maps obtained from the middle layers\\nof Decoder, ψcrnn represents the pre-trained CRNN model.\\n4.4 Fully Objective\\nOur full objective is\\nL(E, D, F, Dg, Dp, T) =Lrec + λgLDg + λpLDp\\n+ λtLtext, (9)\\nwhere λg, λp and λt control the relative importance of the\\ndifferent objectives. We aim to solve:\\nE∗, F∗, D∗ = arg min\\nE,F,D,T\\nmax\\nDg,Dp\\nL(E, D, F, Dg, Dp, T).\\n(10)\\n5 Experiment\\n5.1 Experimental Setups\\nDataset setup. The proposed LPBlur dataset is partitioned\\ninto a training set with 9,288 image pairs, and a test set with\\n1,000 image pairs. The test set encompasses 500 pairs ac-\\nquired under normal light conditions and another 500 pairs\\ncaptured in low light conditions.\\nEvaluation metrics. To evaluate the image quality of de-\\nblurred images, we adopt three evaluation metrics: PSNR,\\nSSIM, and Perceptual Loss (PerL). The Perceptual Loss is\\nspecifically defined by comparing the generated images with\\nthe ground truth images at the output feature maps of sequen-\\ntial layers 8, 15, and 22 of a pre-trained VGG-19 model.\\nTo assess the recognisability of the generated license plate\\nimages, we calculate the Text Levenshtein Distance (TLD)\\n[Levenshtein and others, 1966 ] between the detected text of\\nthe generated images and the real images.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='Ground \\nTruth\\nLPDGAN \\n(ours)\\nLBAG\\nMSSNet\\nMIMO-Unet\\nPix2Pix\\nDeblurGAN-v2\\nBlurred \\nImage\\nNormal Light Low Light\\nFigure 5: Visual comparison of different deblurring methods on LPBlur dataset. The test scene is divided into normal light and low light.\\nSince the results in low light scenes are difficult to distinguish visually, we uniformly increase their brightness. The original brightness of the\\nlow light scene refers to the first line, which is Blur Image.\\nTable 1: Quantitative results of comparing motion deblurring models. “PerL” and “TLD” denote Perceptual Loss and Text Levenshtein\\nDistance, respectively.\\nScenario Normal Light Low Light\\nPerL↓ PSNR↑ SSIM↑ TLD↓ PerL↓ PSNR↑ SSIM↑ TLD↓\\nPix2Pix 5.57 28.89 0.6669 1.35 2.24 28.71 0.7491 2.53\\nDeblurGAN v2 8.02 28.51 0.5257 2.28 4.32 28.11 0.5451 4.34\\nMIMO-UNet 3.79 29.12 0.7448 1.02 1.65 29.03 0.8083 2.68\\nMSSNet 3.39 29.63 0.7891 0.62 2.74 29.62 0.8725 1.05\\nLBAG 3.34 29.24 0.7916 0.58 1.44 29.44 0.8889 1.13\\nLPDGAN (ours) 3.31 29.95 0.7950 0.57 1.01 30.96 0.9214 0.81\\nImplementation details. The shape of multi-scale input\\nimages for LPDGAN are (112, 224, 3), (56, 112, 3), and\\n(28, 56, 3) respectively. Random rain adding and random\\ncutout are utilized for data augmentation. The optimizer we\\nuse is Adam [Kingma and Ba, 2014 ]. The batch size is set\\nto 7. The initial learning rate is 10−4, and the linear weight\\ndecay is used after the100th epoch. All experiments are con-\\nducted on a GeForce RTX 3090 GPU.\\n5.2 Deblur Results\\nTo evaluate the deblur performance of our method, we com-\\npare LPDGAN with five SOTA methods: Pix2Pix [Isola et\\nal., 2017], DeblurGAN v2 [Kupyn et al., 2019], MIMO-Unet\\n[Cho et al., 2021], MSSNet [Kim et al., 2022], and LBAG[Li\\net al., 2023] on LPBlur.\\nFrom the results presented in Table 1, it can be observed\\nthat our LPDGAN outperforms all other models in both nor-\\nmal and low light conditions. In normal light conditions,\\nour LPDGAN achieves a PerL of 3.31, PSNR of 29.95, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='To evaluate the deblur performance of our method, we com-\\npare LPDGAN with five SOTA methods: Pix2Pix [Isola et\\nal., 2017], DeblurGAN v2 [Kupyn et al., 2019], MIMO-Unet\\n[Cho et al., 2021], MSSNet [Kim et al., 2022], and LBAG[Li\\net al., 2023] on LPBlur.\\nFrom the results presented in Table 1, it can be observed\\nthat our LPDGAN outperforms all other models in both nor-\\nmal and low light conditions. In normal light conditions,\\nour LPDGAN achieves a PerL of 3.31, PSNR of 29.95, and\\nSSIM of 0.795, which are superior to the latest deblurring\\nmethod LBAG. The performance gap becomes even more\\npronounced when dealing with low light images, with our\\nmodel exhibiting improvements of 29.8%, 4.5%, and 3.7%\\nin PerL, PSNR, and SSIM, respectively, compared to LBAG\\nand MSSNet.\\nFigure 5 provides a visual comparison between sets of\\nblurred and deblurred images under two light conditions. In\\nthe case of normal light, our LPDGAN effectively restores\\nlicense plates afflicted with severe motion blur, accurately\\ngenerating and reconstructing characters such as ‘D’, ‘O’ and\\n‘B’, as well as numbers like ‘7’, ‘1’, and ‘L’, which often\\npose challenges for other models, as shown in the 1st, 3rd\\nand 7th column of Figure 5. In low light scenarios, where\\nlicense plates are barely visible to the human eye, our model\\nexcels in generating details that significantly surpass the per-\\nformance of other models. This highlights the inadequacy of\\nmodels designed for minor blurs in large scenes when applied\\nto the deblurring of license plates, which are subject to more\\nsevere blurring.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='Blurred \\nLicense Plate\\nTrained on \\nSynthetic Data\\nTrained on \\nLPBlur Data\\nGround \\nTruth\\nFigure 6: Visual results comparison of LPDGAN trained on syn-\\nthetic data and LPBlur data.\\nNormal Light PerL ↓ PSNR↑ SSIM↑ TLD↓\\nSynthetic Data 3.45 28.74 0.7061 1.68\\nLPBlur Data 3.31 29.95 0.7950 0.57\\nLow Light PerL ↓ PSNR↑ SSIM↑ TLD↓\\nSynthetic Data 2.05 28.28 0.7933 2.65\\nLPBlur Data 1.01 30.96 0.9214 0.81\\nTable 2: Quantitative comparison using synthetic and LPBlur data\\nin normal and low light scenarios, respectively.\\n5.3 Text Recognition Results\\nWe further evaluate the plate text recognition accuracy based\\non those deblurred images. The CRNN [Shi et al. , 2016 ]\\nis incorporated for the recognition of generated and sharp\\nlicense plate characters. The 4th and 8th columns in Ta-\\nble 1 compare the TLD between the generated images and\\noriginal sharp images. In the context of normal light condi-\\ntions, LPDGAN exhibits comparable performance to LBAG\\nin terms of TLD but surpasses all other models. Under low\\nlight conditions, LPDGAN is the only model with a TLD\\nlower than 1. This implies that a pre-trained CRNN model,\\nwhen employed to recognize deblurred license plate images\\nproduced by LPDGAN, will obtain an average error in less\\nthan one character per instance. Consequently, LPDGAN has\\nthe best performance overall, demonstrating robust capability\\nin deblurring license plates across two scenarios.\\n5.4 Ablation Study\\nTo evaluate the effectiveness of each proposed module, a se-\\nries of ablation experiments are performed, which is shown\\nin Table 3. The omission of the Latent Fusion Module led to\\na decline in global metrics, underscoring its effectiveness in\\nfusing multi-scale features and improving the model’s perfor-\\nmance in restoring sharp images. Removing the Text Recon-\\nstruction Module resulted in a significant downturn in global\\nmetrics, particularly noticeable under low light conditions.\\nThis highlights the pivotal role of the Text Reconstruction'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='ries of ablation experiments are performed, which is shown\\nin Table 3. The omission of the Latent Fusion Module led to\\na decline in global metrics, underscoring its effectiveness in\\nfusing multi-scale features and improving the model’s perfor-\\nmance in restoring sharp images. Removing the Text Recon-\\nstruction Module resulted in a significant downturn in global\\nmetrics, particularly noticeable under low light conditions.\\nThis highlights the pivotal role of the Text Reconstruction\\nModule in enabling the model to have a deeper understand-\\ning and restoration capability for license plates affected by\\nsevere pixel disruption. Similarly, the exclusion of the Par-\\ntition Discriminator Module deteriorated global metrics and\\nnotably affected the SSIM metric to a greater extent. This\\nconfirms the module’s contribution to enhancing the model\\nfocus on the details of each letter on the license plate.\\nModel No.\\nNormal Light\\nLa. T e. P DPerL↓ PSNR↑ SSIM↑ TLD↓\\n1 ✓ ✓ 3.49 29.68 0.7883 0.69\\n2 ✓ ✓ 3.56 29.41 0.7797 0.72\\n3 ✓ ✓ 3.41 29.73 0.7829 0.61\\nLPDGAN ✓ ✓ ✓ 3.31 29.95 0.7950 0.57\\nModel No.\\nLow Light\\nLa. T e. P DPerL↓ PSNR↑ SSIM↑ TLD↓\\n1 ✓ ✓ 1.26 30.05 0.9165 0.92\\n2 ✓ ✓ 1.79 29.12 0.8861 1.45\\n3 ✓ ✓ 1.38 29.93 0.9012 1.01\\nLPDGAN ✓ ✓ ✓ 1.01 30.96 0.9214 0.81\\nTable 3: Ablations of LPDGAN on LPBlur. La.,Te. and PD de-\\nnote the Latent Fusion Module, Text Reconstruction Module and\\nPartition Discriminator Module, respectively.\\n5.5 Necessity of LPBlur\\nWe further demonstrate the importance of introducing a\\ndataset consisting of real blurred images for the task of li-\\ncense plate deblurring. To assess this, we employ different\\nblur kernels randomly to the sharp images in LPBlur dataset\\nand finally create a synthetic dataset. The result samples, il-\\nlustrated in Figure 6 and summarized in Table 2, clearly in-\\ndicate that LPDGAN trained on the synthetic dataset fails to\\neliminate real-world license plate blur effectively. This is ev-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='We further demonstrate the importance of introducing a\\ndataset consisting of real blurred images for the task of li-\\ncense plate deblurring. To assess this, we employ different\\nblur kernels randomly to the sharp images in LPBlur dataset\\nand finally create a synthetic dataset. The result samples, il-\\nlustrated in Figure 6 and summarized in Table 2, clearly in-\\ndicate that LPDGAN trained on the synthetic dataset fails to\\neliminate real-world license plate blur effectively. This is ev-\\nident both visually and in the metric evaluations, showcasing\\npoorer performance compared to when trained on the LPBlur\\ndataset.\\nThe aforementioned outcomes highlight a significant dis-\\nparity between synthetic and real-world license plate blur,\\nemphasizing that synthetic blurred image data cannot serve as\\na substitute for the LPBlur dataset. Thus, the LPBlur dataset\\nproves to be more effective in training models for real-world\\nlicense plate deblurring.\\n6 Conclusion\\nIn this paper, we study the issue of motion license plates de-\\nblurring. We introduce the first large-scale license plate de-\\nblurring dataset for this research and address color bias and\\nmisalignment problems through appropriate data collection\\nmethods and post-processing. Furthermore, given that the de-\\ngree of blur caused by vehicular motion substantially exceeds\\nthat induced by camera shake, we propose a model based on\\nmulti-scale input and output for license plate deblurring. This\\nincludes a latent fusion module, a supervision module for tex-\\ntual modality information, and a partition discriminator mod-\\nule. Experimental results indicate that our model performs\\nfavorably in comparison to current state-of-the-art deblurring\\nalgorithms. In the future, we intend to augment our dataset\\nwith license plates from a broader range of countries and re-\\ngions to enhance its diversity. Regarding the model, we in-\\ntend to incorporate modules that ensure the restoration capa-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='includes a latent fusion module, a supervision module for tex-\\ntual modality information, and a partition discriminator mod-\\nule. Experimental results indicate that our model performs\\nfavorably in comparison to current state-of-the-art deblurring\\nalgorithms. In the future, we intend to augment our dataset\\nwith license plates from a broader range of countries and re-\\ngions to enhance its diversity. Regarding the model, we in-\\ntend to incorporate modules that ensure the restoration capa-\\nbility for spatially complex characters, such as Chinese char-\\nacters.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='Acknowledgments\\nThis work was jointly supported by the National Natural Sci-\\nence Foundation of China (62201474 and 62206180), Suzhou\\nScience and Technology Development Planning Programme\\n(Grant No.ZXL2023171) and XJTLU Research Development\\nFunds (RDF-21-02-084, RDF-22-01-129, RDF-22-01-134,\\nand RDF-23-01-053).\\nEthical Statement\\nTo prevent the disclosure of personal privacy, all private in-\\nformation, including human faces and surrounding scenery, is\\nremoved from the images in the LPBlur, and only the license\\nplate number is retained. In addition, sensitive metadata in\\nthe image is removed, including GPS location, timestamp,\\netc.\\nReferences\\n[Chang et al., 2021] Meng Chang, Chenwei Yang, Huajun\\nFeng, Zhihai Xu, and Qi Li. Beyond camera motion\\nblur removing: How to handle outliers in deblurring.\\nIEEE Transactions on Computational Imaging , 7:463–\\n474, 2021.\\n[Cho and Lee, 2009] Sunghyun Cho and Seungyong Lee.\\nFast motion deblurring. In ACM SIGGRAPH Asia 2009\\npapers, pages 1–8. 2009.\\n[Cho et al., 2021] Sung-Jin Cho, Seo-Won Ji, Jun-Pyo\\nHong, Seung-Won Jung, and Sung-Jea Ko. Rethinking\\ncoarse-to-fine approach in single image deblurring. In\\nProceedings of the IEEE/CVF international conference on\\ncomputer vision, pages 4641–4650, 2021.\\n[Evangelidis and Psarakis, 2008] Georgios D Evangelidis\\nand Emmanouil Z Psarakis. Parametric image align-\\nment using enhanced correlation coefficient maximization.\\nIEEE transactions on pattern analysis and machine intel-\\nligence, 30(10):1858–1865, 2008.\\n[Fergus et al., 2006] Rob Fergus, Barun Singh, Aaron Hertz-\\nmann, Sam T Roweis, and William T Freeman. Removing\\ncamera shake from a single photograph. In Acm Siggraph\\n2006 Papers, pages 787–794. 2006.\\n[Gong et al., 2017] Dong Gong, Jie Yang, Lingqiao Liu,\\nYanning Zhang, Ian Reid, Chunhua Shen, Anton Van\\nDen Hengel, and Qinfeng Shi. From motion blur to mo-\\ntion flow: A deep learning solution for removing heteroge-\\nneous motion blur. In Proceedings of the IEEE conference'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='[Fergus et al., 2006] Rob Fergus, Barun Singh, Aaron Hertz-\\nmann, Sam T Roweis, and William T Freeman. Removing\\ncamera shake from a single photograph. In Acm Siggraph\\n2006 Papers, pages 787–794. 2006.\\n[Gong et al., 2017] Dong Gong, Jie Yang, Lingqiao Liu,\\nYanning Zhang, Ian Reid, Chunhua Shen, Anton Van\\nDen Hengel, and Qinfeng Shi. From motion blur to mo-\\ntion flow: A deep learning solution for removing heteroge-\\nneous motion blur. In Proceedings of the IEEE conference\\non computer vision and pattern recognition , pages 2319–\\n2328, 2017.\\n[Hyun Kim et al., 2013] Tae Hyun Kim, Byeongjoo Ahn,\\nand Kyoung Mu Lee. Dynamic scene deblurring. In Pro-\\nceedings of the IEEE international conference on com-\\nputer vision, pages 3160–3167, 2013.\\n[Isola et al., 2017] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou,\\nand Alexei A Efros. Image-to-image translation with con-\\nditional adversarial networks. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition ,\\npages 1125–1134, 2017.\\n[Ji et al., 2022] Seo-Won Ji, Jeongmin Lee, Seung-Wook\\nKim, Jun-Pyo Hong, Seung-Jin Baek, Seung-Won Jung,\\nand Sung-Jea Ko. Xydeblur: divide and conquer for sin-\\ngle image deblurring. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition,\\npages 17421–17430, 2022.\\n[Jiang et al., 2020] Zhe Jiang, Yu Zhang, Dongqing Zou,\\nJimmy Ren, Jiancheng Lv, and Yebin Liu. Learning event-\\nbased motion deblurring. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition,\\npages 3320–3329, 2020.\\n[Jocher, 2020] Glenn Jocher. YOLOv5 by Ultralytics, May\\n2020.\\n[Karnewar and Wang, 2020] Animesh Karnewar and Oliver\\nWang. Msg-gan: Multi-scale gradients for generative ad-\\nversarial networks. In Proceedings of the IEEE/CVF con-\\nference on computer vision and pattern recognition, pages\\n7799–7808, 2020.\\n[Kim et al., 2022] Kiyeon Kim, Seungyong Lee, and\\nSunghyun Cho. Mssnet: Multi-scale-stage network for\\nsingle image deblurring. In European Conference on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='pages 3320–3329, 2020.\\n[Jocher, 2020] Glenn Jocher. YOLOv5 by Ultralytics, May\\n2020.\\n[Karnewar and Wang, 2020] Animesh Karnewar and Oliver\\nWang. Msg-gan: Multi-scale gradients for generative ad-\\nversarial networks. In Proceedings of the IEEE/CVF con-\\nference on computer vision and pattern recognition, pages\\n7799–7808, 2020.\\n[Kim et al., 2022] Kiyeon Kim, Seungyong Lee, and\\nSunghyun Cho. Mssnet: Multi-scale-stage network for\\nsingle image deblurring. In European Conference on\\nComputer Vision, pages 524–539. Springer, 2022.\\n[Kingma and Ba, 2014] Diederik P Kingma and Jimmy Ba.\\nAdam: A method for stochastic optimization. arXiv\\npreprint arXiv:1412.6980, 2014.\\n[K¨ohler et al., 2012] Rolf K ¨ohler, Michael Hirsch, Betty\\nMohler, Bernhard Sch ¨olkopf, and Stefan Harmeling.\\nRecording and playback of camera shake: Benchmark-\\ning blind deconvolution with a real-world database. In\\nComputer Vision–ECCV 2012: 12th European Conference\\non Computer Vision, Florence, Italy, October 7-13, 2012,\\nProceedings, Part VII 12, pages 27–40. Springer, 2012.\\n[Kupyn et al., 2018] Orest Kupyn, V olodymyr Budzan,\\nMykola Mykhailych, Dmytro Mishkin, and Ji ˇr´ı Matas.\\nDeblurgan: Blind motion deblurring using conditional\\nadversarial networks. In Proceedings of the IEEE confer-\\nence on computer vision and pattern recognition , pages\\n8183–8192, 2018.\\n[Kupyn et al., 2019] Orest Kupyn, Tetiana Martyniuk, Junru\\nWu, and Zhangyang Wang. Deblurgan-v2: Deblurring\\n(orders-of-magnitude) faster and better. In Proceedings of\\nthe IEEE/CVF international conference on computer vi-\\nsion, pages 8878–8887, 2019.\\n[Lai et al., 2016] Wei-Sheng Lai, Jia-Bin Huang, Zhe Hu,\\nNarendra Ahuja, and Ming-Hsuan Yang. A comparative\\nstudy for single image blind deblurring. In Proceedings\\nof the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 1701–1709, 2016.\\n[Levenshtein and others, 1966] Vladimir I Levenshtein et al.\\nBinary codes capable of correcting deletions, insertions,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='the IEEE/CVF international conference on computer vi-\\nsion, pages 8878–8887, 2019.\\n[Lai et al., 2016] Wei-Sheng Lai, Jia-Bin Huang, Zhe Hu,\\nNarendra Ahuja, and Ming-Hsuan Yang. A comparative\\nstudy for single image blind deblurring. In Proceedings\\nof the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 1701–1709, 2016.\\n[Levenshtein and others, 1966] Vladimir I Levenshtein et al.\\nBinary codes capable of correcting deletions, insertions,\\nand reversals. In Soviet physics doklady, volume 10, pages\\n707–710. Soviet Union, 1966.\\n[Levin et al., 2009] Anat Levin, Yair Weiss, Fredo Durand,\\nand William T Freeman. Understanding and evaluating\\nblind deconvolution algorithms. In 2009 IEEE conference'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='on computer vision and pattern recognition , pages 1964–\\n1971. IEEE, 2009.\\n[Li et al., 2023] Haoying Li, Ziran Zhang, Tingting Jiang,\\nPeng Luo, Huajun Feng, and Zhihai Xu. Real-world deep\\nlocal motion deblurring. In Proceedings of the AAAI Con-\\nference on Artificial Intelligence, volume 37, pages 1314–\\n1322, 2023.\\n[Liu et al., 2020] Wei Liu, Qiong Yan, and Yuzhi Zhao.\\nDensely self-guided wavelet network for image denoising.\\nIn Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition Workshops , pages 432–\\n433, 2020.\\n[Liu et al., 2021] Ze Liu, Yutong Lin, Yue Cao, Han Hu,\\nYixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\\nSwin transformer: Hierarchical vision transformer using\\nshifted windows. In Proceedings of the IEEE/CVF in-\\nternational conference on computer vision, pages 10012–\\n10022, 2021.\\n[Nah et al., 2017] Seungjun Nah, Tae Hyun Kim, and Ky-\\noung Mu Lee. Deep multi-scale convolutional neural net-\\nwork for dynamic scene deblurring. In Proceedings of the\\nIEEE conference on computer vision and pattern recogni-\\ntion, pages 3883–3891, 2017.\\n[Noroozi et al., 2017] Mehdi Noroozi, Paramanand Chan-\\ndramouli, and Paolo Favaro. Motion deblurring in the\\nwild. In Pattern Recognition: 39th German Conference,\\nGCPR 2017, Basel, Switzerland, September 12–15, 2017,\\nProceedings 39, pages 65–77. Springer, 2017.\\n[Ramakrishnan et al., 2017] Sainandan Ramakrishnan,\\nShubham Pachori, Aalok Gangopadhyay, and Shan-\\nmuganathan Raman. Deep generative filter for motion\\ndeblurring. In Proceedings of the IEEE international con-\\nference on computer vision workshops, pages 2993–3000,\\n2017.\\n[Ren et al., 2017] Dongwei Ren, Wangmeng Zuo, David\\nZhang, Jun Xu, and Lei Zhang. Partial deconvolution with\\ninaccurate blur kernel. IEEE transactions on image pro-\\ncessing, 27(1):511–524, 2017.\\n[Ren et al., 2018] Wenqi Ren, Jiawei Zhang, Lin Ma, Jin-\\nshan Pan, Xiaochun Cao, Wangmeng Zuo, Wei Liu, and\\nMing-Hsuan Yang. Deep non-blind deconvolution via gen-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='deblurring. In Proceedings of the IEEE international con-\\nference on computer vision workshops, pages 2993–3000,\\n2017.\\n[Ren et al., 2017] Dongwei Ren, Wangmeng Zuo, David\\nZhang, Jun Xu, and Lei Zhang. Partial deconvolution with\\ninaccurate blur kernel. IEEE transactions on image pro-\\ncessing, 27(1):511–524, 2017.\\n[Ren et al., 2018] Wenqi Ren, Jiawei Zhang, Lin Ma, Jin-\\nshan Pan, Xiaochun Cao, Wangmeng Zuo, Wei Liu, and\\nMing-Hsuan Yang. Deep non-blind deconvolution via gen-\\neralized low-rank approximation. Advances in neural in-\\nformation processing systems, 31, 2018.\\n[Rim et al., 2020] Jaesung Rim, Haeyun Lee, Jucheol Won,\\nand Sunghyun Cho. Real-world blur dataset for learning\\nand benchmarking deblurring algorithms. In Computer\\nVision–ECCV 2020: 16th European Conference, Glas-\\ngow, UK, August 23–28, 2020, Proceedings, Part XXV 16,\\npages 184–201. Springer, 2020.\\n[Shan et al., 2008] Qi Shan, Jiaya Jia, and Aseem Agarwala.\\nHigh-quality motion deblurring from a single image. Acm\\ntransactions on graphics (tog), 27(3):1–10, 2008.\\n[Shen et al., 2019] Ziyi Shen, Wenguan Wang, Xiankai Lu,\\nJianbing Shen, Haibin Ling, Tingfa Xu, and Ling Shao.\\nHuman-aware motion deblurring. In Proceedings of the\\nIEEE/CVF International Conference on Computer Vision,\\npages 5572–5581, 2019.\\n[Shi et al., 2016] Baoguang Shi, Xiang Bai, and Cong Yao.\\nAn end-to-end trainable neural network for image-based\\nsequence recognition and its application to scene text\\nrecognition. IEEE transactions on pattern analysis and\\nmachine intelligence, 39(11):2298–2304, 2016.\\n[Simonyan and Zisserman, 2014] Karen Simonyan and An-\\ndrew Zisserman. Very deep convolutional networks\\nfor large-scale image recognition. arXiv preprint\\narXiv:1409.1556, 2014.\\n[Sun et al., 2013] Libin Sun, Sunghyun Cho, Jue Wang, and\\nJames Hays. Edge-based blur kernel estimation using\\npatch priors. In IEEE international conference on com-\\nputational photography (ICCP), pages 1–8. IEEE, 2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='recognition. IEEE transactions on pattern analysis and\\nmachine intelligence, 39(11):2298–2304, 2016.\\n[Simonyan and Zisserman, 2014] Karen Simonyan and An-\\ndrew Zisserman. Very deep convolutional networks\\nfor large-scale image recognition. arXiv preprint\\narXiv:1409.1556, 2014.\\n[Sun et al., 2013] Libin Sun, Sunghyun Cho, Jue Wang, and\\nJames Hays. Edge-based blur kernel estimation using\\npatch priors. In IEEE international conference on com-\\nputational photography (ICCP), pages 1–8. IEEE, 2013.\\n[Sun et al., 2015] Jian Sun, Wenfei Cao, Zongben Xu, and\\nJean Ponce. Learning a convolutional neural network for\\nnon-uniform motion blur removal. In Proceedings of the\\nIEEE conference on computer vision and pattern recogni-\\ntion, pages 769–777, 2015.\\n[Tao et al., 2018] Xin Tao, Hongyun Gao, Xiaoyong Shen,\\nJue Wang, and Jiaya Jia. Scale-recurrent network for deep\\nimage deblurring. In Proceedings of the IEEE conference\\non computer vision and pattern recognition , pages 8174–\\n8182, 2018.\\n[Wang et al., 2018] Xintao Wang, Ke Yu, Chao Dong, and\\nChen Change Loy. Recovering realistic texture in image\\nsuper-resolution by deep spatial feature transform. In Pro-\\nceedings of the IEEE conference on computer vision and\\npattern recognition, pages 606–615, 2018.\\n[Whyte et al., 2012] Oliver Whyte, Josef Sivic, Andrew Zis-\\nserman, and Jean Ponce. Non-uniform deblurring for\\nshaken images. International journal of computer vision ,\\n98:168–186, 2012.\\n[Xu et al., 2018] Zhenbo Xu, Wei Yang, Ajin Meng, Nanxue\\nLu, Huan Huang, Changchun Ying, and Liusheng Huang.\\nTowards end-to-end license plate detection and recogni-\\ntion: A large dataset and baseline. In Proceedings of the\\nEuropean conference on computer vision (ECCV) , pages\\n255–271, 2018.\\n[Zhang et al., 2023] Xiang Zhang, Lei Yu, Wen Yang,\\nJianzhuang Liu, and Gui-Song Xia. Generalizing event-\\nbased motion deblurring in real-world scenarios. In Pro-\\nceedings of the IEEE/CVF International Conference on\\nComputer Vision, pages 10734–10744, 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-23T00:45:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-23T00:45:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'templateversion': 'IJCAI.2024.0', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9', 'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf', 'file_type': 'pdf'}, page_content='Lu, Huan Huang, Changchun Ying, and Liusheng Huang.\\nTowards end-to-end license plate detection and recogni-\\ntion: A large dataset and baseline. In Proceedings of the\\nEuropean conference on computer vision (ECCV) , pages\\n255–271, 2018.\\n[Zhang et al., 2023] Xiang Zhang, Lei Yu, Wen Yang,\\nJianzhuang Liu, and Gui-Song Xia. Generalizing event-\\nbased motion deblurring in real-world scenarios. In Pro-\\nceedings of the IEEE/CVF International Conference on\\nComputer Vision, pages 10734–10744, 2023.\\n[Zhao et al., 2022] Suiyi Zhao, Zhao Zhang, Richang Hong,\\nMingliang Xu, Yi Yang, and Meng Wang. Fcl-gan: A\\nlightweight and real-time baseline for unsupervised blind\\nimage deblurring. In Proceedings of the 30th ACM In-\\nternational Conference on Multimedia, pages 6220–6229,\\n2022.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 0, 'page_label': '26667', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='Received 13 February 2023, accepted 2 March 2023, date of publication 10 March 2023, date of current version 21 March 2023.\\nDigital Object Identifier 10.1 109/ACCESS.2023.3255641\\nAdaptive Lightweight License Plate Image\\nRecovery Using Deep Learning Based on\\nGenerative Adversarial Network\\nWUTTINAN SEREETHAVEKUL\\n AND MONGKOL EKPANYAPONG\\nDepartment of Industrial Systems Engineering, School of Engineering and Technology, Asian Institute of Technology, Khlong Luang, Pathum Thani 12120,\\nThailand\\nCorresponding author: Wuttinan Sereethavekul (st118978@ait.asia)\\nThis work was supported in part by the Thailand Science Research and Innovation (TSRI) under Grant RDG6250036.\\nABSTRACT Many Convolutional Neural Networks (CNNs) methods have already surpassed traditional\\napproaches to image restoration tasks. Those CNNs models were usually designed to enhance single tasks\\nsuch as an image resolution (super-resolution) or image denoising, but we came up with unconventional\\ngoals, that is, multiple recovery tasks from a single network design. Although the Transformer design has\\nrecently gained attention in image recovery tasks, they are too slow. In order to work with license plate\\nimages from a traffic camera stream, the system has to be responsive. So, we proposed a fast and lightweight\\ndeep learning-based data recovery system using a Generative Adversarial Network (GAN) principle named\\nLicense Plate Recovery GAN (LPRGAN). The design has a proposed encoder-decoder style inspired by an\\nautoencoder aided by dual classification networks. This style suits problem-characteristic learning because\\nstrong contextual information is retrieved from the down-scaled representations. This proposed system has\\nthree main features such as identifying a problem, data recovery, and fail-safe mechanism. The core of\\nsystem is a data recovery unit (LPRGAN), is used to recover license plate images from multiple degraded'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 0, 'page_label': '26667', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='autoencoder aided by dual classification networks. This style suits problem-characteristic learning because\\nstrong contextual information is retrieved from the down-scaled representations. This proposed system has\\nthree main features such as identifying a problem, data recovery, and fail-safe mechanism. The core of\\nsystem is a data recovery unit (LPRGAN), is used to recover license plate images from multiple degraded\\ninput images. Most existing image restoration systems do not have self-awareness, leading to an inefficiency\\nproblem. Unlike existing works, this system has anomaly detection and will only process on a degraded input,\\nreducing workload overhead, improving efficiency and a fail-safe feature that prevents an unexpected bad\\noutput. Hence, it is light enough to deploy on a low-power machine such as edge computing devices, opening\\nup new possibilities in on-device computing. Our proposed research can recover several degraded problems\\nup to 720p resolution at 15 frames per second on a single graphic card, 256 × 128 resolution at 17 frames\\nper second on a CPU-only workstation machine, or 7 frames per second on an ultra-low-power tablet PC.\\nINDEX TERMS Data recovery, deep learning, generative adversarial networks, image and video recovery,\\nmachine learning, neural networks, video streaming.\\nI. INTRODUCTION\\nTraffic cameras are now becoming essential tools in part\\nof transportation systems. They are used to monitor traffic\\nactivity and accidents or to detect illegal vehicles on the road.\\nThese cameras help in traffic police workforce reduction.\\nNot only that, a traffic camera can be deployed in very\\nremote areas where traffic police are hard to reach. The\\ntraffic monitoring system can provide a full country-wide\\nThe associate editor coordinating the review of this manuscript and\\napproving it for publication was Yi Zhang\\n.\\nroad area coverage. This traffic monitoring is a worldwide\\nstandard practice to enhance road security and safety. The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 0, 'page_label': '26667', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='These cameras help in traffic police workforce reduction.\\nNot only that, a traffic camera can be deployed in very\\nremote areas where traffic police are hard to reach. The\\ntraffic monitoring system can provide a full country-wide\\nThe associate editor coordinating the review of this manuscript and\\napproving it for publication was Yi Zhang\\n.\\nroad area coverage. This traffic monitoring is a worldwide\\nstandard practice to enhance road security and safety. The\\nmost important aspect of traffic monitoring is vehicle license\\nplate reading, such as in road accidents or illegal road vehicles\\nso that police officers can identify them. However, there are\\nmany shortcomings in reading a license plate. Examples are\\noccasionally corrupted data within a streaming frame, low\\nlight area, slow shutter camera speed that is not fast enough\\nto track a plate, or an intention to save disk space by reducing\\nrecording bitrate, resulting in low-quality media files. A sim-\\nple form of data corruption can be seen as a blocky-looking\\nVOLUME 11, 2023\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ 26667'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 1, 'page_label': '26668', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nor blocky artifact due to low bitrate streaming. Fig. 1 shows\\na bitrate requirement in a video streaming at different resolu-\\ntions.\\nTo overcome those mentioned problems, this research has\\nstudied a deep learning principle which is part of artifi-\\ncial intelligence (AI), to address such problems. It has a\\nunique ability to learn and observe input patterns used for\\nmany complex tasks. This technique is suitable for improv-\\ning low-quality images by learning from high-quality ones.\\nThere are many CNNs available that have their benefits. For\\nexample, an autoencoder [1] is useful when an input has a\\nnoise in the image or audio. It is used to remove noise in\\na signal. An autoencoder also has a convolutional version\\ncalled a convolutional autoencoder. It has multiple convolu-\\ntion computation layers as part of a network. The U-Net [2] is\\na network that shares similarities to an autoencoder but even\\nmore complex layers. It consists of two parts, a contracting\\npath, and an expansive path. There are many GAN [3] varia-\\ntions such as Deep Convolutional GAN (DCGAN) [4], Con-\\nditional GAN (CGAN) [5], and CycleGAN [6]. The DCGAN\\nis usually used to generate a new image from random input\\ndata (normal distribution). A downside of DCGAN is that its\\noutput cannot be controlled, so it is impossible to specify\\nan output appearance or class. This reason is why CGAN\\noffers more control over DCGAN. The CycleGAN is used to\\nswap between two input domains. A GAN relies heavily on\\nCNNs because many CNNs models have posed a potential for\\nimage recovery and enhancement. They learn from a pattern\\nfrom big datasets. Most CNNs setups usually have either\\nan encoder-decoder style or a single-size style. In the first\\ncase, it utilizes a down-sampling method to map an input to\\na lower-resolution representation and then applies a reverse\\noperation (up-sampling) to map to an original resolution. This'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 1, 'page_label': '26668', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='swap between two input domains. A GAN relies heavily on\\nCNNs because many CNNs models have posed a potential for\\nimage recovery and enhancement. They learn from a pattern\\nfrom big datasets. Most CNNs setups usually have either\\nan encoder-decoder style or a single-size style. In the first\\ncase, it utilizes a down-sampling method to map an input to\\na lower-resolution representation and then applies a reverse\\noperation (up-sampling) to map to an original resolution. This\\noperation is a good way to learn input context by down-\\nsampling resolution, but a downside is that the fine spatial\\ndetails are lost in the process. Thus, an output usually has\\nlower details when compared to an original, making this\\nstyle a lossy process. In the latter case, a single-scaling style\\nutilizes feature processing. It does not contain any down-\\nsampling operation, producing images with more fine details.\\nNevertheless, this single-scale style is commonly less effec-\\ntive in learning a pattern of contextual information due to a\\nlimited representative resolution. So these two examples both\\nhave their benefits and drawbacks.\\nIt is a position-sensitive procedure where pixels from two\\nsources need to be matched in a recovery learning process.\\nThe first source is a reference image, and the second is a\\ndistorted image. A slight shift in pixel position between them\\nis undesired because a distorted pattern must be the only\\ncomponent in that image. Otherwise, a true pattern will be\\nmixed up with a dislocation pixel, making it difficult to learn\\na degradation pattern for a specific problem. Therefore, both\\nreference and degraded input images have to be perfectly\\naligned. Then the learning can even further benefit from a\\nlarge context dataset, i.e., image scaling or problem variation.\\nOur goal is to build a GAN model that is light and fast,\\nFIGURE 1. Bitrate required for video streaming at different resolution.\\ncontaining fewer possible layers and complexions. It aims to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 1, 'page_label': '26668', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='mixed up with a dislocation pixel, making it difficult to learn\\na degradation pattern for a specific problem. Therefore, both\\nreference and degraded input images have to be perfectly\\naligned. Then the learning can even further benefit from a\\nlarge context dataset, i.e., image scaling or problem variation.\\nOur goal is to build a GAN model that is light and fast,\\nFIGURE 1. Bitrate required for video streaming at different resolution.\\ncontaining fewer possible layers and complexions. It aims to\\nhelp a license plate reading in several degraded situations.\\nThe main difference from other works is that many cur-\\nrent works including recently released, Transformer design,\\nusually have a big nonsingular network consisting of several\\nconvolutional and other neural network layers, making them\\nslow and heavy on machine resources. They are unable to\\nbe deployed on edge computing machines since it consumes\\ntoo much processing power and memory. The other dif-\\nferences are some GAN-based models usually cover either\\nimage super-resolution or denoising study areas which are\\nnot the main problem of license plate reading. They are also\\nnot flexible, meaning one network design for one particular\\ntask. Thus, we proposed a GAN-based work that is able to\\nsolve several real world situations by using only one network\\nsetup. It features a GAN, contains a modified generator and\\ndiscriminator, resulting in just under a million parameters\\n(for 256 × 128 image recovery). The image recognition and\\nimage recovery systems work together to detect and recover\\na degraded input, greatly improve system efficiency, instead\\nof a simple barebone system.\\nThis study helps to contribute by developing a new sim-\\nple, light, and fast yet practical license plate image quality\\nrecovery, covering low bitrate, low light, and motion blur\\nsituations with a single network setup. It has a faster leap than\\nother approaches, so it is suitable for real-time stream pro-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 1, 'page_label': '26668', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='image recovery systems work together to detect and recover\\na degraded input, greatly improve system efficiency, instead\\nof a simple barebone system.\\nThis study helps to contribute by developing a new sim-\\nple, light, and fast yet practical license plate image quality\\nrecovery, covering low bitrate, low light, and motion blur\\nsituations with a single network setup. It has a faster leap than\\nother approaches, so it is suitable for real-time stream pro-\\ncessing. Thus, it can work on a typical workstation machine to\\nenhance monitoring potential since the system is simple and\\nlightweight. This study also includes comprehensive experi-\\nments on both simulated and real-world license plate image\\nbenchmarks - mathematical and visual inspection, including\\nlocal and international plates.\\nII. RELATED WORKS\\nImage processing has been developed over the past years,\\nand one that benefits are traffic monitoring. Traffic moni-\\ntoring involves the content transmission from a camera unit\\nto a monitoring unit. An interruption in transmission, such\\nas unavailable bandwidth, can produce poor-quality video\\ntransmission. Many have tried to improve content trans-\\nmission, such as Petrov et al. [7] proposed a technique to\\nreduce a blocking artifact by detecting a blocking artifact in\\na macroblock (8 × 8 pixels) and a displaced blocking effect.\\nThen apply a blocking artifact reduction using their proposed\\n26668 VOLUME 11, 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 2, 'page_label': '26669', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nfilters. They targeted mobile platforms with low bitrate video\\nfocused on operation speed. Dar et al. [8] presented a work\\nthat analyses only one slice of low bitrate video compression.\\nThis paper benefits from applying a spatio-temporal down-\\nscaling, i.e., reduction of frame rate and frame size, before the\\ncompression and a corresponding up-scaling afterward. They\\nleft H.264 codec untouched. Their work covers 16 ×16 mac-\\nroblock from very low (2 bits/slice) through low (around\\n30 bits/slice) and up to high (210 bits/slice) bitrate. So they\\npresented that the downsampling video before compression\\ntook place is better. Li et al. [9] proposed a new five layers\\nCNN-based block up-sampling scheme for intra-frame cod-\\ning. A block can be down-sampled before being compressed\\nby normal intra-coding and then up-sampled to its original\\nresolution. This way differs from the previous hand-craft\\ndown and upsampling because this paper is based on train-\\ning a CNN. A new CNN structure for upsampling features\\nthe deconvolution of feature maps, multi-scale fusion, and\\nresidue learning, making the network compact and efficient.\\nThey also designed different networks for the up-sampling\\nof luma and chroma components, respectively, where the\\nchroma up-sampling CNN utilizes the luma information to\\nboost its performance. This scheme is built into HEVC refer-\\nence software. Resulting in an average 5.5% BD-rate reduc-\\ntion on common test sequences and an average 9.0% BD-\\nrate reduction on ultra-high definition (UHD) test sequences.\\nInstead of using traditional downsampling, they presented\\na CNN-based sampling scheme. Lin et al. [10] proposed\\nan adaptive downsampling-based coding model to improve\\nthe low bitrate compression efficiency of high-efficiency\\nvideo coding (HEVC). They use motion estimation to find\\nthe most similar blocks between upscaled Non-Key Frames'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 2, 'page_label': '26669', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='tion on common test sequences and an average 9.0% BD-\\nrate reduction on ultra-high definition (UHD) test sequences.\\nInstead of using traditional downsampling, they presented\\na CNN-based sampling scheme. Lin et al. [10] proposed\\nan adaptive downsampling-based coding model to improve\\nthe low bitrate compression efficiency of high-efficiency\\nvideo coding (HEVC). They use motion estimation to find\\nthe most similar blocks between upscaled Non-Key Frames\\n(NKFs) and associated high-resolution Key Frames (KFs).\\nThen, an adaptive patching-based method is used to warp\\nthe low-quality NKF blocks with the high-quality KF blocks.\\nTheir experimental results demonstrate significant improve-\\nments compared to existing methods but only work on HEVC.\\nYang et al. [11] worked on enhancing low bitrate HEVC video\\nquality. They enhanced the visual quality of HEVC videos\\non the decoder side. So they proposed a Quality Enhance-\\nment Convolutional Neural Network (QE-CNN) method that\\ndoes not require any encoder modification to achieve qual-\\nity enhancement for HEVC. In particular, their QE-CNN\\nmethod learns QE-CNN-I and QE-CNN-P models to reduce\\nthe distortion of HEVC I and P/B frames, respectively. This\\nmethod differs from the existing CNN-based quality enhance-\\nment approaches, which only handle intra-coding distor-\\ntion and are thus unsuitable for P/B frames. They claimed\\ntheir method validates that the QE-CNN method effectively\\nenhances quality for both I and P/B frames of HEVC videos.\\nThese mentioned works feature both non-machine learning\\nand machine learning forms. Despite the benefit of media\\ntransmissions that enhance streaming quality, resulting in a\\nbetter video output quality, they are restricted to a specific\\nvideo codec.\\nOn the other hand, some previous works tried to improve\\nthe image processing aspect. For instance, Zhu et al. [12]\\nmade use of the CycleGAN to do an image-to-image transla-\\ntion (X − →Y ) which they called the ‘‘pix2pix’’ project [13].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 2, 'page_label': '26669', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='These mentioned works feature both non-machine learning\\nand machine learning forms. Despite the benefit of media\\ntransmissions that enhance streaming quality, resulting in a\\nbetter video output quality, they are restricted to a specific\\nvideo codec.\\nOn the other hand, some previous works tried to improve\\nthe image processing aspect. For instance, Zhu et al. [12]\\nmade use of the CycleGAN to do an image-to-image transla-\\ntion (X − →Y ) which they called the ‘‘pix2pix’’ project [13].\\nHe and his team created this project to swap the texture\\nof two things like, zebra and horse, and swap between two\\npictures style, such as a photograph and an art style. Although\\nswapping two pictures could benefit some areas, such as\\nswapping a noisy image with a clean one in the de-nosing\\ntask, it does not necessarily mean imagery improvement. Guo\\net al. [14] presented a way to improve image denoising with\\nadditive white Gaussian noise (AWGN) by training a con-\\nvolutional blind denoising network (CBDNet) with a more\\nrealistic noise model and real-world noisy-clean image pairs.\\nAlso, they provided an interactive strategy to rectify denois-\\ning results conveniently. A noise estimation subnetwork with\\nasymmetric learning to suppress the underestimation of noise\\nlevel is embedded into CBDNet. Wang et al. [15] built a blind\\nface restoration system. This Generative Facial Prior (GFP)\\nis incorporated into the face restoration process via spatial\\nfeature transform layers, achieving a good balance of real-\\nness and fidelity. The GFP-GAN could jointly restore facial\\ndetails and enhance colors with just a single forward pass.\\nKupyn et al. [16] presented DeblurGAN-v2, a newer ver-\\nsion of DeblurGAN that considerably boosts state-of-the-art\\ndeblurring performance while being much more flexible and\\nefficient. It was claimed to be faster and better than v1. It is\\nmade of GAN with a backend such as Inception ResNet v2.\\nZamir et al. [17] presented the MIRNet, an image restoration'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 2, 'page_label': '26669', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='ness and fidelity. The GFP-GAN could jointly restore facial\\ndetails and enhance colors with just a single forward pass.\\nKupyn et al. [16] presented DeblurGAN-v2, a newer ver-\\nsion of DeblurGAN that considerably boosts state-of-the-art\\ndeblurring performance while being much more flexible and\\nefficient. It was claimed to be faster and better than v1. It is\\nmade of GAN with a backend such as Inception ResNet v2.\\nZamir et al. [17] presented the MIRNet, an image restoration\\nmodel. A proposed architecture maintains high-resolution\\nrepresentations throughout the entire network and receives\\ninformation from the low-resolution representations. Existing\\nCNN-based methods usually operate just on full-resolution\\nor low-resolution representations. Although this network\\npacks much functionality, it also contains several modules.\\nThey used three Recursive Residual Groups (RRGs), each\\nof which contains two Multi-scale Residual Block (MRBs),\\nand each MRB also contains three streams. Zamir et al. [18]\\nalso proposed an efficient Transformer model for capturing\\nlong-range pixel interactions, while remaining applicable to\\nlarge images. It can restore images on several tasks. Liang\\net al. [19] proposed a SwinIR model for image restora-\\ntion based on the Swin Transformer. It consists of shallow\\nfeature extraction, deep feature extraction, and high-quality\\nimage reconstruction. Recently, a biomedical paper utilizing\\nGAN from Zhang et al. [20] presented a method of increas-\\ning contrast in CT scanning images for clinical diagnosis.\\nTheir MALAR system is based on CycleGAN. It has dual\\nGANs that work on ultra-low-dose-ICM aorta CT (UDCT)\\nand low-dose-ICM aorta CT (LDCT) images. However, this\\napproach outputs DICOM format and does not work on\\nstandard RGB images. Wu et al. [21] presented an article\\nfor tomographic image reconstruction in a sparse-view CT\\nscan. They proposed a Dual-domain Residual-based Opti-\\nmization NEtwork (DRONE). It consists of three modules\\nVOLUME 11, 2023 26669'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 3, 'page_label': '26670', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nfor embedding, refinement, and awareness. The results from\\nthe embedding and refinement modules in the data and\\nimage domains are regularized for optimized image qual-\\nity in the awareness module, which ensures the consistency\\nbetween measurements and images with the kernel aware-\\nness of compressed sensing. Wu et al. [22] also presented\\na Deep Embedding-Attention-Refinement (DEAR) network\\nto achieve good images from high sparse-view levels in CT\\nreconstruction tomography imaging. This study was based\\non the DRONE and released later. DEAR also consists of\\nthree modules including deep embedding, deep attention, and\\ndeep refinement. The results demonstrate the efficiency of\\nthe DEAR in edge preservation and feature recovery in deep\\ntomographic reconstruction.\\nAlthough this proposed approach is also in an image pro-\\ncessing area, unlike those existing works, because it does\\nnot need a codec modification and is not limited to one\\nspecific media, i.e., image/video, or one specific codec, i.e.,\\nJPG/PNG/A VC/HEVC. It also does not tie to one specific\\ntask, the proposed system is generalized for several tasks by\\nswapping a training dataset. Finally, it utilizes a lightweight\\nGAN design targeting low-power devices for a fast license\\nplate image recovery operation. Those designs are too com-\\nplex and require a high-performance system to run.\\nIII. METHODOLOGY\\nA. THE DESCRIPTION OF THE PROBLEMS\\nAs mentioned earlier, there are many common problems in\\ntraffic camera streams. Most seen problems were categorized\\ninto each group. A grouping is vital because the detection\\nsystem can provide a correspond description of an input to\\nthe recovery system precisely. Each group has its own label\\nand was used to train a detection system. Below is a list of\\nproblems that this study focuses on.\\n• Low Bitrate Dataset - Represents network congestion\\nand low bandwidth network problems'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 3, 'page_label': '26670', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='As mentioned earlier, there are many common problems in\\ntraffic camera streams. Most seen problems were categorized\\ninto each group. A grouping is vital because the detection\\nsystem can provide a correspond description of an input to\\nthe recovery system precisely. Each group has its own label\\nand was used to train a detection system. Below is a list of\\nproblems that this study focuses on.\\n• Low Bitrate Dataset - Represents network congestion\\nand low bandwidth network problems\\n• Low Light Dataset - Represents low light and nighttime\\nsituations\\n• Motion Blur - Horizontal Dataset - Represents slow\\ncamera shutter speed and speedy object problems\\n• Motion Blur - Vertical Dataset - Represents slow cam-\\nera shutter speed and camera shaking due to vibration\\nproblems\\n• Normal Dataset (Normal/Good Condition) - Represents\\na high-quality, daylight situation in an ideal case\\nIn the case of low bitrate problems where it is feasible to\\narrange them into sub-groups, these ranges refer to the JPG\\ncompression ratio range, which is mapped to a rating score\\nsystem. This rating system will be useful in cases where a\\nregular mathematical picture assessment is not possible to\\ncalculate.\\n• 1-Star: 0-20 JPG Quality Setting (Poorest Looking)\\n• 2-Star: 20-40 JPG Quality Setting\\n• 3-Star: 40-60 JPG Quality Setting\\n• 4-Star: 60-80 JPG Quality Setting\\n• 5-Star: 80-100 JPG Quality Setting (Best Looking)\\nThe above descriptions are used in a deep learning classifica-\\ntion, which is supervised training [23]. It is trained to detect\\nand categorize an occurring problem and acknowledges a\\ndifference between each compression ratio range to assess an\\noutput product.\\nB. DATASET PREPARATION AND PROCESSING\\nThai license plate images are the dataset used in this research\\nand were provided by the AI Center, Asian Institute of Tech-\\nnology. There are 16,194 images in total, and they were sep-\\narated into 14,500 train images and 1,694 evaluation images.\\nHowever, due to ownership and privacy infringement, these'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 3, 'page_label': '26670', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='and categorize an occurring problem and acknowledges a\\ndifference between each compression ratio range to assess an\\noutput product.\\nB. DATASET PREPARATION AND PROCESSING\\nThai license plate images are the dataset used in this research\\nand were provided by the AI Center, Asian Institute of Tech-\\nnology. There are 16,194 images in total, and they were sep-\\narated into 14,500 train images and 1,694 evaluation images.\\nHowever, due to ownership and privacy infringement, these\\nimages cannot be disclosed here. Due to the raw datasets\\nbeing relatively small in resolution, all images after resizing\\nwere capped at 256 × 128 pixels, so a network is primarly\\ndesigned to fit this image size. They were then processed into\\neach category using a random filter to replicate real-world\\nproblems such as low JPG quality level for low bitrate prob-\\nlems or low brightness for a low light problem. These were\\nprepared as the first step before training a model, and their\\ncategory structure is shown below. In Fig. 2a-Fig. 2f, display\\nhistogram plots on each equal probability random distribution\\nof filter level.\\n1) LOW BITRATE DATASET\\nUsing OpenCV2 to rewrite JPG images by setting a quality\\nvalue between 0 to 20 to match a 1-Star rating system range.\\n2) LOW LIGHT DATASET\\nUsing PIL ImageEnhance to drop an original image bright-\\nness from 100% to between 10% to 50% of the original value.\\n3) HORIZONTAL MOTION BLUR DATASET\\nA custom 2D kernel to create a motion blur filter [24] is\\nused with an image to create a motion blur image. Kernel\\nfilter size varies between 10 × 10-40 × 40. A blur kernel\\nfilter size is a pixel-shifting distance. For example, using\\n15 blur kernel size gives a 15-pixel shifting distance from the\\norigin. This method creates a motion blur along the X-axis\\n(0 degrees). A motion blur kernel filter has a formula as\\nin (1).\\nh = 1\\nm (1)\\n• h is the horizontal kernel value\\n• m is the size of the kernel\\nHm×m =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0 0 · · ·0\\n0 0 · · ·0\\n..\\n. .\\n.\\n. · · · .\\n.\\n.\\nhm/2 hm/2 · · ·hm/2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 3, 'page_label': '26670', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='filter size varies between 10 × 10-40 × 40. A blur kernel\\nfilter size is a pixel-shifting distance. For example, using\\n15 blur kernel size gives a 15-pixel shifting distance from the\\norigin. This method creates a motion blur along the X-axis\\n(0 degrees). A motion blur kernel filter has a formula as\\nin (1).\\nh = 1\\nm (1)\\n• h is the horizontal kernel value\\n• m is the size of the kernel\\nHm×m =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0 0 · · ·0\\n0 0 · · ·0\\n..\\n. .\\n.\\n. · · · .\\n.\\n.\\nhm/2 hm/2 · · ·hm/2\\n.\\n.\\n. .\\n.\\n. ... .\\n.\\n.\\n0 0 · · ·0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\n\\uf8fa\\uf8fb\\n26670 VOLUME 11, 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 4, 'page_label': '26671', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nFIGURE 2. Histogram plot on each problem type dataset - (a) Low Bitrate train set, (b) Low Bitrate test set, (c) Low light train set, (d) Low\\nlight test set, (e) Motion blur train set and (f) Motion blur test set.\\n4) VERTICAL MOTION BLUR DATASET\\nThis vertical motion blur is the same idea as a horizontal\\nmotion blur but is different in the filter kernel. Again, kernel\\nfilter size varies between 10 × 10 − 40 × 40. This method\\ncreates a motion blur along the Y-axis (90 degrees). A motion\\nblur kernel filter has a formula as in (2).\\nv = 1\\nm (2)\\n• v is a vertical kernel value\\n• m is the size of the kernel\\nVm×m =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n0 0 · · ·vm/2 · · ·0\\n0 0 · · ·vm/2 · · ·0\\n...\\n... · · ·\\n..\\n. ... .\\n.\\n.\\n0 0 · · ·vm/2 · · ·0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n5) NORMAL DATASET\\nA reference dataset, i.e., normal-looking, high-quality, and\\ngood-condition images. This dataset is used in recovery and\\nmeasurement processes.\\nC. PROPOSED SYSTEM, MODEL AND LAYERS\\nA proposed system is an end-to-end system combining\\ntwo image classifications and one image recovery into one\\napplication. It helps traffic monitoring system to detect the\\nanomaly and efficiently recover a bad one when needed.\\nA detection system is built from CNN image classifica-\\ntion and used to handle an incoming stream frame, detect a\\ndegraded frame and select a matching pre-trained model for\\na recovery system. This detection system can be found as a\\n‘‘Detector’’ in Fig. 3. It is based on VGG-16 [25] network but\\nwith a reduced layers count, resulting in three convolutional\\nlevels. The kernel size used in this network is 2. Its output\\n(Image Description) is used in ‘‘Model Selector’’. It currently\\nhas five description output classes plus five star-rating classes\\nin low-bitrate situations. This unit matches an input descrip-\\ntion with a predefined description found in the description\\nof the problems to select a proper recovery model for that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 4, 'page_label': '26671', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='‘‘Detector’’ in Fig. 3. It is based on VGG-16 [25] network but\\nwith a reduced layers count, resulting in three convolutional\\nlevels. The kernel size used in this network is 2. Its output\\n(Image Description) is used in ‘‘Model Selector’’. It currently\\nhas five description output classes plus five star-rating classes\\nin low-bitrate situations. This unit matches an input descrip-\\ntion with a predefined description found in the description\\nof the problems to select a proper recovery model for that\\ninput, i.e., low bitrate input needs trained low bitrate model.\\nA selected model will then be passed to a recovery system.\\nA recovery system (LPRGAN) is a pixel-based license\\nplate image recovery system. This proposed network features\\na reduction layer count configuration, including replacing\\nthe max pooling layer with a convolutional stride to speed\\nup model performance [26]. After receiving an input image\\nand its corresponding trained model from a model selector,\\nthe LPRGAN uses a trained model to recreate a high-quality\\nversion of the degraded input. Thus, this step is called the\\nrecovery process. The LPRGAN has two parts, a genera-\\ntor, and a discriminator. A generator is based on a convo-\\nlutional autoencoder but optimized with a less complicated\\nconfiguration. The main benefit of an autoencoder is that\\nit can down-sampling data while preserving a significant\\nrepresentation of original data. All max-pooling layers from\\nthe original version are replaced with striding. An upscale\\nprocess is also replaced with a convolutional transpose layer\\ninstead of the original upsampling layer. A generator layer\\nconfiguration can be found in Fig. 4. A discriminator is\\nconfigured with a light VGG-16 version that also features a\\nmax-pooling replacement. Both generator and discriminator\\nhave kernel size 3 in the main layer, except the last one (output\\nlayer) in the generator has kernel size equal to 1. They also\\nfeature a sigmoid activation function instead of a hyperbolic'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 4, 'page_label': '26671', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='process is also replaced with a convolutional transpose layer\\ninstead of the original upsampling layer. A generator layer\\nconfiguration can be found in Fig. 4. A discriminator is\\nconfigured with a light VGG-16 version that also features a\\nmax-pooling replacement. Both generator and discriminator\\nhave kernel size 3 in the main layer, except the last one (output\\nlayer) in the generator has kernel size equal to 1. They also\\nfeature a sigmoid activation function instead of a hyperbolic\\ntangent (tanh) for better output value coverage. These setups\\nmake a network more compact and responsive. A generated\\nproduct from the LPRGAN will later be fed to a qualifier for\\nevaluation purposes.\\nAfter a recovery step, a qualifier will validate a result with\\na rating score according to its problem type. This validation\\ncomes in handy when a degraded input is severely distorted\\nand cannot be recovered. Since most existing works do not\\nreport on this case where the input is too distorted beyond\\nGAN recovery capability, It is usually because the output will\\nresult in even worse quality. This occasion can sometimes\\nhappen when GAN could not reproduce the desired output\\nimage due to too much damage in an input image. Because\\nof this inadequacy, our proposed system has one final touch,\\na fail-safe mechanism using a ‘‘Qualifier’’. It will determine\\nwhich final result should be presented, either from a degraded\\ninput or an unrecovered result. This action is to prevent the\\nend user gets an unpleasant disaster output. A qualifier has\\nthe same CNN image classification setup but is trained with\\na different purpose, featuring three convolutional levels. The\\nkernel size used in this network is 2. It is used to evaluate and\\nvalidate a result at the end of the process.\\nD. DATA RECONSTRUCTION USING LPRGAN\\nThe basic idea in any image compression [27] is that the\\nmore the compression ratio is, the more space-saving, but it\\nresults in a blocky, bad-looking image. In the JPEG compres-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 4, 'page_label': '26671', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='the same CNN image classification setup but is trained with\\na different purpose, featuring three convolutional levels. The\\nkernel size used in this network is 2. It is used to evaluate and\\nvalidate a result at the end of the process.\\nD. DATA RECONSTRUCTION USING LPRGAN\\nThe basic idea in any image compression [27] is that the\\nmore the compression ratio is, the more space-saving, but it\\nresults in a blocky, bad-looking image. In the JPEG compres-\\nsion stage [28], once an image is translated from the spatial\\n2D domain into the frequency domain via DCT (Discrete\\nVOLUME 11, 2023 26671'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 5, 'page_label': '26672', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nFIGURE 3. Overview of proposed system block diagram.\\nCosine Transformation), a low to mid-frequency is usually\\ndiscarded to reduce file size, leaving out a high-frequency\\narea untouched. This high-frequency signal comes from a\\nsharp edge in the image. The reason to leave a sharp edge\\narea in the image is that human eyes are sensitive to them.\\nRemoving the rest would not affect on final image in terms\\nof visuals. This mentioned principle is also adopted for video\\ncompression with inter and intra-coding to save a bitrate.\\nOn the other hand, GAN has the unique ability to gener-\\nate fake data based on training in the deep learning world.\\nGAN is used to learn a data loss pattern from a compression\\nmechanism in this case. In the training process, pairs of\\ngood/ordinary-looking images (references) (x ) alongside real\\nlabels (y) and pairs of fake images (x ∗) with fake labels\\n(y∗) are used to train a discriminator (D) to learn on how\\nto differentiate a good and a bad. Then, prepared degraded\\ninput images (X ) with inverted real labels (y) are used in a\\ngenerator to generate a fake image but due to the fact that a\\ngenerator cannot be trained. So to train a GAN (G), the input\\n(X) is passed through a series of Convolutional 2D layers\\n(Conv2D), and their dimensions are halved in every Conv2D\\nlayer until they start to be upscaled back in Convolutional\\n26672 VOLUME 11, 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 6, 'page_label': '26673', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nFIGURE 4. LPRGAN generator layers visualization.\\n2D Transpose layers (Conv2DTranspose). Conv2D is used to\\nextract an input feature, called feature extraction, to represent\\nits distinctive. In contrast, Conv2DTranspose, an invert of\\nConv2D, is used to rebuild pixel information based on the\\nextracted feature. After a generator generates a new image,\\na discriminator will compare a generated image (x ∗) with an\\noriginal/reference image (x ) to decide whether a generated\\nimage looks realistic enough. This way, a discriminator helps\\nimprove a generator’s performance to generate an even better\\nresult as if it was trained. Ultimately, a generator target is to\\ngenerate a realistic image that a discriminator can no longer\\ntell them apart. This recovery system predicts and recovers\\ndata from compression or degraded loss to reduce a blocky\\nartifact, smooth out a color gradient, and brighten or reduce a\\nblurry image. Thus, this concept can help to repair degraded\\nproblems found in real life.\\nE. TRAINING PROCESS\\nA training result can be mediocre if there are insufficient\\nresources such as computation power, dataset, or time avail-\\nable. A sufficient amount of training datasets is crucial since\\nthey directly impact a training performance. The more variety\\nof dataset available, usually the more system performance is\\nlikely to be. Also, a reasonable time is needed for the model\\nto fit perfectly (balanced-fit). This research also features a\\ndecay learning rate [29] to accelerate a begining of training\\nprocess, as shown in (3).\\nLR = LR0 ∗ 1\\n(1 + LRDecay ∗ N) (3)\\n• LR is the current learning rate in that iteration\\n• LR0 is an initial constant learning rate value\\n• LRDecay is a decay value used to control how fast the\\nlearning rate decreases\\n• N is the current iteration number\\nThe training parameters except the β1 value are common to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 6, 'page_label': '26673', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='to fit perfectly (balanced-fit). This research also features a\\ndecay learning rate [29] to accelerate a begining of training\\nprocess, as shown in (3).\\nLR = LR0 ∗ 1\\n(1 + LRDecay ∗ N) (3)\\n• LR is the current learning rate in that iteration\\n• LR0 is an initial constant learning rate value\\n• LRDecay is a decay value used to control how fast the\\nlearning rate decreases\\n• N is the current iteration number\\nThe training parameters except the β1 value are common to\\nall experiments. The detail of values used in these studies is\\nbelow.\\n• Main Kernel Size = 3\\n• Initial Learning Rate = 0.0001\\n• Optimizer = Adam (Low Bitrate β1 = 0.5, Low Light\\nβ1 = 0.3, Horizontal Motion Blur β1 = 0.7, Vertical\\nMotion Blur β1 = 0.5)\\n• Iterations = 200\\nAlgorithm 1LPRGAN Training\\n1: procedure Train GAN\\n2: for iteration = 1, 2, 3, . . .do\\n3: for batch = 1, 2, 3, . . .do\\n4: Load random image samples x in a mini-batch\\nmanner\\n5: Train the discriminator on loaded samples\\nwith its real labels (x , y)\\n6: Compute the discriminator loss D(x) and\\nbackpropagation a total error θ(D) to minimize a loss\\n7: Using the generator to generate fake images\\nfrom input G(x′) = x∗\\n8: Train the discriminator on fake images and\\nfake labels sample (x ∗, y∗)\\n9: Compute the discriminator loss D(x∗) and\\nbackpropagation a total error θ(D) to minimize a loss\\n10: Train the GAN by using low-quality input\\nimages with real labels sample (x ′, y)\\n11: Compute and update GAN Loss θ(G) to max-\\nimize a loss\\n12: end for\\n13: LR ← LRiteration\\n14: Load different random image samples x0 in a\\nmini-batch\\n15: Evaluate the discriminator on real images set\\n(x0, y)\\n16: Generate fake images from input G(x′\\n0) = x∗\\n0\\n17: Evaluate the discriminator on fake images set\\nwith fake labels (x ∗\\n0 , y∗)\\n18: psnr ← PSNR(x, x∗) and psnr0 ← PSNR(x0, x∗\\n0\\n19: scc ← SCC(x, x∗) and scc0 ← SCC(x0, x∗\\n0 )\\n20: ssim ← SSIM(x, x∗) and ssim0 ← SSIM(x0, x∗\\n0 )\\n21: vif ← VIF(x, x∗) and vif0 ← VIF(x0, x∗\\n0 )\\n22: if psnr0 > saved_psnr0 ∨ scc0 > saved_scc0 ∨'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 6, 'page_label': '26673', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='14: Load different random image samples x0 in a\\nmini-batch\\n15: Evaluate the discriminator on real images set\\n(x0, y)\\n16: Generate fake images from input G(x′\\n0) = x∗\\n0\\n17: Evaluate the discriminator on fake images set\\nwith fake labels (x ∗\\n0 , y∗)\\n18: psnr ← PSNR(x, x∗) and psnr0 ← PSNR(x0, x∗\\n0\\n19: scc ← SCC(x, x∗) and scc0 ← SCC(x0, x∗\\n0 )\\n20: ssim ← SSIM(x, x∗) and ssim0 ← SSIM(x0, x∗\\n0 )\\n21: vif ← VIF(x, x∗) and vif0 ← VIF(x0, x∗\\n0 )\\n22: if psnr0 > saved_psnr0 ∨ scc0 > saved_scc0 ∨\\nssim0 > saved_ssim0 ∨ vif0 > saved_vif0 then\\nsaved_alue ← new_value\\n23: end if\\n24: end for\\n25: fid ← FID(x0, x∗\\n0 )\\n26: end procedure\\nF. METRIC MEASUREMENTS DEFINITION\\nIn order to measure the quality of any given image, there\\nare two major quality assessments. The first is a human\\nperception or human visual system (HVS), and the second is\\nmathematical measurements. Using a human is hard to get a\\nconsistent result since each person has a different perception,\\nand the human eye is difficult to tell a slight change in\\nbetween images. So a deep learning unit (image qualifier) is\\nused to assess in HVS instead. On the other hand, a mathemat-\\nical assessment in this study includes these measurements,\\nFID, PSNR, SCC, SSIM, and VIF. FID is explicitly designed\\nto assess a non-authentic, generated image. This FID score is\\noffered from Pytorch-fid [30] package. The rest of the metrics\\nVOLUME 11, 2023 26673'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 7, 'page_label': '26674', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nare offered by Sewar [31] package. Below list are all metrics\\nconducted in this research.\\n• FID (Fréchet Inception Distance) [32]\\n• PSNR (Peak Signal-to-Noise Ratio) [33]\\n• SCC (Spatial Correlation Coefficient) [34]\\n• SSIM (Structural Similarity Index) [35]\\n• VIF (Visual Information Fidelity) [36]\\n• File size\\n• Time Usage\\n• Render Speed\\n1) FID\\nThe Fréchet inception distance (FID) is a metric used to\\nassess the quality of images specifically created by the GAN.\\nIt compares the distribution of generated images with the\\ndistribution of real images used to train the generator. In other\\nwords, this score tells how well the GAN is from comparing\\ngenerated dataset with a training dataset. FID compares the\\nmean and standard deviation of one of the deeper layers in\\nthe Inception v3 network. These near-end layers are near\\noutput nodes that correspond to real-world objects. Thus,\\nit can mimic the human perception of similarity in images.\\nThe FID value will be 0 if paired datasets are identical, and the\\nvalue will go higher when there is more difference (deviation)\\nbetween two input datasets. The lower it is, the better. FID has\\na formula as in (4).\\nFID = ||µ1 − µ2||2 + tr(61 + 62 − 2(6\\n1\\n2\\n1 · 61 · 6\\n1\\n2\\n2 )\\n1\\n2 )\\n(4)\\n• mu_1 and mu_2 refer to the feature-wise mean of the\\nreal and generated images, i.e., 2,048 element vectors\\nwhere each element is the mean feature observed across\\nthe images.\\n• 61 and 62 are the covariance matrix for the real and\\ngenerated feature vectors\\n• ||µ1−µ2||2 refers to the sum squared difference between\\nthe two mean vectors. tr is the trace linear algebra\\noperation, i.e., the sum of the elements along the main\\ndiagonal of the square matrix.\\n2) PSNR\\nPSNR takes two inputs to calculate a signal power using the\\nMSE of the reference image from the original image. Its range\\nis usually in-between 25-48dB for an 8-bit image, where'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 7, 'page_label': '26674', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='the images.\\n• 61 and 62 are the covariance matrix for the real and\\ngenerated feature vectors\\n• ||µ1−µ2||2 refers to the sum squared difference between\\nthe two mean vectors. tr is the trace linear algebra\\noperation, i.e., the sum of the elements along the main\\ndiagonal of the square matrix.\\n2) PSNR\\nPSNR takes two inputs to calculate a signal power using the\\nMSE of the reference image from the original image. Its range\\nis usually in-between 25-48dB for an 8-bit image, where\\nhigher is better. PSNR has a formula as in (5).\\nPSNR = 10 ∗ log10( 2562\\nMSE ) (5)\\n• MSE is the mean squared error that measures the average\\nof the squares of the errors, the average squared differ-\\nence between estimated values and actual value\\n3) SCC\\nThe Spatial Correlation Coefficient calculates the spatial cor-\\nrelation coefficient score from paired images. SCC is defined\\nas a spatial concordance coefficient for second-order station-\\nary processes, and it detects a misalignment between two\\nimages. It has a value between 0 to 1. The generalised SCC\\nformula [37] is shown in (6).\\nρc(h) = 2σX σY\\nσ2\\nX + σ2\\nY\\nρXY R(h, φXY ) (6)\\n• |ρc(h)| ≤ |ρXY (h)| ≤1\\n• |ρc(h)| =0 if |ρXY (h)| =0\\n• σX is a standard deviation of X, and σY is a standard\\ndeviation of Y . Given that X and Y are two random\\nvariables.\\n• R(h, φXY ) is a correlation function with parameter vector\\nφ in which a covariance function is defined\\n4) SSIM\\nThis image quality assessment techniques rely on quantifying\\nerrors between reference and sample image and needs two\\nimages to do a calculation. SSIM can identify structural infor-\\nmation from a scene and the differences between the infor-\\nmation extracted from reference and sample scenes. It takes\\ntwo inputs, original and distorted. The outcome value varies\\nbetween 0 to 1, where higher is better. SSIM has a generalized\\nformula as in (7).\\nSSIM(i1, i2) = [l(i1, i2)]α · [c(i1, i2)]β · [s(i1, i2)]γ (7)\\n• i1 is the first image\\n• i2 is the second image\\n• l(i1, i2) is luminance comparison function'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 7, 'page_label': '26674', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='images to do a calculation. SSIM can identify structural infor-\\nmation from a scene and the differences between the infor-\\nmation extracted from reference and sample scenes. It takes\\ntwo inputs, original and distorted. The outcome value varies\\nbetween 0 to 1, where higher is better. SSIM has a generalized\\nformula as in (7).\\nSSIM(i1, i2) = [l(i1, i2)]α · [c(i1, i2)]β · [s(i1, i2)]γ (7)\\n• i1 is the first image\\n• i2 is the second image\\n• l(i1, i2) is luminance comparison function\\n• c(i1, i2) is contrast comparison function\\n• s(i1, i2) is structural comparison function\\n• α > 0, β > 0, γ > 0 denote the relative importance of\\neach of the metrics\\n5) VIF\\nVisual Information Fidelity calculates pixel-based visual\\ninformation fidelity. The VIF is computed for a collection\\nof wavelet coefficients that could represent either an entire\\nsubband of an image or a spatially localized set of subband\\ncoefficients. In the VIF system, the higher score is, the better,\\nwhere 1 is the best case. The scoring system is similar to\\nSSIM, using a value between 0 and 1. A general term of the\\nVIF formula is in (8).\\nVIF = 6j(subbands)I( ⃗CNj ; ⃗FNj |sNj )\\n6j(subbands)I( ⃗CNj ; ⃗ENj |sNj )\\n(8)\\n• I( ⃗CN ; ⃗FN |sN ) and I( ⃗CN ; ⃗EN |sN ) represent the informa-\\ntion that could ideally be extracted by the brain from\\na particular subband of the reference and test images,\\nrespectively\\n• ⃗CNj represents N elements of the RF that describe the\\ncoefficients from subband j, and so on\\n26674 VOLUME 11, 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 8, 'page_label': '26675', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\n6) FILE SIZE\\nUsually, any image file size will be larger when its resolution\\nand compression quality value is bigger. However, suppose\\nall images have the exact resolution and compression ratio.\\nIn that case, their file size can reflect how much that image\\nholds information (image detail). The bigger size, the more\\nimage contains fine detail. The file size in this study has a\\nunit as kilobytes (kB).\\n7) TIME USAGE\\nTime usage in this study is a training time usage per training\\nepoch. It indicates how fast a model setup is, and the faster\\napproach is always preferable. A unit for the time used in this\\nstudy is second.\\n8) RENDER SPEED\\nA render speed has a unit of frames per second (FPS). It indi-\\ncates how fast a model setup is. The faster approach is always\\npreferable. This metric is used in a testing process. This\\nFPS can also tell if a testing model is viable for real-time\\noperation since a CCTV camera typically operates from\\n15 to 30 FPS.\\n9) MEMORY USAGE\\nMemory usage can ultimately be a deciding factor if any\\naaproach could be deployed on edge computing devices since\\nmost of them have a very limited amount of memory. Usually,\\nthese embedded systems have around 32MB to 512MB in\\nmemory capacity. So a model should use as the least RAM as\\npossible.\\nG. HARDWARE\\nBelow is a custom build model training PC running Ubuntu\\n18.04. This PC was used throughout the entire research.\\n• AMD Ryzen 7 2700× 8 Cores 16 Threads CPU\\n• Asrock B450 Gaming K4 Motherboard\\n• Galax NVIDIA GeForce RTX 2080 Ti 11GB GDDR6\\n352-bit 260 Watts GPU\\n• Corsair 32GB DDR4 Memory\\n• WD Green SATA SSD 120GB\\n• Cooler Master 80PLUS Gold Full Modular 750W Power\\nSupply\\nThis is a workstation PC used in testing.\\n• Intel Xeon Processor E3-1200 v6 72 Watts CPU\\n• 8GB DDR4 Memory\\n• 2 × 3.5′′ Enterprise SATA 7.2k 1TB\\nThe Microsoft Surface Go 3 tablet PC is an ultra-low-power\\nPC used in testing.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 8, 'page_label': '26675', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='• AMD Ryzen 7 2700× 8 Cores 16 Threads CPU\\n• Asrock B450 Gaming K4 Motherboard\\n• Galax NVIDIA GeForce RTX 2080 Ti 11GB GDDR6\\n352-bit 260 Watts GPU\\n• Corsair 32GB DDR4 Memory\\n• WD Green SATA SSD 120GB\\n• Cooler Master 80PLUS Gold Full Modular 750W Power\\nSupply\\nThis is a workstation PC used in testing.\\n• Intel Xeon Processor E3-1200 v6 72 Watts CPU\\n• 8GB DDR4 Memory\\n• 2 × 3.5′′ Enterprise SATA 7.2k 1TB\\nThe Microsoft Surface Go 3 tablet PC is an ultra-low-power\\nPC used in testing.\\n• Intel Core i3 10100Y 5 Watts Ultra Low Power CPU\\n• 1866MHz 8GB DDR3 Memory\\n• 128GB SSD PCIe Storage\\nThis is a specification of a camera used in this study.\\n• Lilin ZR8022EX10 1080p 2MP CMOS Sensor IP\\nCamera\\nFIGURE 5. Test scene illustration.\\nFIGURE 6. Original/reference image.\\nH. TEST SCENE\\nThe below figure illustrates how a test scene was set up.\\nA camera model used in the testing is Lilin ZR8022EX10.\\nThe distance between a camera and the front bumper of a\\ncar where a license plate is located is 3±0.5 meters. A car\\nis stationary in front of the camera to be safe and avoid\\nan accident. There is no physical restriction on the distance\\nbetween a camera and a plate since all collected images are\\ncropped to fit a license plate area in post-processing.\\nIV. RESULT\\nThis section contains all related training and testing exper-\\niments details. The below image is an original/reference\\nimage1 found in Fig. 6. It will be used to compare against\\neach problem type generated images.\\nA. LAYER DEPTH CONFIGURATION\\nTable. 1 shows ablation studies between 11, 13, and 15 of the\\ngenerator’s layers configurations. A training time is a time\\nusage per epoch, unit in seconds. As shown below, the more\\nlayer counts the more computational time is needed making\\noverall performance drop. Also, at the current stage, using\\n11 layers count produces the best outcome for both metrics.\\nB. KERNEL SIZE OPTIMIZATION\\nA Table. 2 represents the combination between each layer and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 8, 'page_label': '26675', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='A. LAYER DEPTH CONFIGURATION\\nTable. 1 shows ablation studies between 11, 13, and 15 of the\\ngenerator’s layers configurations. A training time is a time\\nusage per epoch, unit in seconds. As shown below, the more\\nlayer counts the more computational time is needed making\\noverall performance drop. Also, at the current stage, using\\n11 layers count produces the best outcome for both metrics.\\nB. KERNEL SIZE OPTIMIZATION\\nA Table. 2 represents the combination between each layer and\\nthe last layer kernel size settings. This experiment studies the\\nresult difference between each set and finds the best kernel\\nsize setting for a current generator setup. Furthermore, a main\\nlayer kernel size setting is used in both a generator and a\\ndiscriminator.\\nC. SIGMOID VS TANH AS ACTIVATION FUNCTION\\nThe proposed models’ activation functions have been mod-\\nified to achieve the best result. It is slightly different\\nfrom selecting the Sigmoid to Tanh function. However,\\n1This license plate is the author’s ownership, and it does not violate other\\nprivacy or rights.\\nVOLUME 11, 2023 26675'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 9, 'page_label': '26676', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nTABLE 1. SSIM score on each layer depth configuration table.\\nTABLE 2. SSIM score on each kernel size table.\\nFIGURE 7. Result from Using Sigmoid VS Tanh Function - (a) Sigmoid\\n(SSIM 0.787), (b) Tanh (SSIM 0.784).\\nFIGURE 8. Training performance of learning Rate= 0.001.\\nFIGURE 9. Training performance of learning Rate= 0.0001.\\nan experiment result shows that the Sigmoid function per-\\nforms better in the SSIM score.\\nD. LEARNING RATE ADJUSTMENT\\nThis subsection demonstrates how different when selecting a\\nlearning rate. In Fig. 8, picking too big a learning rate value\\nwould make a training overshooting (overfitting), mean-\\ning that a loss has gone saturated and accuracy hit almost\\n100% (optimal accuracy in GAN training is 50%), includ-\\ning those measurement metrics started to decline, in very\\nearly point of the training. On the other hand, when pick-\\ning a proper learning rate value [Fig. 9], a training loss\\nis saturated and an accuracy value hangs in between the\\nmiddle, especially near the end of the training, accuracy\\nis stable at around 50% (balanced-fit) as well as mea-\\nsurement metrics that reach a very high peak in the later\\nresults.\\nFIGURE 10. Training performance ofβ1 = 0.1.\\nFIGURE 11. Training performance ofβ1 = 0.5.\\nFIGURE 12. Training performance ofβ1 = 0.9.\\nE. ADAM OPTIMIZER ADJUSTMENT\\nAn optimizer used in this work is an Adaptive Moment\\nEstimation (ADAM) optimizer [38]. A β1 is one of the hyper-\\nparameters and adjustable value. It is the initial decay rate\\nused when estimating the first moment of the gradient while\\ntraining, which is multiplied at the end of each training step\\nor batch. Decreasing β1 will slow down a training process and\\nincreasing a value will result in the opposite way. This value\\nneeds to be adjusted according to batch size. Normally, a large\\nbatch size will result in faster learning and a small batch will'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 9, 'page_label': '26676', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='Estimation (ADAM) optimizer [38]. A β1 is one of the hyper-\\nparameters and adjustable value. It is the initial decay rate\\nused when estimating the first moment of the gradient while\\ntraining, which is multiplied at the end of each training step\\nor batch. Decreasing β1 will slow down a training process and\\nincreasing a value will result in the opposite way. This value\\nneeds to be adjusted according to batch size. Normally, a large\\nbatch size will result in faster learning and a small batch will\\nresult in slower learning. Once using a very large or very low\\nbatch size could lead to non-optimal learning, adjusting this\\nβ1 value can help in these situations. In this showing case,\\nusing β1 = 0.5 (default is 0.9) is a middle ground that matches\\nthe other training values used in this learning process and\\nresults in the best balance point between training speed and\\nperformance. However, each training may also require tuning\\nand has its own optimal β1 value.\\nF. LOW BITRATE PROBLEM\\nThis subsection shows the LPRGAN testing results in low\\nbitrate conditions. Overall performance graphs are displayed\\nin Fig. 13-Fig. 14.\\nIn this case, a poor-quality version of the reference image,\\nis the input image. The input image can be found in\\n26676 VOLUME 11, 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 10, 'page_label': '26677', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nFIGURE 13. Overall training performance on low Bitrate problem.\\nFIGURE 14. Overall evaluating performance on low Bitrate problem.\\nFIGURE 15. Result on simulated Low Bitrate problem - (a) Input Image,\\n(b) CBDNet output image, (c) GFPGAN-SR output image, (d) Convolutional\\nAutoencoder output image, (e) GAN+U-Net output image, (f) SwinIR\\nOutput Image and (g) LPRGAN output image.\\nFig. 15a.2 Output results from each network are shown in\\nFig. 15b-Fig. 15g respectively. As a result, the CBDNet, the\\nGFPGAN-SR and the SwinIR do not work in this case, but the\\nLPRGAN can recover most of the lost data in input images,\\nespecially in low-detail areas like a grey plate background.\\nAt the same time, a convolutional autoencoder has a smooth\\noutput but fails to remove a blocky artifact, and the GAN+U-\\nNet has a not-so-sharp image.\\nHere is an example of using the LPRGAN on a low bitrate\\nvideo, an actual use case, instead of a JPG compression\\nimage. A video was recorded with a very low bitrate, 8kbps,\\nH264 format. It was also resized to a 256 × 128 frame size,\\nmatching a proposed network configuration. Unfortunately,\\nthere is no original/reference image to compare with the\\noutput in an actual situation, so only the input [Fig. 16a]\\nand outputs [Fig. 16b-Fig. 16g] results are available. When\\nzoomed in, all results [Fig. 17a-Fig. 17f] are visible, showing\\nthat the LPRGAN gives out the best result over the rest.\\nIt produces more detail and contrasts. Although the SwinIR\\ncould remove compression artifact, it also removes some\\n2Some PDF viewers (i.e., Preview in macOS) have an antialiasing feature\\nthat smooths out a rough rendered object in a document. In order to see raw\\nresult images, this feature must be turned off.\\nFIGURE 16. Result on Actual Low Bitrate Video Problem - (a) Input Image,\\n(b) CBDNet Output Image, (c) GFPGAN-SR Output Image,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 10, 'page_label': '26677', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='that the LPRGAN gives out the best result over the rest.\\nIt produces more detail and contrasts. Although the SwinIR\\ncould remove compression artifact, it also removes some\\n2Some PDF viewers (i.e., Preview in macOS) have an antialiasing feature\\nthat smooths out a rough rendered object in a document. In order to see raw\\nresult images, this feature must be turned off.\\nFIGURE 16. Result on Actual Low Bitrate Video Problem - (a) Input Image,\\n(b) CBDNet Output Image, (c) GFPGAN-SR Output Image,\\n(d) Convolutional Autoencoder Output Image, (e) GAN+U-Net Output\\nImage, (f) SwinIR Output Image and (g) LPRGAN Output Image.\\nFIGURE 17. Result on Actual Low Bitrate Video Problem with 3×Zoom on\\nLast 3 Digits - (a) CBDNet Output Image, (b) GFPGAN-SR Output Image,\\n(c) Convolutional Autoencoder Output Image, (d) GAN+U-Net Output\\nImage, (e) SwinIR Output Image and (f) LPRGAN Output Image.\\nFIGURE 18. Overall training performance on low light problem.\\nfine detail from the image, making it looks less sharp. The\\nGFPGAN-SR has an aspect ratio distortion due to the nature\\nof the super-resolution technique. The rest of the outputs are\\ndull and blurry.\\nG. LOW LIGHT PROBLEM\\nThis subsection demonstrates two types of testing images in\\na low-light situation. One simulates a low light by reduc-\\ning image brightness, and the other captures an actual low\\nlight nighttime. Overall performance graphs are displayed in\\nFig. 18-Fig. 19.\\nIn the simulation [Fig. 20a], both the LPRGAN and the\\nGAN+U-Net show improved brightened images but not the\\nrest. In addition, the GAN+U-Net output has some yel-\\nlow tint and is not as sharp whereas the MIRNet and the\\nLPRGAN produce the correct color temperature results.\\nFig. 20b-Fig. 20g show all outputs. In the real-world scene\\ntest [Fig. 21a], only the GAN+U-Net, the MIRNet, and the\\nLRPGAN outputs have improved brightness from the input\\nbut the GAN+U-Net also has a weird tint in the output, only\\nVOLUME 11, 2023 26677'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 11, 'page_label': '26678', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nFIGURE 19. Overall evaluating performance on low light problem.\\nFIGURE 20. Result on simulated low light problem - (a) Input image,\\n(b) CBDNet output image, (c) GFPGAN-SR output image, (d) Convolutional\\nAutoencoder output image, (e) GAN+U-Net output image, (f) MIRnet\\noutput image and (g) LPRGAN output image.\\nFIGURE 21. Result on actual low light problem - (a) Input Image,\\n(b) CBDNet output image, (c) GFPGAN-SR output image, (d) Convolutional\\nautoencoder output image, (e) GAN+U-Net output image, (f) MIRnet\\noutput image and (g) LPRGAN output image.\\nFIGURE 22. Overall training performance on horizontal motion blur\\nproblem.\\nFIGURE 23. Overall evaluating performance on horizontal motion blur\\nproblem.\\nthe MIRNet and the LPRGAN produce the most realistic and\\ncorrected color tone images [Fig. 21b-Fig. 21g]. Ultimately,\\nthe LPRGAN and the MIRNet are the first and second candi-\\ndates in low light recovery, but the CBDNet, the GFPGAN-\\nSR, and a convolutional autoencoder failed completely.\\nH. 1-AXIS MOTION BLUR PROBLEM\\nThese are examples of solving one-directional motion blur\\nproblems. Both horizontal (0 degrees) and vertical (90\\ndegrees) motion blur were studied in this research. Overall\\nperformance graphs are displayed in Fig. 22-Fig. 25.\\nFIGURE 24. Overall training performance on vertical motion blur problem.\\nFIGURE 25. Overall evaluating performance on vertical motion blur\\nproblem.\\nFIGURE 26. Result on simulated horizontal motion blur problem -\\n(a) Input image, (b) CBDNet output image, (c) GFPGAN-SR output image,\\n(d) Convolutional Autoencoder output image, (e) GAN+U-Net output\\nimage, (f) DeblurGANv2+MobileNet output image, (g) Restormer output\\nimage and (h) LPRGAN output image.\\nA blurred input image in the horizontal blur problem is\\nshown in Fig. 26a for a simulated test case. Simulation output\\nresults for horizontal blur demonstrated from each network'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 11, 'page_label': '26678', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='problem.\\nFIGURE 26. Result on simulated horizontal motion blur problem -\\n(a) Input image, (b) CBDNet output image, (c) GFPGAN-SR output image,\\n(d) Convolutional Autoencoder output image, (e) GAN+U-Net output\\nimage, (f) DeblurGANv2+MobileNet output image, (g) Restormer output\\nimage and (h) LPRGAN output image.\\nA blurred input image in the horizontal blur problem is\\nshown in Fig. 26a for a simulated test case. Simulation output\\nresults for horizontal blur demonstrated from each network\\nare shown in Fig. 26b-Fig. 26h. However, in order to get\\nactual motion blur images, a high-speed panning camera\\nin left-right directions creates a horizontal blur [Fig. 27a].\\nThis action is a much safer measurement than driving a car\\nspeeding toward a camera. The actual case output results for\\nhorizontal blur demonstrated from each network are shown\\nin Fig. 27b-Fig. 27h. Also, in a vertical motion blur problem,\\na simulated vertical blur image is in Fig. 28a, and these\\nFig. 28b-Fig. 28h are the output. Once again, in order to\\nget an actual vertical motion blur to quickly pan a camera\\nin up-down directions to create a vertical blur [Fig. 29a].\\nThe outputs are shown in Fig. 29b-Fig. 29h. In the end,\\nthe DeblurGANv2 and the Restormer can fix a motion blur\\nproblem only in a simulation case but not a real-world one,\\nand only the LPRGAN can recover blurred images in both\\ncases.\\n26678 VOLUME 11, 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 12, 'page_label': '26679', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nFIGURE 27. Result on actual horizontal motion blur problem - (a) Input\\nimage, (b) CBDNet output image, (c) GFPGAN-SR output image,\\n(d) Convolutional Autoencoder output image, (e) GAN+U-Net output\\nimage, (f) DeblurGANv2+MobileNet output image, (g) Restormer output\\nimage and (h) LPRGAN output image.\\nFIGURE 28. Result on simulated vertical motion blur problem - (a) Input\\nimage, (b) CBDNet output image, (c) GFPGAN-SR output image,\\n(d) Convolutional Autoencoder output image, (e) GAN+U-Net output\\nimage, (f) DeblurGANv2+MobileNet output image, (g) Restormer output\\nimage and (h) LPRGAN output image.\\nFIGURE 29. Result on actual vertical motion blur problem - (a) Input\\nimage, (b) CBDNet output image, (c) GFPGAN-SR output image,\\n(d) Convolutional Autoencoder output image, (e) GAN+U-Net output\\nimage, (f) DeblurGANv2+MobileNet output image, (g) Restormer output\\nimage and (h) LPRGAN output image.\\nFIGURE 30. Original US and UK License Plates - (a) US and (b) UK.\\nI. INTERNATIONAL COUNTRIES LICENSE PLATE TEST\\nAlthough the current LPRGAN model has been trained on the\\nThai license plate dataset without knowing other countries’\\nplate appearances, this model can still recover a poor-quality\\nplate at a reasonable level thanks to its generalization.\\nOf course, its performance would not be near a Thai license\\nplate recovery on which it was trained, but it can be solved\\nby retraining with a target country dataset. These figures\\n[Fig. 30a-Fig. 30b] show a US [39] and UK [40] license plate\\nsamples.\\nFIGURE 31. Result on US plate from LPRGAN (a) Low Bitrate input image,\\n(b) Low Bitrate output image, (c) Low light input image, (d) Low light\\noutput image, (e) Horizontal motion blur input image, (f) Horizontal\\nmotion blur output image, (g) Vertical motion blur input image and\\n(h) Vertical motion blur output image.\\nFIGURE 32. Result on UK plate from LPRGAN (a) Low Bitrate input image,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 12, 'page_label': '26679', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='[Fig. 30a-Fig. 30b] show a US [39] and UK [40] license plate\\nsamples.\\nFIGURE 31. Result on US plate from LPRGAN (a) Low Bitrate input image,\\n(b) Low Bitrate output image, (c) Low light input image, (d) Low light\\noutput image, (e) Horizontal motion blur input image, (f) Horizontal\\nmotion blur output image, (g) Vertical motion blur input image and\\n(h) Vertical motion blur output image.\\nFIGURE 32. Result on UK plate from LPRGAN (a) Low Bitrate input image,\\n(b) Low Bitrate output image, (c) Low light input image, (d) Low light\\noutput image, (e) Horizontal motion blur input image, (f) Horizontal\\nmotion blur output image, (g) Vertical motion blur input image and\\n(h) Vertical motion blur output image.\\nJ. METRIC MEASUREMENTS RESULT\\nThere are two types of measurement in this research. First,\\nusing an image qualifier that gives out a Star Rating, and\\nsecond, mathematical measurements 3 including five metrics\\n(FID, PSNR, SCC, SSIM, and VIF) and three synthetic\\nbenchmarks (file size, training time usage per epoch, and\\nrecovery render speed). A proposed model tested up against\\nother approaches4 is presented in this subsection.\\nA quality rating test using an image qualifier shows that the\\nLPRGAN has no problem fixing poor-quality input images.\\nIt generated a 5-Star output image in the low bitrate case\\nand normal-looking images in the rest of the test cases.\\nAt the same time, U-Net architecture with GAN also did\\na reasonable job in most cases. However, a convolutional\\nautoencoder did not perform any good. The reason behind\\nthis measurement is that all mathematical measurements do\\nnot work when there is no reference image (its counterpart)\\nto calculate a value in an actual situation. In reality, only a\\ndegraded image is presented.\\n3Rating score is output from classifier both detector and qualifier. These\\ntwo models have an accuracy of over 99%. All mathematical measurement\\nvalues were computed with a pair of original/reference and distorted images.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 12, 'page_label': '26679', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='autoencoder did not perform any good. The reason behind\\nthis measurement is that all mathematical measurements do\\nnot work when there is no reference image (its counterpart)\\nto calculate a value in an actual situation. In reality, only a\\ndegraded image is presented.\\n3Rating score is output from classifier both detector and qualifier. These\\ntwo models have an accuracy of over 99%. All mathematical measurement\\nvalues were computed with a pair of original/reference and distorted images.\\nAll delta values in each case were calculated against each own low-quality\\nimage. However, it is impossible to compute these metrics values in actual\\ntest cases due to a lack of reference material. In the motion blur problem\\ncase, HB is a horizontal blur, and VB is a vertical blur.\\n4All methods experimented in this study were under the same environment\\nand parameters, such as the same computer machine, an equal number of\\niterations, learning rate, batch size, and the same input image under each\\ntest. Also, all images used in this study were encoded at 100% Quality to\\navoid a compression loss, revealing an actual image data size.\\nVOLUME 11, 2023 26679'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 13, 'page_label': '26680', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nTABLE 3. A metric measurement score table on low Bitrate problem.\\nTABLE 4. A metric measurement score table on low light problem.\\nNext, a visual quality inspection using mathematical cal-\\nculation tests shows that the GAN+U-Net is superior to a\\nconvolutional autoencoder. However, the LPRGAN has the\\nbest outcome in most cases among convolutional autoencoder\\nand GAN+U-Net approaches. As in a low bitrate scenario\\n[Table. 3], the LPRGAN generated the biggest output file\\nsize. It also has the highest (as high as SwinIR) SSIM score\\non low bitrate recovery tests. File size value is essential\\nhere since they tell how much an image holds a piece of\\ninformation. The bigger the file size, the more fine detail\\nis generated. SSIM score indicates that reconstuction image\\nhas a similar structure to an original. These outcomes prove\\nthat the LPRGAN helps predict lost data back, producing\\na richer detailed image. In a low light situation [Table. 4],\\nthe LPRGAN beats out the other models on SSIM score,\\nbut when comparing the LPRGAN to the MIRNet in this\\nscenario (MIRNet cannot reconstruct other cases, only a low\\nlight case), a competitor has three out of five metrics (FID,\\nSCC, and VIF) score higher than the LPRGAN. Even though\\nthe LPRGAN has only two metrics (PSNR and SSIM) wins\\nover the MIRNet. Because of low light situation, these PSNR\\nand SSIM metrics are crucial. Since PSNR is a Power Signal\\nto Noise Ratio, the more PSNR is, the more power signal and\\nthe lesser noise, producing a cleaner image, and SSIM is also\\ncalculated based on image luminosity factors meaning that\\nSSIM is high when the output has a structural and luminosity\\nclose to an original one. A higher SSIM is a brighter image\\nbecause a reference is bright. Thus, the LPRGAN output has\\na lower noise yet a brighter image as the result. In motion\\nblur cases [Table. 5], the LPRGAN has SCC and file size (in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 13, 'page_label': '26680', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='to Noise Ratio, the more PSNR is, the more power signal and\\nthe lesser noise, producing a cleaner image, and SSIM is also\\ncalculated based on image luminosity factors meaning that\\nSSIM is high when the output has a structural and luminosity\\nclose to an original one. A higher SSIM is a brighter image\\nbecause a reference is bright. Thus, the LPRGAN output has\\na lower noise yet a brighter image as the result. In motion\\nblur cases [Table. 5], the LPRGAN has SCC and file size (in\\nhorizontal blur case) metrics higher then the rest. The SCC is\\nthe concerning metric here since it is designed to detect any\\npixel location shifting, which is suitable for this case. A better\\nSCC value means the lesser pixel is shifted from a reference\\nimage. In other words, the lesser blurry image.\\nOn the other hand, US and UK were chosen for this inter-\\nnational plate test. The result from Table. 6 shows that every\\nproblem case passes a visual inspection test. Even though the\\nLPRGAN could not always produce better metric values than\\nthe original input, all generated output always contains more\\ndata than the original ones by looking at the file size metric.\\nThe outputs from the LPRGAN surprisingly have consistent\\nfile sizes on every problem test in a margin of ±1.4 kB.\\nMove over to a speed test, a speed comparison between\\neach approached method [Table. 7] shows that although a\\nconvolutional autoencoder is the fastest in an FPS count due\\nto a non-GAN design it does not produce an output well at all.\\nThe CBDNet is also a non-GAN design but it performs slowly\\nat the same level as GAN-based designs. In contrarily, com-\\nparing GAN-based models, results show that the LPRGAN\\nis the fastest in every render speed test. When compared\\nto the GAN+U-Net approache in 256 × 128 resolution, the\\n26680 VOLUME 11, 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 14, 'page_label': '26681', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nTABLE 5. A metric measurement score table on motion blur problem.\\nTABLE 6. A metric measurement score table on other countries plate.\\nLPRGAN is 2.93× speedup in training speed and more\\nthan twice as fast (2.42×) in a render test. When com-\\npared to the Restomer and the SwinIR in render test, the\\nLPRGAN is 1.77× and 9.32× speedup. At a higher resolu-\\ntion (600 × 400), comparing the LPRGAN to the MIRNet,\\nresults are as expected because the MIRNet has a much\\nbigger network (one complex parallel stream vs. one sim-\\nplified serial stream) and is very limited in resolution (only\\nproducing 600 × 400 output), so the LPRGAN is way faster\\nthan the MIRNet by 25.26× in the render test while the\\nDeblurGANv2+MobileNet is a relatively compact network,\\nthere is still a gap between itself and the LPRGAN in render\\nspeed (1.71× difference), while the GAN+U-Net and the\\nRestomer are dropped behind the LPRGAN. The same story\\nat HD resolution, the LPRGAN is slightly faster than the\\nDeblurGANv2+MobileNet (1.36×) in the same test sce-\\nnario due to fewer parameter counts. The GAN+U-Net is\\neven lacking behind these two GAN-based models while the\\nSwinIR could not finish a test due to painfully low speed. This\\nmakes the LPRGAN the fastest.\\nVOLUME 11, 2023 26681'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 15, 'page_label': '26682', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nTABLE 7. Approached methods speed comparison table.\\nFurthermore, there are three videos used in the LPRGAN\\nspeed on a different type of processor test [Table. 8]. The first\\nvideo is 21.5 seconds long, 932 frames, full-length (100%\\ndegradation) degraded video file named Video#1. The second\\nand the third test case are 21.5 seconds long, 932 frames,\\nhalf-length (50%, 466 frames degradation) degraded video\\nfile and 21.5 seconds long, 932 frames, 25% length (233\\nframes degradation) degraded video files, named Video#2\\nand Video#3. This test represents real-world usage in a mix-\\ning environment because not every video frame would be\\ndegraded all the time, so running recovery on a good frame\\nis a waste. This test shows a difference between the plain\\nLPRGAN and the adaptive LPRGAN (detection+recovery)\\nperformance in action. However, the adaptive LPRGAN did\\nnot actually double the frame rate in Video#2 even though\\na test video contains only half degraded frames due to\\nan additional detection workload, but it is still close to\\nTABLE 8. LPRGAN speed test table.\\nTABLE 9. Memory usage comparison table.\\ndoubling a frame rate than an unequipped detection system by\\n1.71× and 3.3× in Video#2 and #3 on ultra-low power CPU,\\n1.76× and 3.4× in Video#2 and #3 on workstation CPU and\\n1.7× and 2.92× in Video#2 and #3 on a single GPU.\\nThe last one is the memory usage test [Table. 9] (unit in\\nMB), this test is a measurement of memory usage on a single\\nframe recovery from each method. Separated into two groups,\\na non-GAN and a GAN based. Although, non-GAN methods\\ntend to use less RAM as result in this article shows that they\\ndo not work well in many situations. On the other hand, the\\nLPRGAN still uses the least amount of RAM in GAN based\\n26682 VOLUME 11, 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 16, 'page_label': '26683', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nFIGURE 33. Result on additional actual thai license plates from LPRGAN\\n(a) Low Bitrate Input Image#1, (b) Low Bitrate Output Image#1, (c) Low\\nBitrate Input Image#2, (d) Low Bitrate Output Image#2, (e) Low Light\\nInput Image#1, (f) Low Light Output Image#1, (g) Low Light Input\\nImage#2, (h) Low Light Output Image#2, (i) Horizontal Blur Input\\nImage#1, (j) Horizontal Blur Output Image#1, (k) Horizontal Blur Input\\nImage#2, (l) Horizontal Blur Output Image#2, (m) Vertical Blur Input\\nImage#1, (n) Vertical Blur Output Image#1, (o) Vertical Blur Input\\nImage#2 and (p) Vertical Blur Output Image#2.\\nFIGURE 34. Average real world data recovery performance result.\\ngroup. Thus, it is possible to be used in an edge computing\\ndevice where it has a limited amount of RAM.\\nK. REAL WORLD LICENSE PLATES TEST\\nThese are real-world samples [Fig. 33a-Fig. 33p] recovery\\nusing the proposed system. However, these plates in this\\nsubsection do not belong to the author, so to protect their\\nowner’s privacy, they cannot be exposed to the full license\\nplate area [41]. The data recovery result in Fig. 34 shows that\\nin every degraded type, average file sizes have gained more\\ndata after a recovery in low bitrate, low light, horizontal blur,\\nand vertical blur situations at rate 1.51×, 1.25×, 1.61×, and\\n1.63×, respectively.\\n1) DIFFICULT REAL WORLD SITUATION\\nIn extremely poor input images where input information is too\\nmuch distorted, a generated result could be confusing as in\\nFig. 33k- 33l. This problem comes from the generator apply-\\ning deblurring aggressively from its learning which can cause\\nFIGURE 35. (a) Input image, (b) DeblurGANv2+MobileNet output image,\\n(c) Restormer output image and (d) LPRGAN output image.\\nFIGURE 36. Recognizer prediction confidence result.\\nFIGURE 37. Images set used in recognition test.\\nFIGURE 38. Average real world prediction confidence result.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 16, 'page_label': '26683', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='much distorted, a generated result could be confusing as in\\nFig. 33k- 33l. This problem comes from the generator apply-\\ning deblurring aggressively from its learning which can cause\\nFIGURE 35. (a) Input image, (b) DeblurGANv2+MobileNet output image,\\n(c) Restormer output image and (d) LPRGAN output image.\\nFIGURE 36. Recognizer prediction confidence result.\\nFIGURE 37. Images set used in recognition test.\\nFIGURE 38. Average real world prediction confidence result.\\nmisleading information. It can be solved by selecting a differ-\\nent weight from a lower number of iterations where the gener-\\nator is not overpowered by the discriminator. When compar-\\ning the LPRGAN to both the DeblurGANv2+MobileNet and\\nthe Restormer, only the LPRGAN output produces a sharper\\nresult while the rest do not deblur well since those remain\\nunsharp in this real license plate test.\\nL. REAL WORLD LICENSE PLATE RECOGNITION TEST\\nThis test is another aspect of the LPRGAN helping license\\nplate recognition. A recognizer is a recognition system simi-\\nlar to a classifier, a typical VGG-16 network found in Fig. 3\\nVOLUME 11, 2023 26683'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 17, 'page_label': '26684', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\nwhich has three classes, EU, TH 5 [42] and US plates (EU-\\nBelgian and US datasets were from [43] and [44]), in total\\n1,237 training images. These training images differ from\\nthe LPRGAN dataset, making predictions the most neutral.\\nHowever, we only focus on the TH license plate class to prove\\nthat with the help of LPRGAN can make a recognizer has\\nmore confidence in recognizing a license plate. In Fig. 36 is\\na result from Fig. 37 images set, there is not much different\\nresult in low bitrate and vertical blur problems but in low light\\nand horizontal blur cases result in a great benefit from using\\nthe LPRGAN system, whereas normal is a good quality image\\nso its confidence is the highest. The next test is an average\\nconfidence value result by sampling a set of each degraded\\ntype from real-world images (some of them were shown in\\nFig. 33a-Fig. 33p). The result in Fig. 38 shows that recovery\\nimages can increase an average prediction performance in\\nlow bitrate by 1.1 ×, low light by 1.41×, horizontal blur by\\n1.52×, and horizontal blur by 1.16×.\\nV. CONCLUSION\\nThe research presented in this article studied a way to imple-\\nment a fast license plate image quality recovery for traffic\\nmonitoring in various poor situations. The proposed frame-\\nwork uses the optimized lightweight encoder-decoder style\\nCNN architecture built inside a GAN model to do a recov-\\nery job alongside image classifications that detect inputs\\nand verify outputs, helping the LPRGAN in a much more\\nefficient and effective way. This study proved that it could\\nimprove low bitrate, low light, and motion blur problems\\nfrom a single design network in many test cases. Not only\\nthat, this system is able to outpace or be at the same quality\\nlevel as other complex networks while performing the task\\nquickest. As a result, the proposed system can run on less\\ncomputational power machines like most typical workstation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 17, 'page_label': '26684', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='and verify outputs, helping the LPRGAN in a much more\\nefficient and effective way. This study proved that it could\\nimprove low bitrate, low light, and motion blur problems\\nfrom a single design network in many test cases. Not only\\nthat, this system is able to outpace or be at the same quality\\nlevel as other complex networks while performing the task\\nquickest. As a result, the proposed system can run on less\\ncomputational power machines like most typical workstation\\nPCs without a discreet graphic card at a reasonable pace\\nand is possible to deploy in embedded systems such as edge\\ncomputing devices. This study opens a new door for many\\npower-constrained image recovery applications. Such bene-\\nfits make this framework easy to be deployed on traffic officer\\ncomputers or even embedded within camera recording boxes,\\naiding them in identifying vehicle licenses in inadequate\\nconditions. Thus removing the need for a high-performance\\nserver machine and greatly reducing network bandwidth\\nusage between devices.\\nAt this stage, the LPRGAN can render a real-time frame\\nrecovery up to 1280 × 720@15fps, which is sufficient for\\nmost typical CCTV/IP cameras, for example, Merit Lilin\\nZG1232EX3 (3MP, 15FPS) or Merit Lilin LR832 (2MP,\\n15FPS). As time passes, license plates can be collected more,\\ngiving a model retraining even more performance gain. How-\\never, this study demonstrated a few applications that this\\nsystem could handle. It depends on how the user provides a\\ndataset for training because the system uses a good dataset\\n5This set of Thai license plates contains partial dummy plates and is\\navailable upon request.\\nas a template and learns how a distorted dataset differs, so it\\nwould theoretically work in other situations too. Last but not\\nleast, here are some limitations that need to be concerned.\\n• Output dimension size is a fixed size (256 × 128 × 3).\\nIt cannot be used to create other larger or smaller sizes\\nunless reconfiguration and retraining are required.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 17, 'page_label': '26684', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='5This set of Thai license plates contains partial dummy plates and is\\navailable upon request.\\nas a template and learns how a distorted dataset differs, so it\\nwould theoretically work in other situations too. Last but not\\nleast, here are some limitations that need to be concerned.\\n• Output dimension size is a fixed size (256 × 128 × 3).\\nIt cannot be used to create other larger or smaller sizes\\nunless reconfiguration and retraining are required.\\n• Output quality is limited by the training dataset. It cannot\\ncreate a better quality than the original dataset.\\n• Some small detail areas in the generated image could\\nbe somewhat mediocre, such as the province name in\\nsome problem cases. This drawback can be avoided by\\nhaving a higher training dataset resolution i.e., collect-\\ning higher-resolution images or using a super-resolution\\ntechnique.\\n• While the proposed model could be used in other coun-\\ntries rather than its origin, but its performance would\\nnot be on par. Hence, retraining in a target country is\\nrequired.\\nREFERENCES\\n[1] Autoencoder. Building Autoencoders in Keras. Accessed: Jul. 15, 2022.\\n[Online]. Available: https://blog.keras.io/building-autoencoders-in-\\nkeras.html\\n[2] U-Net. U-Net Explained. Accessed: Jul. 15, 2022. [Online]. Available:\\nhttps://paperswithcode.com/method/u-net\\n[3] J. Langr and V. Bok, GANs in Action—Deep Learning With Generative\\nAdversarial Networks, vol. 3, 8th ed. Shelter Island, NY, USA: Manning,\\n2019.\\n[4] DCGAN. Deep Convolutional Generative Adversarial Network.\\nAccessed: Jul. 15, 2022. [Online]. Available: https://www.tensorflow.\\norg/tutorials/generative/dcgan\\n[5] CGAN. Conditional GAN. Accessed: Jul. 15, 2022. [Online]. Available:\\nhttps://keras.io/examples/generative/conditional_gan\\n[6] CycleGAN. Accessed: Jul. 15, 2022. [Online]. Available: https://\\npaperswithcode.com/method/cyclegan\\n[7] A. Petrov, T. Kartalov, and Z. Ivanovski, ‘‘Blocking effect reduc-\\ntion in low bitrate video on a mobile platform,’’ presented at the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 17, 'page_label': '26684', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='Accessed: Jul. 15, 2022. [Online]. Available: https://www.tensorflow.\\norg/tutorials/generative/dcgan\\n[5] CGAN. Conditional GAN. Accessed: Jul. 15, 2022. [Online]. Available:\\nhttps://keras.io/examples/generative/conditional_gan\\n[6] CycleGAN. Accessed: Jul. 15, 2022. [Online]. Available: https://\\npaperswithcode.com/method/cyclegan\\n[7] A. Petrov, T. Kartalov, and Z. Ivanovski, ‘‘Blocking effect reduc-\\ntion in low bitrate video on a mobile platform,’’ presented at the\\nIEEE Int. Conf. Image Process. (ICIP), Nov. 2009. [Online]. Available:\\nhttps://ieeexplore.ieee.org/abstract/document/5414031\\n[8] Y. Dar and A. M. Bruckstein, ‘‘Improving low bit-rate video coding using\\nspatio-temporal down-scaling,’’ 2014, arXiv:1404.4026.\\n[9] Y. Li, D. Liu, H. Li, L. Li, F. Wu, H. Zhang, and H. Yang, ‘‘Convolutional\\nneural network-based block up-sampling for intra frame coding,’’ 2017,\\narXiv:1702.06728.\\n[10] H. Lin, X. He, and L. Qing, ‘‘Improved low-bitrate HEVC video coding\\nusing deep learning based super-resolution and adaptive block patching,’’\\nIEEE Trans. Multimedia, vol. 21, no. 12, pp. 3010–3023, Dec. 2019.\\n[11] R. Yang, M. Xu, T. Liu, Z. Wang, and Z. Guan, ‘‘Enhancing quality for\\nHEVC compressed videos,’’ 2017, arXiv:1709.06734.\\n[12] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, ‘‘Unpaired image-to-\\nimage translation using cycle-consistent adversarial networks,’’ 2017,\\narXiv:1703.10593.\\n[13] Pix2Pix. Image-to-Image Translation using Pix2Pix. Accessed:\\nJul. 15, 2022. [Online]. Available: https://www.geeksforgeeks.org/image-\\nto-image-translation-using-pix2pix\\n[14] S. Guo, Z. Yan, K. Zhang, W. Zuo, and L. Zhang, ‘‘Toward convolutional\\nblind denoising of real photographs,’’ presented at the IEEE/CVF Conf.\\nComput. Vis. Pattern Recognit. (CVPR), Jun. 2019. [Online]. Available:\\nhttps://ieeexplore.ieee.org/document/8954448\\n[15] X. Wang, Y. Li, H. Zhang, and Y. Shan, ‘‘Towards real-world blind\\nface restoration with generative facial prior,’’ in Proc. CVPR, Jun. 2021,\\npp. 9168–9178.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 17, 'page_label': '26684', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='to-image-translation-using-pix2pix\\n[14] S. Guo, Z. Yan, K. Zhang, W. Zuo, and L. Zhang, ‘‘Toward convolutional\\nblind denoising of real photographs,’’ presented at the IEEE/CVF Conf.\\nComput. Vis. Pattern Recognit. (CVPR), Jun. 2019. [Online]. Available:\\nhttps://ieeexplore.ieee.org/document/8954448\\n[15] X. Wang, Y. Li, H. Zhang, and Y. Shan, ‘‘Towards real-world blind\\nface restoration with generative facial prior,’’ in Proc. CVPR, Jun. 2021,\\npp. 9168–9178.\\n[16] O. Kupyn, T. Martyniuk, J. Wu, and Z. Wang, ‘‘DeblurGAN-v2:\\nDeblurring (orders-of-magnitude) faster and better,’’ presented at the\\nIEEE/CVF Int. Conf. Comput. Vis. (ICCV), Aug. 2019. [Online]. Avail-\\nable: https://ieeexplore.ieee.org/document/9008540\\n26684 VOLUME 11, 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 18, 'page_label': '26685', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='W. Sereethavekul, M. Ekpanyapong: Adaptive Lightweight License Plate Image Recovery Using Deep Learning\\n[17] S. W. Zamir, A. Arora, and S. Khan, ‘‘Zamir2020MIRNet : Learning\\nenriched features for real image restoration and enhancement,’’ in Proc.\\nEur. Conf. Comput. Vis. (ECCV), Aug. 2020, pp. 1934–1948.\\n[18] S. W. Zamir, A. Arora, and S. Khan, ‘‘Restormer: Efficient transformer\\nfor high-resolution image restoration,’’ presented at the IEEE/CVF Conf.\\nComput. Vis. Pattern Recognit. (CVPR), Sep. 2022. [Online]. Available:\\nhttps://ieeexplore.ieee.org/document/9878962\\n[19] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte, ‘‘SwinIR:\\nImage restoration using swin transformer,’’ 2021, arXiv:2108.10257.\\n[20] W. Zhang, Z. Zhou, Z. Gao, G. Yang, L. Xu, W. Wu, and H. Zhang,\\n‘‘Multiple adversarial learning based angiography reconstruction for ultra-\\nlow-dose contrast medium CT,’’IEEE J. Biomed. Health Informat., vol. 27,\\nno. 1, pp. 409–420, Jan. 2023.\\n[21] W. Wu, D. Hu, C. Niu, H. Yu, V. Vardhanabhuti, and G. Wang, ‘‘DRONE:\\nDual-domain residual-based optimization NEtwork for sparse-view CT\\nreconstruction,’’IEEE Trans. Med. Imag., vol. 40, no. 11, pp. 3002–3014,\\nNov. 2021.\\n[22] W. Wu, X. Guo, Y. Chen, S. Wang, and J. Chen, ‘‘Deep embedding-\\nattention-refinement for sparse-view CT reconstruction,’’ IEEE Trans.\\nInstrum. Meas., vol. 72, pp. 1–11, 2023.\\n[23] Supervised vs Unsupervised. Clustering vs Classification: Difference\\nBetween Clustering & Classification. Accessed: Jul. 15, 2022. [Online].\\nAvailable: https://www.upgrad.com/blog/clustering-vs-classification\\n[24] Motion Blur in Numpy Array. How to Add Motion Blur to Numpy\\nArray. Accessed: Jul. 15, 2022. [Online]. Available: https://stackoverflow.\\ncom/questions/40305933/how-to-add-motion-blur-to-numpy-array\\n[25] VGG-16. VGG-16 CNN Model. Accessed: Jul. 15, 2022. [Online]. Avail-\\nable: https://www.geeksforgeeks.org/vgg-16-cnn-model\\n[26] J. Tobias Springenberg, A. Dosovitskiy, T. Brox, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 18, 'page_label': '26685', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='Available: https://www.upgrad.com/blog/clustering-vs-classification\\n[24] Motion Blur in Numpy Array. How to Add Motion Blur to Numpy\\nArray. Accessed: Jul. 15, 2022. [Online]. Available: https://stackoverflow.\\ncom/questions/40305933/how-to-add-motion-blur-to-numpy-array\\n[25] VGG-16. VGG-16 CNN Model. Accessed: Jul. 15, 2022. [Online]. Avail-\\nable: https://www.geeksforgeeks.org/vgg-16-cnn-model\\n[26] J. Tobias Springenberg, A. Dosovitskiy, T. Brox, and\\nM. Riedmiller, ‘‘Striving for simplicity: The all convolutional net,’’\\n2014, arXiv:1412.6806.\\n[27] Image Compression. Wikipedia. Accessed: Jul. 15, 2022. [Online]. Avail-\\nable: https://en.wikipedia.org/wiki/Image_compression\\n[28] JPEG. Wikipedia. Accessed: Jul. 15, 2022. [Online]. Available:\\nhttps://en.wikipedia.org/wiki/JPEG\\n[29] Decay Learning Rate. Learning Rate Decay and Methods in Deep Learn-\\ning. Accessed: Jul. 15, 2022. [Online]. Available: https://medium.com/\\nanalytics-vidhya/learning-rate-decay-and-methods-in-deep-learning-\\n2cee564f910b\\n[30] Pytorch-FID. FID Score for PyTorch. Accessed: Jul. 15, 2022. [Online].\\nAvailable: https://pypi.org/project/pytorch-fid\\n[31] Sewar. Sewar Python Package. Accessed: Jul. 15, 2022. [Online]. Avail-\\nable: https://pypi.org/project/sewar\\n[32] FID. How to Implement the Frechet Inception Distance (FID) for\\nEvaluating GANs. Accessed: Jul. 15, 2022. [Online]. Available:\\nhttps://machinelearningmastery.com/how-to-implement-the-frechet-\\ninception-distance-fid-from-scratch\\n[33] PSNR. Peak to Signal Noise Ratio. Accessed: Jul. 15, 2022. [Online].\\nAvailable: https://sonalsart.com/what-is-psnr\\n[34] R. Vallejos, J. Pérez, A. M. Ellison, and A. D. Richardson, ‘‘A spatial\\nconcordance correlation coefficient with an application to image analysis,’’\\n2019, arXiv:1905.05016.\\n[35] SSIM. Structural Similarity Index. Accessed: Jul. 15, 2022. [Online].\\nAvailable: https://medium.com/srm-mic/all-about-structural-similarity-\\nindex-ssim-theory-code-in-pytorch-6551b455541e'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 18, 'page_label': '26685', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='[33] PSNR. Peak to Signal Noise Ratio. Accessed: Jul. 15, 2022. [Online].\\nAvailable: https://sonalsart.com/what-is-psnr\\n[34] R. Vallejos, J. Pérez, A. M. Ellison, and A. D. Richardson, ‘‘A spatial\\nconcordance correlation coefficient with an application to image analysis,’’\\n2019, arXiv:1905.05016.\\n[35] SSIM. Structural Similarity Index. Accessed: Jul. 15, 2022. [Online].\\nAvailable: https://medium.com/srm-mic/all-about-structural-similarity-\\nindex-ssim-theory-code-in-pytorch-6551b455541e\\n[36] VIF. Visual Information Fidelity . Accessed: Jul. 15, 2022. [Online].\\nAvailable: https://www.sciencedirect.com/topics/computer-science/\\nvisual-information-fidelity\\n[37] SCC. Pearson Correlation Coefficient Formula. Accessed: Aug. 15, 2022.\\n[Online]. Available: https://www.cuemath.com/correlation-coefficient-\\nformula\\n[38] ADAM. Adam Optimizer in Tensorflow. Accessed: Jan. 13, 2023. [Online].\\nAvailable: https://www.geeksforgeeks.org/adam-optimizer-in-tensorflow\\n[39] Jeff’s License Plates. Jeffsplates. Accessed: Aug. 17, 2022. [Online].\\nAvailable: https://www.jeffsplates.ca/wp-content/uploads/2018/07/\\nE5E421DF-5108-456C-AE5B-A479BA65A1B2.jpeg\\n[40] U.K. European License Plate. European License Plates. Accessed:\\nAug. 17, 2022. [Online]. Available: https://www.customeuropeanplates.\\ncom/images/uk-license-plate.jpg\\n[41] Use of Disclosure of Personal Data, Section 24 and 27. Thai\\nGovernment Gazette. Accessed: Aug. 20, 2022. [Online].\\nAvailable: https://data.opendevelopmentmekong.net/dataset/78c90118-\\n6671-4c19-afe1-7bfbace4d46a/resource/ec616be5-9fbf-\\n4071-b4b5-cb1f3e46e826/download/entranslation_\\nof_the_personal_data_protection_act_0.pdf\\n[42] Offence Relating to Documents. Chapter 3, Section 264. Thailand Penal\\nCode Thai Criminal Law. Accessed: Nov. 25, 2022. [Online]. Available:\\nhttps://www.samuiforsale.com/law-texts/thailand-penal-code.html#3\\n[43] Belgian License Plates. Kaggle. Accessed: Nov. 24, 2022. [Online]. Avail-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-18T19:10:55+05:30', 'moddate': '2023-03-20T17:27:41-04:00', 'ieee article id': '10065435', 'trapped': 'False', 'ieee issue id': '10005208', 'subject': 'IEEE Access;2023;11; ;10.1109/ACCESS.2023.3255641', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Adaptive Lightweight License Plate Image Recovery Using Deep Learning Based on Generative Adversarial Network', 'source': '..\\\\data\\\\pdf_files\\\\Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'total_pages': 19, 'page': 18, 'page_label': '26685', 'source_file': 'Adaptive_Lightweight_License_Plate_Image_Recovery_Using_Deep_Learning_Based_on_Generative_Adversarial_Network.pdf', 'file_type': 'pdf'}, page_content='Available: https://data.opendevelopmentmekong.net/dataset/78c90118-\\n6671-4c19-afe1-7bfbace4d46a/resource/ec616be5-9fbf-\\n4071-b4b5-cb1f3e46e826/download/entranslation_\\nof_the_personal_data_protection_act_0.pdf\\n[42] Offence Relating to Documents. Chapter 3, Section 264. Thailand Penal\\nCode Thai Criminal Law. Accessed: Nov. 25, 2022. [Online]. Available:\\nhttps://www.samuiforsale.com/law-texts/thailand-penal-code.html#3\\n[43] Belgian License Plates. Kaggle. Accessed: Nov. 24, 2022. [Online]. Avail-\\nable: https://www.kaggle.com/datasets/aladdinss/license-plate-annotated-\\nimage-dataset\\n[44] U.S. License Plates. Kaggle. Accessed: Nov. 24, 2022. [Online]. Available:\\nhttps://www.kaggle.com/datasets/tolgadincer/us-license-plates\\nWUTTINAN SEREETHAVEKUL received the\\nB.Eng. degree in electrical engineering from\\nKasetsart University, Bangkok, Thailand, in 2009,\\nand the M.Eng. degree from Asian Institute of\\nTechnology, Pathum Thani, Thailand, in 2013,\\nwhere he is currently pursuing the D.Eng. degree\\nwith the Department of Industrial Systems Engi-\\nneering. His main research interests include\\nimage/video processing, data recovery, deep learn-\\ning, microelectronics, and computer architecture.\\nMONGKOL EKPANYAPONG received the\\nB.Eng. degree from Chulalongkorn University,\\nThailand, in 1997, the M.Eng. degree from Asian\\nInstitute of Technology, Thailand, in 2000, and the\\nM.Sc. and Ph.D. degrees from Georgia Institute\\nof Technology, in 2003 and 2006, respectively.\\nFrom 1997 to 1998, he was a System Engineer\\nwith United Communication Network, Thailand.\\nFrom 2006 to 2009, he was a Senior Computer\\nArchitect with the Core 2 Architecture Design\\nTeam, Intel Corporation, USA. He joined the School of Engineering and\\nTechnology, Asian Institute of Technology, in 2009, where he is currently\\nan Associate Professor. His research interests include microarchitecture,\\nembedded systems, mechatronics, deep learning, and computer vision.\\nVOLUME 11, 2023 26685'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-11-04T18:57:35+00:00', 'title': 'Arcega_CV', 'moddate': '2025-11-04T18:57:35+00:00', 'keywords': 'DAG3vzieRD8,BAEa-_UscSk,0', 'author': 'Lance Angelo Arcega', 'source': '..\\\\data\\\\pdf_files\\\\Arcega_CV.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'Arcega_CV.pdf', 'file_type': 'pdf'}, page_content='L A N C E  A N G E L O  P  A R C E G A\\nK E Y  S K I L L S P r o g r a m m i n g  L a n g u a g e s  ( J a v a ,\\nJ S ,  P y t h o n ,  M y S Q L )\\nD a t a  A n a l y s i s\\nO r g a n i z a t i o n a l  a n d  t i m e\\nm a n a g e m e n t  s k i l l s .  \\nP r o b l e m - s o l v i n g .\\nT e a m  M a n a g e m e n t / S e r v i c e\\nA t t e n t i o n  t o  d e t a i l s\\nM i c r o s o f t  3 6 5\\nE D U C A T I O N A u g  2 0 2 2  -  P r e s e n tB a c h e l o r  o f  S c i e n c e  i n  C o m p u t e r  S c i e n c e\\nA n g e l e s  U n i v e r s i t y  F o u n d a t i o n\\nS p e c i a l i z a t i o n  i n  D a t a  S c i e n c e\\nC o l l e g e  S c h o l a r\\nP R O J E C T S J a n  2 0 2 5  -  D e c  2 0 2 5R i c e  P r o d u c t i o n  P r e d i c t i o n  u s i n g  R F + G P R  H y b r i d\\nA  w e b  a p p l i c a t i o n  d e s i g n e d  t o  a s s i s t  p o l i c y m a k e r s  i n  p r e d i c t i n g  p o t e n t i a l  r i c e\\np r o d u c t i o n  b a s e d  o n  s p e c i f i c  c o n d i t i o n s  a n d  p a r a m e t e r s .\\nO n e  o f  t h e  d e v e l o p e r s  r e s p o n s i b l e  f o r  d a t a  c o l l e c t i o n ,  d i s a g g r e g a t i o n ,  a n d\\np r e p r o c e s s i n g  t o  t r a i n  t h e  h y b r i d  m o d e l  u s e d  f o r  g e n e r a t i n g  p r e d i c t i o n s .\\nJ a n  2 0 2 3  -  M a y  2 0 2 3I r i s  C h a t  A p p\\nA  c l o u d - b a s e d  m e s s e n g e r  a p p l i c a t i o n  d e v e l o p e d  u s i n g  C #  f o r  t h e  f r o n t  e n d\\na n d  M i c r o s o f t  A z u r e  S Q L  f o r  t h e  b a c k  e n d .\\nO n e  o f  t h e  d e v e l o p e r s  r e s p o n s i b l e  f o r  b u i l d i n g  t h e  c h a t  s y s t e m  a n d  d e p l o y i n g\\nt h e  i n t e g r a t i o n  b e t w e e n  t h e  a p p l i c a t i o n  a n d  t h e  d a t a b a s e .\\nF e b  2 0 2 4  -  D e c  2 0 2 4A U F  B a r a n g a y  E M R'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-11-04T18:57:35+00:00', 'title': 'Arcega_CV', 'moddate': '2025-11-04T18:57:35+00:00', 'keywords': 'DAG3vzieRD8,BAEa-_UscSk,0', 'author': 'Lance Angelo Arcega', 'source': '..\\\\data\\\\pdf_files\\\\Arcega_CV.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'Arcega_CV.pdf', 'file_type': 'pdf'}, page_content='a n d  M i c r o s o f t  A z u r e  S Q L  f o r  t h e  b a c k  e n d .\\nO n e  o f  t h e  d e v e l o p e r s  r e s p o n s i b l e  f o r  b u i l d i n g  t h e  c h a t  s y s t e m  a n d  d e p l o y i n g\\nt h e  i n t e g r a t i o n  b e t w e e n  t h e  a p p l i c a t i o n  a n d  t h e  d a t a b a s e .\\nF e b  2 0 2 4  -  D e c  2 0 2 4A U F  B a r a n g a y  E M R\\nA n  E l e c t r o n i c  M e d i c a l  R e c o r d  s y s t e m  d e s i g n e d  t o  a u t o m a t e ,  s t r e a m l i n e ,  a n d\\no r g a n i z e  m e d i c a l  d a t a .\\nC o - d e v e l o p e r  o f  t h e  s y s t e m ,  i n c l u d i n g  t h e  i m p l e m e n t a t i o n  o f  a n  o n l i n e  f o r m\\nt h a t  c a n  b e  c o n v e r t e d  i n t o  a  p r i n t e d  c o p y  i n  c o m p l i a n c e  w i t h  D e p a r t m e n t  o f\\nH e a l t h  ( D O H )  s t a n d a r d s .\\nS U M M A R Y\\nI  a m  a  f o u r t h - y e a r  C o m p u t e r  S c i e n c e  U n d e r g r a d u a t e  s p e c i a l i z i n g  i n  D a t a  S c i e n c e  a t\\nA n g e l e s  U n i v e r s i t y  F o u n d a t i o n .  O v e r  t h e  c o u r s e  o f  m y  c o l l e g e  u n d e r t a k i n g ,  I\\nd e v e l o p e d  a  d e e p  u n d e r s t a n d i n g  i n  d a t a  e n g i n e e r i n g ,  d a t a  a n a l y s i s ,  A I  i n t e g r a t i o n\\nt h r o u g h  m a c h i n e  l e a r n i n g  a n d  d e e p  l e a r n i n g .\\na r c e g a . l a n c e a n g e l o @ o u t l o o k . c o m  |  9 1 8 - 6 4 0 - 9 0 9 1  |  C i t y  o f  S a n  F e r n a n d o ,  P a m p a n g a ,  P h i l i p p i n e s  \\nh t t p s : / / w w w . l i n k e d i n . c o m / i n / l a n c e - a n g e l o - a r c e g a /\\nG e s t u r a  O S S e p  2 0 2 5  -  O c t  2 0 2 5\\nD e v e l o p e d  a n  a p p l i c a t i o n  t h a t  u s e s  c o m p u t e r  v i s i o n  t o  c o n t r o l  t h e  c u r s o r'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-11-04T18:57:35+00:00', 'title': 'Arcega_CV', 'moddate': '2025-11-04T18:57:35+00:00', 'keywords': 'DAG3vzieRD8,BAEa-_UscSk,0', 'author': 'Lance Angelo Arcega', 'source': '..\\\\data\\\\pdf_files\\\\Arcega_CV.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'Arcega_CV.pdf', 'file_type': 'pdf'}, page_content='a r c e g a . l a n c e a n g e l o @ o u t l o o k . c o m  |  9 1 8 - 6 4 0 - 9 0 9 1  |  C i t y  o f  S a n  F e r n a n d o ,  P a m p a n g a ,  P h i l i p p i n e s  \\nh t t p s : / / w w w . l i n k e d i n . c o m / i n / l a n c e - a n g e l o - a r c e g a /\\nG e s t u r a  O S S e p  2 0 2 5  -  O c t  2 0 2 5\\nD e v e l o p e d  a n  a p p l i c a t i o n  t h a t  u s e s  c o m p u t e r  v i s i o n  t o  c o n t r o l  t h e  c u r s o r\\nt h r o u g h  h a n d  g e s t u r e s ,  u t i l i z i n g  a  c a m e r a  t o  d e t e c t  h a n d  m o v e m e n t s  a n d\\np e r f o r m  m o u s e  f u n c t i o n s .\\nC E R T I F I C A T I O N A Z 9 0 0  A z u r e  F u n d a m e n t a l s\\nC r e d e n t i a l  I D :  A 7 F 2 4 A 8 9 1 0 4 A D E 0 5'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-19T21:22:00+08:00', 'author': 'JANSEN  CRUZ', 'moddate': '2025-11-19T21:22:00+08:00', 'source': '..\\\\data\\\\pdf_files\\\\Jansen_Cruz_CV.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'Jansen_Cruz_CV.pdf', 'file_type': 'pdf'}, page_content='JANSEN C. CRUZ \\njansen.c.cruz@gmail.com | LinkedIn | GitHub | +63-921-723-8199 \\nEDUCATION \\nAngeles University Foundation – BS Computer Science       2022 - 2026 \\nUniversity Scholar (President’s Lister), BYA Scholar, Ranked 1st in Program (2022 – Present) \\nRelevant Coursework: Artificial Intelligence, Machine Learning, Data Science, Software Engineering \\nSTI College Angeles – IT in Mobile App and Web Development     2020 - 2022 \\nRanked 3rd in the graduating batch \\n \\nPROJECTS \\nLicense Plate AI Deblurring – AI/ML Engineer, Backend Developer | Thesis                July 2025 – Oct 2025 \\n• Designed and trained deep learning models achieving 96.2% accuracy in distortion classification \\n(CNN) and 90% accuracy license plate recognition (CRNN), enhancing the reliability and \\neffectiveness of traffic enforcement in low-quality surveillance conditions. \\n• Developed 4x distortion-specific GANs using Residual Attention U-Net architectures that restored \\nimage quality by up to 84% (SSIM), effectively reducing motion blur, low-light artifacts, and \\ncompression noise critical for real-world Philippines license recognition. \\n• Optimized end-to-end pipeline performance to achieve real-time processing speeds of  \\n1.57s (GPU) / 2.42s (CPU), enabling practical deployment on resource-constrained devices.  \\nLoan Management System – Full-stack Developer | Freelance                      Aug 2025 – Sept 2025 \\n• Engineered a scalable financial web application using C# ASP .NET Core, MSSQL, and Entity \\nFramework Core, applying Clean Architecture and CQRS patterns for high maintainability. \\n• Delivered end-to-end features including loan, borrower, and branch management with automated \\npayment tracking and real-time KPI dashboards with interactive charts for actionable insights. \\n• Replaced manual Excel tracking with a secure, automated solution featuring role-based and real-time \\ncustomer tracking, reducing operational overhead by ~70%.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-19T21:22:00+08:00', 'author': 'JANSEN  CRUZ', 'moddate': '2025-11-19T21:22:00+08:00', 'source': '..\\\\data\\\\pdf_files\\\\Jansen_Cruz_CV.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'Jansen_Cruz_CV.pdf', 'file_type': 'pdf'}, page_content='Framework Core, applying Clean Architecture and CQRS patterns for high maintainability. \\n• Delivered end-to-end features including loan, borrower, and branch management with automated \\npayment tracking and real-time KPI dashboards with interactive charts for actionable insights. \\n• Replaced manual Excel tracking with a secure, automated solution featuring role-based and real-time \\ncustomer tracking, reducing operational overhead by ~70%. \\n• Received positive client validation: “Files are secure and customer tracking is effortless.” \\nBrgy. Ninoy Aquino EMR – Full-stack Developer, Project Lead | Community Project                Feb 2024 – Dec 2024 \\n• Spearheaded a 5-member team to develop, deploy, and maintain a healthcare management system \\nusing C#, ASP .NET Core, Entity Framework Core, and MySQL, implementing MVC architecture to \\nreplace disorganized paper records and eliminate data loss risks.  \\n• Developed data analytics dashboards transforming raw health data into actionable insights using \\nreal-time visualizations, enabling evidence-based decision making for midwives. \\n• Integrated FINDRISC-based diabetes risk assessment module; validated through CSUQ surveys \\nwith a near-perfect usability score (mean = 1.06, where 1.00 is best) and “Best Imaginable” ratings \\nacross all dimensions – information quality, interface quality, and overall usability.  \\n \\nCERTIFICATIONS \\nMicrosoft AZ-900: Azure Fundamentals            May 2025 \\nIT Specialist – Cloud Computing                                 Oct 2025 \\nOracle Cloud Infrastructure: AI Foundations Associate                             Oct 2025 \\nOracle Cloud Infrastructure: Generative AI Professional                            Oct 2025 \\n \\nTECHNICAL SKILLS  \\nLanguages: Python, C#, JavaScript, Java, Dart, SQL, HTML, CSS \\nFrameworks: PyTorch, TensorFlow / Keras, LangChain, ASP .NET Core, Entity Framework Core, Django, Tailwind'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-11-19T21:22:00+08:00', 'author': 'JANSEN  CRUZ', 'moddate': '2025-11-19T21:22:00+08:00', 'source': '..\\\\data\\\\pdf_files\\\\Jansen_Cruz_CV.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'Jansen_Cruz_CV.pdf', 'file_type': 'pdf'}, page_content='IT Specialist – Cloud Computing                                 Oct 2025 \\nOracle Cloud Infrastructure: AI Foundations Associate                             Oct 2025 \\nOracle Cloud Infrastructure: Generative AI Professional                            Oct 2025 \\n \\nTECHNICAL SKILLS  \\nLanguages: Python, C#, JavaScript, Java, Dart, SQL, HTML, CSS \\nFrameworks: PyTorch, TensorFlow / Keras, LangChain, ASP .NET Core, Entity Framework Core, Django, Tailwind \\nDeveloper Tools: Git, GitHub, Anaconda, Jupyter, Visual Studio Code, Visual Studio, Hugging Face, Power BI \\n \\nACTIVITIES & AWARDS \\nHackathons: JPCS Hackathon Champion (2025), 2nd Place (2025), 3rd Place (2024), iCode 2nd Place (2021, 2022)')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28e3fb9-d0c2-4bd9-86cc-67c1376c8262",
   "metadata": {},
   "source": [
    "### Embedding and VectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c63ddd72-af59-46e3-b2fd-78d0f60b764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2fee5f59-8a01-45bc-8591-ef080d14d190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x26ddb23a750>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles documeng embedding generating using SentenceTransformer\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str=\"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"Initialize the embedding manager\n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name=model_name\n",
    "        self.model=None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for a list of texts\n",
    "        Args:\n",
    "            texts: List of text string to embed\n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "\n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b16309a-50ff-4aff-9226-0cae21c07d23",
   "metadata": {},
   "source": [
    "### VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c011e4df-5f84-4a29-866a-4c2db977e59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectore store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x26dd3ace240>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "\n",
    "    def __init__(self, collection_name: str=\"pdf_documents\", persist_directory: str=\"../data/vector_store\"):\n",
    "        \"\"\"Initialize the vector store\n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name=collection_name\n",
    "        self.persist_directory=persist_directory\n",
    "        self.client=None\n",
    "        self.collection=None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vectore store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Document], embeddings: np.ndarray):\n",
    "        \"\"\"Add documents and their embeddings to the vector store\n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match the number of embeddings\")\n",
    "\n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "\n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully loaded {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vector_store = VectorStore()\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2007256b-4659-40de-b2cf-2e5e98d27532",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 89 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|███████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (89, 384)\n",
      "Adding 89 documents to vector store...\n",
      "Successfully loaded 89 documents to vector store\n",
      "Total documents in collection: 590\n"
     ]
    }
   ],
   "source": [
    " # Convert the text to embeddings\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "\n",
    "# Generate the embeddings\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "# Store in the vector database\n",
    "vector_store.add_documents(documents=chunks, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121507c7-515c-4152-8c0b-e1169cecf9c1",
   "metadata": {},
   "source": [
    "### Retriever Pipeline from VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "56dcb8bf-4db5-42f2-a754-6658dd3066e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "\n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"Initialize the retriever\n",
    "        Args:\n",
    "            vector_store: Vectore store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store=vector_store\n",
    "        self.embedding_manager=embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int=5, score_threshold: float=0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve relevant documents for a query\n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "\n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "\n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "\n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f'Retrieved {len(retrieved_docs)} documents (after filtering)')\n",
    "            else:\n",
    "                print('No documents found')\n",
    "\n",
    "            return retrieved_docs\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error during retrieval: {e}')\n",
    "            return []\n",
    "\n",
    "rag_retriever = RAGRetriever(vector_store=vector_store, embedding_manager=embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3b2e41a5-8935-40f3-969b-886818442aef",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is License Plate AI Deblurring'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 143.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_ba646422_5',\n",
       "  'content': 'pacted the image deblurring field[Ramakrishnan et al., 2017;\\nKupyn et al., 2018; Kupyn et al., 2019; Zhao et al., 2022].\\nDespite these advancements, deblurring license plate images\\nremains a significant challenge, primarily due to the lack of\\nlarge-scale, tailored datasets. The complexity of license plate\\nblurring, with its more severe degradation compared to stan-\\ndard motion blur, poses an additional challenge. To better jus-\\ntify the performance of existing image deblurring algorithms\\non real-world blurred license plate images, we evaluate the\\nperformance of several state-of-the-art deblurring algorithms\\narXiv:2404.13677v1  [cs.CV]  21 Apr 2024',\n",
       "  'metadata': {'file_type': 'pdf',\n",
       "   'subject': '',\n",
       "   'creationdate': '2024-04-23T00:45:45+00:00',\n",
       "   'page_label': '1',\n",
       "   'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf',\n",
       "   'moddate': '2024-04-23T00:45:45+00:00',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'author': '',\n",
       "   'title': '',\n",
       "   'content_length': 655,\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'trapped': '/False',\n",
       "   'doc_index': 5,\n",
       "   'keywords': '',\n",
       "   'total_pages': 9,\n",
       "   'page': 0,\n",
       "   'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'templateversion': 'IJCAI.2024.0'},\n",
       "  'similarity_score': 0.36261606216430664,\n",
       "  'distance': 0.6373839378356934,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_b132245b_5',\n",
       "  'content': 'pacted the image deblurring field[Ramakrishnan et al., 2017;\\nKupyn et al., 2018; Kupyn et al., 2019; Zhao et al., 2022].\\nDespite these advancements, deblurring license plate images\\nremains a significant challenge, primarily due to the lack of\\nlarge-scale, tailored datasets. The complexity of license plate\\nblurring, with its more severe degradation compared to stan-\\ndard motion blur, poses an additional challenge. To better jus-\\ntify the performance of existing image deblurring algorithms\\non real-world blurred license plate images, we evaluate the\\nperformance of several state-of-the-art deblurring algorithms\\narXiv:2404.13677v1  [cs.CV]  21 Apr 2024',\n",
       "  'metadata': {'keywords': '',\n",
       "   'page': 0,\n",
       "   'file_type': 'pdf',\n",
       "   'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'content_length': 655,\n",
       "   'creationdate': '2024-04-23T00:45:45+00:00',\n",
       "   'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf',\n",
       "   'title': '',\n",
       "   'moddate': '2024-04-23T00:45:45+00:00',\n",
       "   'page_label': '1',\n",
       "   'trapped': '/False',\n",
       "   'total_pages': 9,\n",
       "   'doc_index': 5,\n",
       "   'author': '',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'subject': '',\n",
       "   'templateversion': 'IJCAI.2024.0'},\n",
       "  'similarity_score': 0.36261606216430664,\n",
       "  'distance': 0.6373839378356934,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_48058d55_5',\n",
       "  'content': 'pacted the image deblurring field[Ramakrishnan et al., 2017;\\nKupyn et al., 2018; Kupyn et al., 2019; Zhao et al., 2022].\\nDespite these advancements, deblurring license plate images\\nremains a significant challenge, primarily due to the lack of\\nlarge-scale, tailored datasets. The complexity of license plate\\nblurring, with its more severe degradation compared to stan-\\ndard motion blur, poses an additional challenge. To better jus-\\ntify the performance of existing image deblurring algorithms\\non real-world blurred license plate images, we evaluate the\\nperformance of several state-of-the-art deblurring algorithms\\narXiv:2404.13677v1  [cs.CV]  21 Apr 2024',\n",
       "  'metadata': {'author': '',\n",
       "   'templateversion': 'IJCAI.2024.0',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'file_type': 'pdf',\n",
       "   'keywords': '',\n",
       "   'page_label': '1',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'doc_index': 5,\n",
       "   'content_length': 655,\n",
       "   'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf',\n",
       "   'trapped': '/False',\n",
       "   'title': '',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'moddate': '2024-04-23T00:45:45+00:00',\n",
       "   'subject': '',\n",
       "   'total_pages': 9,\n",
       "   'creationdate': '2024-04-23T00:45:45+00:00',\n",
       "   'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf',\n",
       "   'page': 0},\n",
       "  'similarity_score': 0.36261606216430664,\n",
       "  'distance': 0.6373839378356934,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_c75f5d28_2',\n",
       "  'content': 'ring methods in realistic license plate deblurring\\nscenarios. The dataset and code are available at\\nhttps://github.com/haoyGONG/LPDGAN.\\n1 Introduction\\nEfficient recognition of vehicle license plates is crucial for\\nintelligent traffic management systems, however real-world\\nscenarios often pose a significant challenge due to motion\\nblur. This blur, making license plates unreadable, is espe-\\ncially problematic in situations involving high-speed vehicles\\nor low-light conditions. Such issues are exacerbated during\\nnighttime or in bad weather, resulting in considerable motion\\nblur in captured images. To tackle these issues, our study in-\\ntroduces a comprehensive dataset and a novel model tailored\\nfor realistic license plate deblurring.\\n∗Corresponding author\\nMSSNet LBAGMIMO-UnetBlurred \\nLicense Plate Ground Truth\\nLPDGAN\\n(Ours)\\nFigure 1: The visual deblurring results of several state-of-the-art\\nmodels and our model for real-world motion blurred license plate\\nimages.',\n",
       "  'metadata': {'keywords': '',\n",
       "   'author': '',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'doc_index': 2,\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'page': 0,\n",
       "   'page_label': '1',\n",
       "   'total_pages': 9,\n",
       "   'templateversion': 'IJCAI.2024.0',\n",
       "   'creationdate': '2024-04-23T00:45:45+00:00',\n",
       "   'file_type': 'pdf',\n",
       "   'title': '',\n",
       "   'trapped': '/False',\n",
       "   'subject': '',\n",
       "   'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'content_length': 972,\n",
       "   'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf',\n",
       "   'moddate': '2024-04-23T00:45:45+00:00'},\n",
       "  'similarity_score': 0.36063069105148315,\n",
       "  'distance': 0.6393693089485168,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_d3cb7c25_2',\n",
       "  'content': 'ring methods in realistic license plate deblurring\\nscenarios. The dataset and code are available at\\nhttps://github.com/haoyGONG/LPDGAN.\\n1 Introduction\\nEfficient recognition of vehicle license plates is crucial for\\nintelligent traffic management systems, however real-world\\nscenarios often pose a significant challenge due to motion\\nblur. This blur, making license plates unreadable, is espe-\\ncially problematic in situations involving high-speed vehicles\\nor low-light conditions. Such issues are exacerbated during\\nnighttime or in bad weather, resulting in considerable motion\\nblur in captured images. To tackle these issues, our study in-\\ntroduces a comprehensive dataset and a novel model tailored\\nfor realistic license plate deblurring.\\n∗Corresponding author\\nMSSNet LBAGMIMO-UnetBlurred \\nLicense Plate Ground Truth\\nLPDGAN\\n(Ours)\\nFigure 1: The visual deblurring results of several state-of-the-art\\nmodels and our model for real-world motion blurred license plate\\nimages.',\n",
       "  'metadata': {'moddate': '2024-04-23T00:45:45+00:00',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'total_pages': 9,\n",
       "   'page_label': '1',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'source': '..\\\\data\\\\pdf_files\\\\A Dataset and Model for Realistic License Plate Deblurring.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'title': '',\n",
       "   'creationdate': '2024-04-23T00:45:45+00:00',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'templateversion': 'IJCAI.2024.0',\n",
       "   'author': '',\n",
       "   'trapped': '/False',\n",
       "   'subject': '',\n",
       "   'content_length': 972,\n",
       "   'doc_index': 2,\n",
       "   'page': 0,\n",
       "   'source_file': 'A Dataset and Model for Realistic License Plate Deblurring.pdf',\n",
       "   'keywords': ''},\n",
       "  'similarity_score': 0.36063069105148315,\n",
       "  'distance': 0.6393693089485168,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve('What is License Plate AI Deblurring')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1262c5f7-42b8-47f9-9bbe-bb2aae93a620",
   "metadata": {},
   "source": [
    "### Integration VectorDB Context Pipeline with LLM Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "053f8199-3595-44a8-a16d-2f8c33752016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG pipeline with Groq LLM\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Groq\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "llm = ChatGroq(\n",
    "    groq_api_key=groq_api_key,\n",
    "    model_name=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "# RAG function: retrieve context + generate response\n",
    "def rag_simple(query, retriever, llm, top_k=3):\n",
    "    results = retriever.retrieve(query, top_k=top_k)\n",
    "    context = '\\n\\n'.join([doc['content'] for doc in results]) if results else ''\n",
    "\n",
    "    if not context:\n",
    "        return 'No relevant context found to answer the question'\n",
    "\n",
    "    # Generate the answer using Groq LLM\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\n",
    "        Context: \n",
    "        {context}\n",
    "    \n",
    "        Question: {query}\n",
    "    \n",
    "        Answer:\"\"\"\n",
    "\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    return response.content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c2be1d5e-1923-4da5-a900-c0a81410bbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is License Plate Deblurring?'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 166.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Efficient recognition of vehicle license plates is crucial for intelligent traffic management systems, however real-world scenarios often pose a significant challenge due to motion blur.\n"
     ]
    }
   ],
   "source": [
    "answer = rag_simple(\"What is License Plate Deblurring?\", rag_retriever, llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c49dfbe-6344-46c4-851b-146510ed0e36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-thesis",
   "language": "python",
   "name": "rag-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
